{
  "nodes": [
    {
      "index_id": 0,
      "parent_id": null,
      "type": "root",
      "meta_info": {
        "file_name": "BOOKRAG_VLDB_2026_full.pdf",
        "file_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\BOOKRAG_VLDB_2026_full.pdf",
        "page_idx": null,
        "page_path": null,
        "pdf_id": 0,
        "pdf_para_block": null,
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": null,
        "title_level": -1
      },
      "summary": ""
    },
    {
      "index_id": 1,
      "parent_id": 0,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 1,
        "pdf_para_block": {
          "bbox": [
            52,
            80,
            558,
            120
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                52,
                80,
                558,
                120
              ],
              "spans": [
                {
                  "bbox": [
                    52,
                    80,
                    558,
                    120
                  ],
                  "type": "text",
                  "content": "BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents"
                }
              ]
            }
          ],
          "index": 0
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents",
        "title_level": 0
      },
      "summary": "This section introduces BookRAG, a novel hierarchical structure-aware RAG methodology that utilizes a specialized index and agent-based querying to achieve state-of-the-art performance in complex document question answering."
    },
    {
      "index_id": 2,
      "parent_id": 1,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 2,
        "pdf_para_block": {
          "bbox": [
            115,
            132,
            168,
            144
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                115,
                132,
                168,
                144
              ],
              "spans": [
                {
                  "bbox": [
                    115,
                    132,
                    168,
                    144
                  ],
                  "type": "text",
                  "content": "Shu Wang"
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Shu Wang",
        "title_level": -1
      },
      "summary": "Shu Wang is a name, likely referring to a person."
    },
    {
      "index_id": 3,
      "parent_id": 1,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 3,
        "pdf_para_block": {
          "bbox": [
            74,
            144,
            208,
            156
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                74,
                144,
                208,
                156
              ],
              "spans": [
                {
                  "bbox": [
                    74,
                    144,
                    208,
                    156
                  ],
                  "type": "text",
                  "content": "The Chinese University of Hong"
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The Chinese University of Hong Kong, Shenzhen",
        "title_level": -1
      },
      "summary": "The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) is a research-intensive university established in 2014 through a partnership between Shenzhen University and The Chinese University of Hong Kong. It operates as an independent legal entity in mainland China, inheriting the educational philosophy and academic systems of its Hong Kong counterpart. The university offers programs in English and Chinese across disciplines like science, engineering, business, and humanities, with a strong focus on internationalization and innovation."
    },
    {
      "index_id": 4,
      "parent_id": 1,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 4,
        "pdf_para_block": {
          "bbox": [
            82,
            169,
            201,
            180
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                82,
                169,
                201,
                180
              ],
              "spans": [
                {
                  "bbox": [
                    82,
                    169,
                    201,
                    180
                  ],
                  "type": "text",
                  "content": "shuwang3@link.cuhk.edu.cn"
                }
              ]
            }
          ],
          "index": 4
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "shuwang3@link.cuhk.edu.cn",
        "title_level": -1
      },
      "summary": "The provided content is an email address: shuwang3@link.cuhk.edu.cn."
    },
    {
      "index_id": 5,
      "parent_id": 1,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 5,
        "pdf_para_block": {
          "bbox": [
            275,
            132,
            335,
            144
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                275,
                132,
                335,
                144
              ],
              "spans": [
                {
                  "bbox": [
                    275,
                    132,
                    335,
                    144
                  ],
                  "type": "text",
                  "content": "Yingli Zhou"
                }
              ]
            }
          ],
          "index": 5
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Yingli Zhou",
        "title_level": -1
      },
      "summary": "Yingli Zhou is the core subject identified."
    },
    {
      "index_id": 6,
      "parent_id": 1,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 6,
        "pdf_para_block": {
          "bbox": [
            239,
            145,
            372,
            156
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                239,
                145,
                372,
                156
              ],
              "spans": [
                {
                  "bbox": [
                    239,
                    145,
                    372,
                    156
                  ],
                  "type": "text",
                  "content": "The Chinese University of Hong"
                }
              ]
            }
          ],
          "index": 6
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The Chinese University of Hong Kong, Shenzhen",
        "title_level": -1
      },
      "summary": "The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) is a research university established in 2014 through a partnership between Shenzhen University and The Chinese University of Hong Kong. It is a legally independent institution in mainland China that inherits the educational philosophy and academic systems of its Hong Kong counterpart. The university offers programs in English and Chinese across disciplines like science, engineering, business, and humanities, and it awards its own academic credentials."
    },
    {
      "index_id": 7,
      "parent_id": 1,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 7,
        "pdf_para_block": {
          "bbox": [
            245,
            169,
            365,
            180
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                245,
                169,
                365,
                180
              ],
              "spans": [
                {
                  "bbox": [
                    245,
                    169,
                    365,
                    180
                  ],
                  "type": "text",
                  "content": "yinglizhou@link.cuhk.edu.cn"
                }
              ]
            }
          ],
          "index": 8
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "yinglizhou@link.cuhk.edu.cn",
        "title_level": -1
      },
      "summary": "This is an email address for someone named Yingli Zhou at the Chinese University of Hong Kong (CUHK)."
    },
    {
      "index_id": 8,
      "parent_id": 1,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 8,
        "pdf_para_block": {
          "bbox": [
            437,
            132,
            503,
            144
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                437,
                132,
                503,
                144
              ],
              "spans": [
                {
                  "bbox": [
                    437,
                    132,
                    503,
                    144
                  ],
                  "type": "text",
                  "content": "Yixiang Fang"
                }
              ]
            }
          ],
          "index": 9
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Yixiang Fang",
        "title_level": -1
      },
      "summary": "Yixiang Fang is a name, likely referring to a person."
    },
    {
      "index_id": 9,
      "parent_id": 1,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 9,
        "pdf_para_block": {
          "bbox": [
            403,
            145,
            536,
            156
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                403,
                145,
                536,
                156
              ],
              "spans": [
                {
                  "bbox": [
                    403,
                    145,
                    536,
                    156
                  ],
                  "type": "text",
                  "content": "The Chinese University of Hong"
                }
              ]
            }
          ],
          "index": 10
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The Chinese University of Hong Kong, Shenzhen",
        "title_level": -1
      },
      "summary": "The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) is a research-intensive university established in 2014 through a partnership between Shenzhen University and The Chinese University of Hong Kong. It operates as an independent legal entity under mainland China's regulations while inheriting the educational philosophy and academic systems of its Hong Kong counterpart. The university offers programs in English and Chinese across disciplines such as science, engineering, business, and humanities, and it grants its own degrees. Its mission is to nurture innovative talent with a global perspective for the Greater Bay Area and China."
    },
    {
      "index_id": 10,
      "parent_id": 1,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 10,
        "pdf_para_block": {
          "bbox": [
            416,
            169,
            523,
            180
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                416,
                169,
                523,
                180
              ],
              "spans": [
                {
                  "bbox": [
                    416,
                    169,
                    523,
                    180
                  ],
                  "type": "text",
                  "content": "fangyixiang@cuhk.edu.cn"
                }
              ]
            }
          ],
          "index": 12
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "fangyixiang@cuhk.edu.cn",
        "title_level": -1
      },
      "summary": "This is an email address for Fang Yixiang at the Chinese University of Hong Kong (CUHK)."
    },
    {
      "index_id": 11,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 11,
        "pdf_para_block": {
          "bbox": [
            51,
            188,
            113,
            198
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                188,
                113,
                198
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    188,
                    113,
                    198
                  ],
                  "type": "text",
                  "content": "ABSTRACT"
                }
              ]
            }
          ],
          "index": 13
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "ABSTRACT",
        "title_level": 1
      },
      "summary": "This section introduces BookRAG, a novel hierarchical structure-aware RAG method that uses a specialized index and agent-based querying to achieve state-of-the-art question answering performance on complex documents."
    },
    {
      "index_id": 12,
      "parent_id": 11,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 12,
        "pdf_para_block": {
          "bbox": [
            50,
            202,
            296,
            468
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                202,
                296,
                468
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    202,
                    296,
                    468
                  ],
                  "type": "text",
                  "content": "As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency."
                }
              ]
            }
          ],
          "index": 14
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency.",
        "title_level": -1
      },
      "summary": "BookRAG is a new Retrieval-Augmented Generation (RAG) method designed to significantly improve question answering for documents with a hierarchical structure, such as books and handbooks. It addresses the limitation of existing RAG approaches, which perform poorly on such documents by ignoring their organization. The method introduces a novel index called BookIndex, which models the document's hierarchy as a tree (like a table of contents), captures entity relationships with a graph, and links entities to the tree. Using this index, BookRAG employs an agent-based query system that dynamically classifies questions and follows a tailored retrieval process. Experiments on three benchmarks show that BookRAG achieves state-of-the-art results, greatly outperforming baseline methods in both retrieval recall and QA accuracy while remaining efficient."
    },
    {
      "index_id": 13,
      "parent_id": 11,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 13,
        "pdf_para_block": {
          "bbox": [
            51,
            475,
            149,
            484
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                475,
                149,
                484
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    475,
                    149,
                    484
                  ],
                  "type": "text",
                  "content": "PVLDB Reference Format:"
                }
              ]
            }
          ],
          "index": 15
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "PVLDB Reference Format:",
        "title_level": -1
      },
      "summary": "The PVLDB reference format is a standardized citation style for academic papers published in the Proceedings of the VLDB Endowment."
    },
    {
      "index_id": 14,
      "parent_id": 11,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 14,
        "pdf_para_block": {
          "bbox": [
            50,
            485,
            278,
            525
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                485,
                278,
                525
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    485,
                    278,
                    525
                  ],
                  "type": "text",
                  "content": "Shu Wang, Yingli Zhou, and Yixiang Fang. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents. PVLDB, 19(1): XXX-XXX, 2025. doi:XX.XX/XXX.XX"
                }
              ]
            }
          ],
          "index": 16
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Shu Wang, Yingli Zhou, and Yixiang Fang. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents. PVLDB, 19(1): XXX-XXX, 2025. doi:XX.XX/XXX.XX",
        "title_level": -1
      },
      "summary": "BookRAG is a new approach for Retrieval-Augmented Generation (RAG) designed to handle complex documents. It introduces a hierarchical structure-aware index to improve the accuracy and efficiency of retrieving information from documents with intricate layouts, such as those containing tables, figures, and multi-level headings. The method is presented in a 2025 article in the Proceedings of the VLDB Endowment (PVLDB)."
    },
    {
      "index_id": 15,
      "parent_id": 11,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 15,
        "pdf_para_block": {
          "bbox": [
            51,
            534,
            157,
            544
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                534,
                157,
                544
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    534,
                    157,
                    544
                  ],
                  "type": "text",
                  "content": "PVLDB Artifact Availability:"
                }
              ]
            }
          ],
          "index": 17
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "PVLDB Artifact Availability:",
        "title_level": -1
      },
      "summary": "PVLDB artifact availability refers to the accessibility of research artifacts—such as code, data, and scripts—associated with papers published in the Proceedings of the VLDB Endowment. This practice promotes reproducibility, verification, and further research by ensuring that supplementary materials are publicly available for review and reuse."
    },
    {
      "index_id": 16,
      "parent_id": 11,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 16,
        "pdf_para_block": {
          "bbox": [
            50,
            545,
            295,
            567
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                545,
                295,
                567
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    545,
                    295,
                    567
                  ],
                  "type": "text",
                  "content": "The source code, data, and/or other artifacts have been made available at https://github.com/sam234990/BookRAG."
                }
              ]
            }
          ],
          "index": 18
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The source code, data, and/or other artifacts have been made available at https://github.com/sam234990/BookRAG.",
        "title_level": -1
      },
      "summary": "The source code, data, and other artifacts for the project \"BookRAG\" are publicly accessible on the GitHub repository at https://github.com/sam234990/BookRAG."
    },
    {
      "index_id": 17,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 17,
        "pdf_para_block": {
          "bbox": [
            52,
            577,
            157,
            587
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                52,
                577,
                157,
                587
              ],
              "spans": [
                {
                  "bbox": [
                    52,
                    577,
                    157,
                    587
                  ],
                  "type": "text",
                  "content": "1 INTRODUCTION"
                }
              ]
            }
          ],
          "index": 19
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "1 INTRODUCTION",
        "title_level": 1
      },
      "summary": "This section introduces the BookRAG methodology for document question-answering, outlining its novel two-part BookIndex and agent-based retrieval system to address limitations in linking document structure with semantics and enabling flexible, multi-hop reasoning."
    },
    {
      "index_id": 18,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 18,
        "pdf_para_block": {
          "bbox": [
            50,
            591,
            295,
            635
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                591,
                295,
                635
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    591,
                    295,
                    635
                  ],
                  "type": "text",
                  "content": "Large Language Models (LLMs) such as Qwen 3 [60] and Gemini 2.5 [13] have revolutionized the Question Answering (QA) system [15, 61, 65]. The industry has increasingly adopted LLMs to build QA systems that assist users and reduce manual effort in"
                }
              ]
            }
          ],
          "index": 20
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Large Language Models (LLMs) such as Qwen 3 [60] and Gemini 2.5 [13] have revolutionized the Question Answering (QA) system [15, 61, 65]. The industry has increasingly adopted LLMs to build QA systems that assist users and reduce manual effort in",
        "title_level": -1
      },
      "summary": "Large Language Models like Qwen 3 and Gemini 2.5 are revolutionizing Question Answering systems, leading to widespread industry adoption for building user-assistance tools and reducing manual labor."
    },
    {
      "index_id": 19,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 19,
        "pdf_para_block": {
          "bbox": [
            50,
            643,
            294,
            701
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                643,
                294,
                701
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    643,
                    294,
                    701
                  ],
                  "type": "text",
                  "content": "This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment.  \nProceedings of the VLDB Endowment, Vol. 19, No. 1 ISSN 2150-8097.  \ndoi:XX.XX/XXX.XX"
                }
              ]
            }
          ],
          "index": 21
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment.  \nProceedings of the VLDB Endowment, Vol. 19, No. 1 ISSN 2150-8097.  \ndoi:XX.XX/XXX.XX many applications [65, 67], such as financial auditing [29, 37], legal compliance [8], and scientific discovery [56]. However, directly relying on LLMs may lead to missing domain knowledge and generating outdated or unsupported information. To address these issues, Retrieval-Augmented Generation (RAG) has been widely adopted [17, 22] by retrieving relevant domain knowledge from external sources and using it to guide the LLM during response generation. On the other hand, in real-world enterprise scenarios, domain knowledge is often stored in long-form documents, such as technical handbooks, API reference manuals, and operational guidebooks [49]. A notable feature of such documents is that they follow the structure of books, characterized by intricate layouts and rigorous logical hierarchies (e.g., explicit tables of contents, nested chapters, and multi-level sections). In this paper, we aim to design an effective RAG system for QA over long and highly structured documents.",
        "title_level": -1
      },
      "summary": "Retrieval-Augmented Generation (RAG) is widely used to enhance Large Language Models (LLMs) by incorporating external domain knowledge, addressing issues like missing expertise and outdated information. This is particularly important for enterprise applications, where essential knowledge is often contained in long, structured documents like manuals and guidebooks. This paper focuses on designing an effective RAG system specifically for Question Answering (QA) over these long-form, hierarchically structured documents."
    },
    {
      "index_id": 20,
      "parent_id": 17,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 20,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            317,
            186,
            559,
            415
          ],
          "blocks": [
            {
              "bbox": [
                317,
                186,
                559,
                415
              ],
              "lines": [
                {
                  "bbox": [
                    317,
                    186,
                    559,
                    415
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        186,
                        559,
                        415
                      ],
                      "type": "image",
                      "image_path": "f40a8e134ff67e3c0c657345ee1afc5bfa6d85014b2d7dfad9cfaa160f94c60c.jpg"
                    }
                  ]
                }
              ],
              "index": 22,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                314,
                418,
                559,
                441
              ],
              "lines": [
                {
                  "bbox": [
                    314,
                    418,
                    559,
                    441
                  ],
                  "spans": [
                    {
                      "bbox": [
                        314,
                        418,
                        559,
                        441
                      ],
                      "type": "text",
                      "content": "Figure 1: Comparison of existing methods and BookRAG for complex document QA."
                    }
                  ]
                }
              ],
              "index": 23,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 22
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/f40a8e134ff67e3c0c657345ee1afc5bfa6d85014b2d7dfad9cfaa160f94c60c.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Figure 1: Comparison of existing methods and BookRAG for complex document QA.",
        "footnote": "",
        "table_body": null,
        "content": "Figure 1: Comparison of existing methods and BookRAG for complex document QA.",
        "title_level": -1
      },
      "summary": "BookRAG outperforms existing methods in complex document question answering, as visually demonstrated in a comparative figure."
    },
    {
      "index_id": 21,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 0,
        "page_path": null,
        "pdf_id": 21,
        "pdf_para_block": {
          "bbox": [
            313,
            622,
            561,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                622,
                561,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    622,
                    561,
                    710
                  ],
                  "type": "text",
                  "content": "- Prior works. The existing RAG approaches for document-level QA generally fall into two paradigms, as illustrated in Figure 1. The first paradigm relies on OCR (Optical Character Recognition) to convert the document into plain text, after which any text-based RAG method can be directly applied. Among text-based RAG methods, state-of-the-art approaches increasingly adopt graph-based RAG [6, 62, 66], where graph data serves as an external knowledge source because it captures rich semantic information and the"
                }
              ]
            }
          ],
          "index": 25
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Prior works. The existing RAG approaches for document-level QA generally fall into two paradigms, as illustrated in Figure 1. The first paradigm relies on OCR (Optical Character Recognition) to convert the document into plain text, after which any text-based RAG method can be directly applied. Among text-based RAG methods, state-of-the-art approaches increasingly adopt graph-based RAG [6, 62, 66], where graph data serves as an external knowledge source because it captures rich semantic information and the relational structure between entities. As shown in Table 1, two representative methods are GraphRAG [16] and RAPTOR [45]. Specifically, GraphRAG first constructs a knowledge graph (KG) from the textual corpus, and then applies the Leiden community detection algorithm [51] to obtain hierarchical clusters. Summaries are generated for each community, providing a comprehensive, global overview of the entire corpus. RAPTOR builds a recursive tree structure by iteratively clustering document chunks and summarizing them at each level, enabling the model to capture both fine-grained and high-level semantic information across the corpus.",
        "title_level": -1
      },
      "summary": "Existing document-level QA methods using RAG primarily follow two paradigms: one converts documents to plain text via OCR for standard text-based RAG, and the other employs graph-based RAG to leverage structured knowledge. Graph-based approaches, such as GraphRAG and RAPTOR, use knowledge graphs or recursive tree structures to capture semantic relationships and hierarchical information, enabling both detailed and high-level understanding of the corpus."
    },
    {
      "index_id": 22,
      "parent_id": 17,
      "type": "NodeType.TABLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 22,
        "pdf_para_block": {
          "type": "table",
          "bbox": [
            52,
            92,
            558,
            191
          ],
          "blocks": [
            {
              "bbox": [
                168,
                79,
                441,
                91
              ],
              "lines": [
                {
                  "bbox": [
                    168,
                    79,
                    441,
                    91
                  ],
                  "spans": [
                    {
                      "bbox": [
                        168,
                        79,
                        441,
                        91
                      ],
                      "type": "text",
                      "content": "Table 1: Comparison of representative methods and our BookRAG."
                    }
                  ]
                }
              ],
              "index": 0,
              "angle": 0,
              "type": "table_caption"
            },
            {
              "bbox": [
                52,
                92,
                558,
                191
              ],
              "lines": [
                {
                  "bbox": [
                    52,
                    92,
                    558,
                    191
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        92,
                        558,
                        191
                      ],
                      "type": "table",
                      "html": "<table><tr><td>Type</td><td>Representative Method</td><td>Core Feature</td><td>Multi-hop Reasoning</td><td>Document Parsing</td><td>Query Workflow</td></tr><tr><td rowspan=\"2\">Graph-based</td><td>RAPTOR [45]</td><td>Recursive summarization</td><td>✓</td><td>✗</td><td>Static</td></tr><tr><td>GraphRAG [16]</td><td>Global community detection</td><td>✓</td><td>✗</td><td>Static</td></tr><tr><td rowspan=\"2\">Layout segmented</td><td>MM-Vanilla</td><td>Multi-modal retrieval</td><td>✗</td><td>✓</td><td>Static</td></tr><tr><td>DocETL [47]</td><td>LLM-based document processing pipeline</td><td>✗</td><td>✓</td><td>Manual</td></tr><tr><td>Doc-Native</td><td>BookRAG (Ours)</td><td>Structure-award Index &amp; Agent-based retrieval</td><td>✓</td><td>✓</td><td>Dynamic</td></tr></table>",
                      "image_path": "44593393730e88860c6e936a79ed28b9fbcfe0170a42dbc7f2e1862f864dafee.jpg"
                    }
                  ]
                }
              ],
              "index": 1,
              "angle": 0,
              "type": "table_body"
            }
          ],
          "index": 1
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/44593393730e88860c6e936a79ed28b9fbcfe0170a42dbc7f2e1862f864dafee.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Table 1: Comparison of representative methods and our BookRAG.",
        "footnote": "",
        "table_body": "<table><tr><td>Type</td><td>Representative Method</td><td>Core Feature</td><td>Multi-hop Reasoning</td><td>Document Parsing</td><td>Query Workflow</td></tr><tr><td rowspan=\"2\">Graph-based</td><td>RAPTOR [45]</td><td>Recursive summarization</td><td>✓</td><td>✗</td><td>Static</td></tr><tr><td>GraphRAG [16]</td><td>Global community detection</td><td>✓</td><td>✗</td><td>Static</td></tr><tr><td rowspan=\"2\">Layout segmented</td><td>MM-Vanilla</td><td>Multi-modal retrieval</td><td>✗</td><td>✓</td><td>Static</td></tr><tr><td>DocETL [47]</td><td>LLM-based document processing pipeline</td><td>✗</td><td>✓</td><td>Manual</td></tr><tr><td>Doc-Native</td><td>BookRAG (Ours)</td><td>Structure-award Index &amp; Agent-based retrieval</td><td>✓</td><td>✓</td><td>Dynamic</td></tr></table>",
        "content": "Table 1: Comparison of representative methods and our BookRAG.",
        "title_level": -1
      },
      "summary": "BookRAG is a document-native method that uniquely combines structure-aware indexing with agent-based retrieval, enabling multi-hop reasoning, effective document parsing, and a dynamic query workflow. This distinguishes it from other approaches: graph-based methods like RAPTOR and GraphRAG support multi-hop reasoning but lack document parsing; layout-segmented methods like MM-Vanilla and DocETL handle document parsing but lack multi-hop reasoning or rely on a manual workflow."
    },
    {
      "index_id": 23,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 23,
        "pdf_para_block": {
          "bbox": [
            50,
            317,
            295,
            469
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                317,
                295,
                469
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    317,
                    295,
                    469
                  ],
                  "type": "text",
                  "content": "In contrast, the second paradigm, layout-aware segmentation [5, 52], first parses the document into structured blocks that preserve the original layout and information of the document, such as paragraphs, tables, figures, or equations. By doing so, it not only avoids the fixed chunk size used in the first paradigm, which often leads to fragmented information, but also retains document-native structural information. These blocks often exhibit multimodal characteristics, and a typical approach is to apply multimodal retrieval to obtain relevant content for answering queries. Recently, a state-of-the-art method in this category, DocETL [47], provides a declarative interface that allows users to manually define LLM-based processing pipelines to analyze the retrieved blocks. These pipelines consist of LLM-powered operations combined with task-specific optimizations."
                }
              ]
            }
          ],
          "index": 3
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "In contrast, the second paradigm, layout-aware segmentation [5, 52], first parses the document into structured blocks that preserve the original layout and information of the document, such as paragraphs, tables, figures, or equations. By doing so, it not only avoids the fixed chunk size used in the first paradigm, which often leads to fragmented information, but also retains document-native structural information. These blocks often exhibit multimodal characteristics, and a typical approach is to apply multimodal retrieval to obtain relevant content for answering queries. Recently, a state-of-the-art method in this category, DocETL [47], provides a declarative interface that allows users to manually define LLM-based processing pipelines to analyze the retrieved blocks. These pipelines consist of LLM-powered operations combined with task-specific optimizations.",
        "title_level": -1
      },
      "summary": "Layout-aware segmentation improves document analysis by first parsing documents into structured blocks that preserve original layout and information, such as paragraphs and tables. This avoids the fragmented information caused by fixed chunk sizes and retains document-native structure. These multimodal blocks are then processed using multimodal retrieval to answer queries. A recent state-of-the-art method, DocETL, offers a declarative interface for users to manually define LLM-based processing pipelines, combining LLM-powered operations with task-specific optimizations."
    },
    {
      "index_id": 24,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 24,
        "pdf_para_block": {
          "bbox": [
            50,
            471,
            295,
            668
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                471,
                295,
                668
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    471,
                    295,
                    668
                  ],
                  "type": "text",
                  "content": "- Limitations of existing works. However, these methods suffer from two fundamental limitations (L for short): L1: Failure to capture the deep connection of document structure and semantics. Text-based approaches cannot capture the structural layout of the document, resulting in the loss of important relationships stored in the hierarchical blocks, such as tables nested within a specific section. While layout-segmented methods preserve document structure, they cannot capture the relationships between different blocks in the document, which limits their capability for multi-hop reasoning across these blocks and ultimately affects their overall performance. L2: Static of query workflows. In real-world QA scenarios, user queries are highly heterogeneous, ranging from simple keyword lookups to complex multi-hop questions that require synthesizing evidence scattered across different parts of the document. Applying a uniform strategy, such as static or manually predefined workflows, to diverse needs is inefficient; for example, complex queries often require question decomposition, whereas simple queries do not."
                }
              ]
            }
          ],
          "index": 4
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Limitations of existing works. However, these methods suffer from two fundamental limitations (L for short): L1: Failure to capture the deep connection of document structure and semantics. Text-based approaches cannot capture the structural layout of the document, resulting in the loss of important relationships stored in the hierarchical blocks, such as tables nested within a specific section. While layout-segmented methods preserve document structure, they cannot capture the relationships between different blocks in the document, which limits their capability for multi-hop reasoning across these blocks and ultimately affects their overall performance. L2: Static of query workflows. In real-world QA scenarios, user queries are highly heterogeneous, ranging from simple keyword lookups to complex multi-hop questions that require synthesizing evidence scattered across different parts of the document. Applying a uniform strategy, such as static or manually predefined workflows, to diverse needs is inefficient; for example, complex queries often require question decomposition, whereas simple queries do not.",
        "title_level": -1
      },
      "summary": "Existing document question-answering methods have two key limitations: they fail to adequately link document structure with semantics, and they use inflexible query workflows. Specifically, text-based approaches lose structural relationships (like nested tables), while layout-aware methods cannot connect different blocks for multi-hop reasoning. Additionally, applying a uniform strategy to all queries—from simple lookups to complex multi-hop questions—is inefficient."
    },
    {
      "index_id": 25,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 25,
        "pdf_para_block": {
          "bbox": [
            50,
            668,
            295,
            700
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                668,
                295,
                700
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    668,
                    295,
                    700
                  ],
                  "type": "text",
                  "content": "- Our technical contributions. To bridge this gap, we introduce BookRAG, the first retrieval-augmented generation method built upon a document-native BookIndex, designed to document"
                }
              ]
            }
          ],
          "index": 5
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Our technical contributions. To bridge this gap, we introduce BookRAG, the first retrieval-augmented generation method built upon a document-native BookIndex, designed to document QA tasks. Specifically, to capture the deep connection of the relation in the document, BookIndex organizes information through two complementary structures. First, to preserve the document's native logical hierarchy, we organize the parsed content blocks into a hierarchical tree structure, which serves as the role of its table of contents. Second, to capture the intricate relations within these blocks, we construct a KG containing fine-grained entities. Finally, we unify these two structures by mapping the KG entities to their corresponding tree nodes.",
        "title_level": -1
      },
      "summary": "BookRAG is a new retrieval-augmented generation method for document question-answering that uses a unique two-part BookIndex to capture a document's deep structure and relations. Its index combines a hierarchical tree, which mirrors the document's native logical layout like a table of contents, with a knowledge graph (KG) of fine-grained entities to represent intricate connections within the content. These two structures are unified by mapping the KG entities to their corresponding nodes in the tree."
    },
    {
      "index_id": 26,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 26,
        "pdf_para_block": {
          "bbox": [
            313,
            306,
            559,
            394
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                306,
                559,
                394
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    306,
                    559,
                    394
                  ],
                  "type": "text",
                  "content": "However, effective multi-hop reasoning on the graph relies on a high-quality KG [62, 66], which is often compromised by entity ambiguity (e.g., distinct entities with names like \"LLM\" and \"Large Language Model\"). To address this, we propose a novel gradient-based entity resolution method that analyzes the similarity distribution of candidate entities. By identifying sharp drops in similarity scores, we can efficiently distinguish and merge coreferent entities, thereby ensuring graph connectivity and enhancing reasoning capabilities."
                }
              ]
            }
          ],
          "index": 7
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "However, effective multi-hop reasoning on the graph relies on a high-quality KG [62, 66], which is often compromised by entity ambiguity (e.g., distinct entities with names like \"LLM\" and \"Large Language Model\"). To address this, we propose a novel gradient-based entity resolution method that analyzes the similarity distribution of candidate entities. By identifying sharp drops in similarity scores, we can efficiently distinguish and merge coreferent entities, thereby ensuring graph connectivity and enhancing reasoning capabilities.",
        "title_level": -1
      },
      "summary": "A novel gradient-based method resolves entity ambiguity in knowledge graphs by analyzing similarity distributions to identify and merge coreferent entities, ensuring graph connectivity and improving multi-hop reasoning."
    },
    {
      "index_id": 27,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 27,
        "pdf_para_block": {
          "bbox": [
            313,
            394,
            559,
            481
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                394,
                559,
                481
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    394,
                    559,
                    481
                  ],
                  "type": "text",
                  "content": "Building upon the BookIndex, we address the static of query workflows (L2) by implementing an agent-based retrieval. Specifically, our agent first classifies user queries based on their intent and complexity, and then dynamically generates tailored retrieval workflows. Grounded in Information Foraging Theory [42], our retrieval process mimics foraging by using Selector to narrow down the search space via information scents and Reasoner to locate highly relevant evidence."
                }
              ]
            }
          ],
          "index": 8
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Building upon the BookIndex, we address the static of query workflows (L2) by implementing an agent-based retrieval. Specifically, our agent first classifies user queries based on their intent and complexity, and then dynamically generates tailored retrieval workflows. Grounded in Information Foraging Theory [42], our retrieval process mimics foraging by using Selector to narrow down the search space via information scents and Reasoner to locate highly relevant evidence.",
        "title_level": -1
      },
      "summary": "An agent-based retrieval system enhances static query workflows by first classifying user intent and complexity, then dynamically generating tailored retrieval processes. This approach, grounded in Information Foraging Theory, uses a Selector to narrow the search space via information scents and a Reasoner to locate highly relevant evidence."
    },
    {
      "index_id": 28,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 28,
        "pdf_para_block": {
          "bbox": [
            313,
            481,
            559,
            568
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                481,
                559,
                568
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    481,
                    559,
                    568
                  ],
                  "type": "text",
                  "content": "We conduct extensive experiments on three widely adopted datasets to validate the effectiveness and efficiency of our BookRAG, comparing it against several state-of-the-art baselines. The experimental results demonstrate that BookRAG consistently achieves superior performance in both retrieval recall and QA accuracy across all datasets. Furthermore, our detailed analysis validates the critical contributions of our key features, such as the high-quality KG and the agent-based retrieval mechanism."
                }
              ]
            }
          ],
          "index": 9
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "We conduct extensive experiments on three widely adopted datasets to validate the effectiveness and efficiency of our BookRAG, comparing it against several state-of-the-art baselines. The experimental results demonstrate that BookRAG consistently achieves superior performance in both retrieval recall and QA accuracy across all datasets. Furthermore, our detailed analysis validates the critical contributions of our key features, such as the high-quality KG and the agent-based retrieval mechanism.",
        "title_level": -1
      },
      "summary": "BookRAG outperforms existing methods in retrieval and question-answering tasks across three standard datasets. Its superior performance is attributed to key features like a high-quality knowledge graph and an agent-based retrieval mechanism."
    },
    {
      "index_id": 29,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 29,
        "pdf_para_block": {
          "bbox": [
            324,
            570,
            460,
            579
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                324,
                570,
                460,
                579
              ],
              "spans": [
                {
                  "bbox": [
                    324,
                    570,
                    460,
                    579
                  ],
                  "type": "text",
                  "content": "We summarize our contributions as:"
                }
              ]
            }
          ],
          "index": 10
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "We summarize our contributions as:",
        "title_level": -1
      },
      "summary": "The content provided is incomplete and does not contain any substantive information to summarize."
    },
    {
      "index_id": 30,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 30,
        "pdf_para_block": {
          "bbox": [
            334,
            589,
            559,
            710
          ],
          "type": "list",
          "angle": 0,
          "index": 14,
          "blocks": [
            {
              "bbox": [
                334,
                589,
                559,
                632
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    334,
                    589,
                    559,
                    632
                  ],
                  "spans": [
                    {
                      "bbox": [
                        334,
                        589,
                        559,
                        632
                      ],
                      "type": "text",
                      "content": "- We introduce BookRAG, a novel method that constructs a document-native BookIndex by integrating a hierarchical tree of document layout blocks with a KG storing fine-grained entity relations."
                    }
                  ]
                }
              ],
              "index": 11
            },
            {
              "bbox": [
                334,
                633,
                559,
                676
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    334,
                    633,
                    559,
                    676
                  ],
                  "spans": [
                    {
                      "bbox": [
                        334,
                        633,
                        559,
                        676
                      ],
                      "type": "text",
                      "content": "- We propose an Agent-based Retrieval approach inspired by Information Foraging Theory, which dynamically classifies queries and configures optimal retrieval workflows to locate highly relevant evidence within documents."
                    }
                  ]
                }
              ],
              "index": 12
            },
            {
              "bbox": [
                334,
                677,
                559,
                710
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    334,
                    677,
                    559,
                    710
                  ],
                  "spans": [
                    {
                      "bbox": [
                        334,
                        677,
                        559,
                        710
                      ],
                      "type": "text",
                      "content": "- Extensive experiments on multiple benchmarks show that BookRAG significantly outperforms existing baselines, attaining state-of-the-art performance in solving complex"
                    }
                  ]
                }
              ],
              "index": 13
            }
          ],
          "sub_type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The global population is projected to reach 9.7 billion by 2050, with the most significant growth occurring in Africa and Asia. This growth will be unevenly distributed, as populations in Europe and North America are expected to remain stable or decline. The primary driver of this increase is a continued high birth rate in developing regions, coupled with declining global mortality rates. This demographic shift presents major challenges for sustainable development, resource management, and urban infrastructure."
    },
    {
      "index_id": 31,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 1,
        "page_path": null,
        "pdf_id": 31,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            308,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                308,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    308,
                    719
                  ],
                  "type": "text",
                  "content": "2"
                }
              ]
            }
          ],
          "index": 15
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "2",
        "title_level": -1
      },
      "summary": "The content is a single digit: 2."
    },
    {
      "index_id": 32,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 32,
        "pdf_para_block": {
          "bbox": [
            79,
            85,
            295,
            107
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                79,
                85,
                295,
                107
              ],
              "spans": [
                {
                  "bbox": [
                    79,
                    85,
                    295,
                    107
                  ],
                  "type": "text",
                  "content": "document QA tasks while maintaining competitive efficiency."
                }
              ]
            }
          ],
          "index": 0
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "document QA tasks while maintaining competitive efficiency.",
        "title_level": -1
      },
      "summary": "Document QA tasks can be performed effectively without sacrificing speed or performance."
    },
    {
      "index_id": 33,
      "parent_id": 17,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 33,
        "pdf_para_block": {
          "bbox": [
            50,
            109,
            295,
            186
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                109,
                295,
                186
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    109,
                    295,
                    186
                  ],
                  "type": "text",
                  "content": "Outline. We review related work in Section 2. Section 3 introduces the problem formulation, IFT, and RAG workflow. In Section 4, we present the structure of our BookIndex and its construction. Section 5 presents our agent-based retrieval, elaborating on the query classification and operators used in the structured execution of BookRAG. We present the experimental results and detailed analysis in Section 6, and conclude the paper in Section 7."
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Outline. We review related work in Section 2. Section 3 introduces the problem formulation, IFT, and RAG workflow. In Section 4, we present the structure of our BookIndex and its construction. Section 5 presents our agent-based retrieval, elaborating on the query classification and operators used in the structured execution of BookRAG. We present the experimental results and detailed analysis in Section 6, and conclude the paper in Section 7.",
        "title_level": -1
      },
      "summary": "This paper outlines a research study, beginning with a review of related work. It then introduces the problem formulation, Implicit Function Theorem (IFT), and a Retrieval-Augmented Generation (RAG) workflow. The core of the work involves constructing a specialized \"BookIndex\" and developing an agent-based retrieval system that uses query classification and structured operators for execution. The paper concludes with experimental results, analysis, and a final conclusion."
    },
    {
      "index_id": 34,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 34,
        "pdf_para_block": {
          "bbox": [
            51,
            196,
            159,
            207
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                196,
                159,
                207
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    196,
                    159,
                    207
                  ],
                  "type": "text",
                  "content": "2 RELATED WORK"
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "2 RELATED WORK",
        "title_level": 1
      },
      "summary": "This section reviews existing research on the application of large language models (LLMs) for document analysis and examines modern Retrieval-Augmented Generation (RAG) approaches, including their advancements and integration into data pipelines."
    },
    {
      "index_id": 35,
      "parent_id": 34,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 35,
        "pdf_para_block": {
          "bbox": [
            50,
            211,
            295,
            232
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                211,
                295,
                232
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    211,
                    295,
                    232
                  ],
                  "type": "text",
                  "content": "In this section, we review the related works, including LLM in document analysis and the modern representative RAG approaches."
                }
              ]
            }
          ],
          "index": 3
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "In this section, we review the related works, including LLM in document analysis and the modern representative RAG approaches.",
        "title_level": -1
      },
      "summary": "This section reviews existing research on using large language models (LLMs) for document analysis and examines modern, representative Retrieval-Augmented Generation (RAG) approaches."
    },
    {
      "index_id": 36,
      "parent_id": 34,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 36,
        "pdf_para_block": {
          "bbox": [
            50,
            233,
            295,
            451
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                233,
                295,
                451
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    233,
                    295,
                    451
                  ],
                  "type": "text",
                  "content": "- LLM in document analysis. Recent advances in LLMs have offered opportunities to leverage LLMs in document data analysis. Due to the robust semantic reasoning capabilities of LLMs, there is an increasing number of works focusing on transferring unstructured documents (e.g., HTML, PDFs, and raw text) into structured formats, such as relational tables [1, 7, 25, 38]. For example, Evaporate [1] utilizes LLMs to synthesize extraction code, enabling cost-effective conversion of semi-structured web documents into structured databases without heavy manual annotation. In addition, several LLM-based document analysis systems have been proposed to equip standard data pipelines with semantic understanding [28, 40, 47, 53]. For instance, LOTUS [40] extends the relational model with semantic operators, allowing users to execute SQL-like queries with LLM-powered predicates (e.g., filter, join) over unstructured text corpora. Similarly, DocETL [47] introduces an agentic framework to optimize complex information extraction tasks. Furthermore, another line of research proposes to directly analyze or parse documents by viewing the document pages as images, thereby preserving critical layout and visual information [26, 31, 54]."
                }
              ]
            }
          ],
          "index": 4
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-LLM in document analysis. Recent advances in LLMs have offered opportunities to leverage LLMs in document data analysis. Due to the robust semantic reasoning capabilities of LLMs, there is an increasing number of works focusing on transferring unstructured documents (e.g., HTML, PDFs, and raw text) into structured formats, such as relational tables [1, 7, 25, 38]. For example, Evaporate [1] utilizes LLMs to synthesize extraction code, enabling cost-effective conversion of semi-structured web documents into structured databases without heavy manual annotation. In addition, several LLM-based document analysis systems have been proposed to equip standard data pipelines with semantic understanding [28, 40, 47, 53]. For instance, LOTUS [40] extends the relational model with semantic operators, allowing users to execute SQL-like queries with LLM-powered predicates (e.g., filter, join) over unstructured text corpora. Similarly, DocETL [47] introduces an agentic framework to optimize complex information extraction tasks. Furthermore, another line of research proposes to directly analyze or parse documents by viewing the document pages as images, thereby preserving critical layout and visual information [26, 31, 54].",
        "title_level": -1
      },
      "summary": "Recent advances in Large Language Models (LLMs) are transforming document analysis by converting unstructured documents into structured data and enhancing data pipelines with semantic understanding. LLMs are used to extract and structure information from formats like HTML and PDFs into relational tables, reducing the need for manual annotation. Systems like LOTUS and DocETL integrate LLMs into data pipelines to enable semantic queries and optimize complex extraction tasks. Additionally, some approaches treat documents as images to preserve layout and visual information during analysis."
    },
    {
      "index_id": 37,
      "parent_id": 34,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 37,
        "pdf_para_block": {
          "bbox": [
            50,
            452,
            295,
            606
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                452,
                295,
                606
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    452,
                    295,
                    606
                  ],
                  "type": "text",
                  "content": "- RAG approaches. RAG has been proven to excel in many tasks, including open-ended question answering [24, 48], programming context [9, 10], SQL rewrite [30, 50], and data cleaning [35, 36, 43]. The naive RAG technique relies on retrieving query-relevant contexts from external knowledge bases to mitigate the \"hallucination\" of LLMs. Recently, many RAG approaches [16, 18, 19, 21, 27, 32, 32, 45, 55, 58, 66] have adopted graph structures to organize the information and relationships within documents, achieving improved overall retrieval performance. For more details, please refer to the recent survey of graph-based RAG methods [41]. Besides, the Agentic RAG paradigm has been widely studied, employing autonomous agents to dynamically orchestrate and refine the RAG pipeline, thus significantly boosting the reasoning robustness and generation fidelity [2, 23, 59]."
                }
              ]
            }
          ],
          "index": 5
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-RAG approaches. RAG has been proven to excel in many tasks, including open-ended question answering [24, 48], programming context [9, 10], SQL rewrite [30, 50], and data cleaning [35, 36, 43]. The naive RAG technique relies on retrieving query-relevant contexts from external knowledge bases to mitigate the \"hallucination\" of LLMs. Recently, many RAG approaches [16, 18, 19, 21, 27, 32, 32, 45, 55, 58, 66] have adopted graph structures to organize the information and relationships within documents, achieving improved overall retrieval performance. For more details, please refer to the recent survey of graph-based RAG methods [41]. Besides, the Agentic RAG paradigm has been widely studied, employing autonomous agents to dynamically orchestrate and refine the RAG pipeline, thus significantly boosting the reasoning robustness and generation fidelity [2, 23, 59].",
        "title_level": -1
      },
      "summary": "RAG (Retrieval-Augmented Generation) effectively enhances large language models by retrieving relevant external information to reduce inaccuracies. It excels in tasks like open-ended question answering, programming, SQL rewriting, and data cleaning. Recent advancements incorporate graph structures to better organize document relationships, improving retrieval performance. Additionally, Agentic RAG uses autonomous agents to dynamically manage the RAG process, significantly boosting reasoning and output reliability."
    },
    {
      "index_id": 38,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 38,
        "pdf_para_block": {
          "bbox": [
            51,
            615,
            156,
            625
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                615,
                156,
                625
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    615,
                    156,
                    625
                  ],
                  "type": "text",
                  "content": "3 PRELIMINARIES"
                }
              ]
            }
          ],
          "index": 6
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "3 PRELIMINARIES",
        "title_level": 1
      },
      "summary": "This section establishes the research problem of complex document QA, introduces Information Foraging Theory as its theoretical foundation, and proposes a novel RAG workflow that incorporates document hierarchy."
    },
    {
      "index_id": 39,
      "parent_id": 38,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 39,
        "pdf_para_block": {
          "bbox": [
            50,
            630,
            295,
            663
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                630,
                295,
                663
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    630,
                    295,
                    663
                  ],
                  "type": "text",
                  "content": "This section formalizes the research problem of complex document QA, introduces the foundational Information Foraging Theory (IFT), and briefly reviews the general workflow of RAG systems"
                }
              ]
            }
          ],
          "index": 7
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "This section formalizes the research problem of complex document QA, introduces the foundational Information Foraging Theory (IFT), and briefly reviews the general workflow of RAG systems",
        "title_level": -1
      },
      "summary": "This section outlines the research problem of complex document question answering (QA), establishes Information Foraging Theory as its foundational framework, and provides a concise overview of the standard workflow for Retrieval-Augmented Generation (RAG) systems."
    },
    {
      "index_id": 40,
      "parent_id": 38,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 40,
        "pdf_para_block": {
          "bbox": [
            51,
            673,
            186,
            683
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                673,
                186,
                683
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    673,
                    186,
                    683
                  ],
                  "type": "text",
                  "content": "3.1 Problem Formulation"
                }
              ]
            }
          ],
          "index": 8
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "3.1 Problem Formulation",
        "title_level": 2
      },
      "summary": "This section formulates the core research problem of developing a method to answer queries by navigating both the sequential layout and hierarchical structure of long-form documents."
    },
    {
      "index_id": 41,
      "parent_id": 40,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 41,
        "pdf_para_block": {
          "bbox": [
            50,
            687,
            295,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                687,
                295,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    687,
                    295,
                    710
                  ],
                  "type": "text",
                  "content": "We study the problem of Question Answering (QA) over complex documents, which aims to answer user queries based on long-form"
                }
              ]
            }
          ],
          "index": 9
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "We study the problem of Question Answering (QA) over complex documents, which aims to answer user queries based on long-form documents [5, 11, 33]. Formally, a document  $D$  is represented as a sequence of  $N$  pages,  $D = \\{P_{i}\\}_{i = 1}^{N}$ . These pages collectively contain a sequence of content blocks  $\\mathcal{B} = \\{b_j\\}_{j = 1}^M$ , where each block  $b_{j}$  represents a distinct element (e.g., text segment, section header, table, or image) organized within a logical chapter hierarchy. Given a user query  $q$ , the goal is to generate an accurate answer  $A$ , ideally grounded in a specific set of evidence blocks  $E\\subset \\mathcal{B}$ . The task is formulated as developing a method  $S$  that maps the structured document and the query to the final answer: \n$$\nA = \\mathcal {S} (D, q) \\tag {1}\n$$\n where  $S$  should navigate both the sequential page content and the logical hierarchy of  $D$  to synthesize the response.",
        "title_level": -1
      },
      "summary": "This research addresses Question Answering (QA) for complex, long-form documents. These documents are structured as a sequence of pages containing various content blocks (like text, headers, tables, or images) organized in a logical hierarchy. The core problem is to develop a method that, given a user query, can navigate both the sequential page layout and this internal hierarchy to generate an accurate answer grounded in specific evidence from the document."
    },
    {
      "index_id": 42,
      "parent_id": 40,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 42,
        "pdf_para_block": {
          "bbox": [
            413,
            196,
            559,
            208
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                413,
                196,
                559,
                208
              ],
              "spans": [
                {
                  "bbox": [
                    413,
                    196,
                    559,
                    208
                  ],
                  "type": "interline_equation",
                  "content": "A = \\mathcal {S} (D, q) \\tag {1}",
                  "image_path": "f03d4a8aebc40540998e3ca0dee91ee42f25120a406e1b7836085c2f9f191380.jpg"
                }
              ]
            }
          ],
          "index": 11
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nA = \\mathcal {S} (D, q) \\tag {1}\n$$\n",
        "title_level": -1
      },
      "summary": "The equation \\( A = \\mathcal{S}(D, q) \\) defines a variable \\( A \\) as a function \\( \\mathcal{S} \\) of two parameters: \\( D \\) and \\( q \\)."
    },
    {
      "index_id": 43,
      "parent_id": 38,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 43,
        "pdf_para_block": {
          "bbox": [
            315,
            255,
            491,
            268
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                315,
                255,
                491,
                268
              ],
              "spans": [
                {
                  "bbox": [
                    315,
                    255,
                    491,
                    268
                  ],
                  "type": "text",
                  "content": "3.2 Information Foraging Theory"
                }
              ]
            }
          ],
          "index": 13
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "3.2 Information Foraging Theory",
        "title_level": 2
      },
      "summary": "This section introduces Information Foraging Theory, which models how users, particularly experts, efficiently locate and extract needed information by following cues like keywords (information scent) to navigate between content areas."
    },
    {
      "index_id": 44,
      "parent_id": 43,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 44,
        "pdf_para_block": {
          "bbox": [
            314,
            269,
            559,
            356
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                269,
                559,
                356
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    269,
                    559,
                    356
                  ],
                  "type": "text",
                  "content": "Information Foraging Theory (IFT) [42] provides a framework for understanding information access as a process analogous to animal foraging. It suggests that users follow cues, known as information scent (e.g., keywords or icons), to navigate between clusters of content, known as information patches (e.g., sections in handbooks). The goal is to maximize the rate of valuable information gain while minimizing effort, guiding the decision to either stay within a patch or seek a new one."
                }
              ]
            }
          ],
          "index": 14
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Information Foraging Theory (IFT) [42] provides a framework for understanding information access as a process analogous to animal foraging. It suggests that users follow cues, known as information scent (e.g., keywords or icons), to navigate between clusters of content, known as information patches (e.g., sections in handbooks). The goal is to maximize the rate of valuable information gain while minimizing effort, guiding the decision to either stay within a patch or seek a new one.",
        "title_level": -1
      },
      "summary": "Information Foraging Theory models how users seek information by following cues like keywords (information scent) to move between content areas (information patches), aiming to maximize valuable information gained while minimizing effort, which guides decisions to continue in an area or move to a new one."
    },
    {
      "index_id": 45,
      "parent_id": 43,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 45,
        "pdf_para_block": {
          "bbox": [
            314,
            357,
            558,
            423
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                357,
                558,
                423
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    357,
                    558,
                    423
                  ],
                  "type": "text",
                  "content": "Consider experts seeking a solution to a specific problem within a large technical handbook. They first extract key terms related to the problem, which act as information scent. This scent guides them to navigate towards one or more promising sections (the information patches). Within these patches, they analyze the diverse content to extract the precise knowledge required to formulate a final answer"
                }
              ]
            }
          ],
          "index": 15
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Consider experts seeking a solution to a specific problem within a large technical handbook. They first extract key terms related to the problem, which act as information scent. This scent guides them to navigate towards one or more promising sections (the information patches). Within these patches, they analyze the diverse content to extract the precise knowledge required to formulate a final answer",
        "title_level": -1
      },
      "summary": "Experts solve complex problems by first identifying key terms as an \"information scent\" to locate relevant sections in technical handbooks, then analyze those sections to extract the precise knowledge needed for a solution."
    },
    {
      "index_id": 46,
      "parent_id": 38,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 46,
        "pdf_para_block": {
          "bbox": [
            315,
            436,
            417,
            447
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                315,
                436,
                417,
                447
              ],
              "spans": [
                {
                  "bbox": [
                    315,
                    436,
                    417,
                    447
                  ],
                  "type": "text",
                  "content": "3.3 RAG workflow"
                }
              ]
            }
          ],
          "index": 16
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "3.3 RAG workflow",
        "title_level": 2
      },
      "summary": "This section introduces a novel Retrieval-Augmented Generation (RAG) workflow that integrates document hierarchy into the retrieval structure, moving beyond traditional content-only indexing."
    },
    {
      "index_id": 47,
      "parent_id": 46,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 47,
        "pdf_para_block": {
          "bbox": [
            313,
            451,
            559,
            572
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                451,
                559,
                572
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    451,
                    559,
                    572
                  ],
                  "type": "text",
                  "content": "Retrieval-Augmented Generation (RAG) systems typically operate in a two-phase framework [6, 16, 41]. In the Offline Indexing phase, unstructured corpus data is organized into a structured index, which can take various forms such as vector databases or KG [66]. Subsequently, in the Online Retrieval phase, the system retrieves relevant components (e.g., text chunks or subgraphs) based on the user query "
                },
                {
                  "bbox": [
                    313,
                    451,
                    559,
                    572
                  ],
                  "type": "inline_equation",
                  "content": "q"
                },
                {
                  "bbox": [
                    313,
                    451,
                    559,
                    572
                  ],
                  "type": "text",
                  "content": " to inform the LLM's generation. However, these general workflows often treat the index as a structure derived purely from content, potentially detaching it from the document's original logical hierarchy. In contrast, our approach seeks to deeply integrate these retrieval structures with the document's native tree topology."
                }
              ]
            }
          ],
          "index": 17
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Retrieval-Augmented Generation (RAG) systems typically operate in a two-phase framework [6, 16, 41]. In the Offline Indexing phase, unstructured corpus data is organized into a structured index, which can take various forms such as vector databases or KG [66]. Subsequently, in the Online Retrieval phase, the system retrieves relevant components (e.g., text chunks or subgraphs) based on the user query  $q$  to inform the LLM's generation. However, these general workflows often treat the index as a structure derived purely from content, potentially detaching it from the document's original logical hierarchy. In contrast, our approach seeks to deeply integrate these retrieval structures with the document's native tree topology.",
        "title_level": -1
      },
      "summary": "Retrieval-Augmented Generation (RAG) systems traditionally function in two phases: Offline Indexing organizes data into a structured index, and Online Retrieval uses the query to fetch relevant information for the LLM. However, this standard approach often creates an index based solely on content, separating it from the document's inherent hierarchical structure. The proposed method aims to closely integrate retrieval structures with the document's original tree topology."
    },
    {
      "index_id": 48,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 48,
        "pdf_para_block": {
          "bbox": [
            315,
            585,
            400,
            596
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                315,
                585,
                400,
                596
              ],
              "spans": [
                {
                  "bbox": [
                    315,
                    585,
                    400,
                    596
                  ],
                  "type": "text",
                  "content": "4 BOOKINDEX"
                }
              ]
            }
          ],
          "index": 18
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "4 BOOKINDEX",
        "title_level": 1
      },
      "summary": "This section introduces and details the two-phase construction methodology of BookIndex, a hierarchical structure-aware index that builds a document tree through layout parsing and creates a knowledge graph with gradient-based entity resolution."
    },
    {
      "index_id": 49,
      "parent_id": 48,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 49,
        "pdf_para_block": {
          "bbox": [
            313,
            600,
            559,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                600,
                559,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    600,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": "This section introduces our proposed BookIndex, a hierarchical structure-aware index designed to capture both the explicit logical hierarchy and the intricate entity relations within complex documents. We first formally define the structure of the BookIndex (B). Subsequently, we elaborate on the sequential, two-stage construction process: (1) Tree Construction, which parses the document's layout to establish a hierarchical nodes, each categorized by type; and (2) Graph Construction, which extracts fine-grained entity knowledge from the tree nodes and refines it through a novel gradient-based entity resolution method."
                }
              ]
            }
          ],
          "index": 19
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "This section introduces our proposed BookIndex, a hierarchical structure-aware index designed to capture both the explicit logical hierarchy and the intricate entity relations within complex documents. We first formally define the structure of the BookIndex (B). Subsequently, we elaborate on the sequential, two-stage construction process: (1) Tree Construction, which parses the document's layout to establish a hierarchical nodes, each categorized by type; and (2) Graph Construction, which extracts fine-grained entity knowledge from the tree nodes and refines it through a novel gradient-based entity resolution method.",
        "title_level": -1
      },
      "summary": "BookIndex is a hierarchical structure-aware index designed to model both the explicit logical hierarchy and the complex entity relations within documents. Its construction is a two-stage process: first, Tree Construction parses the document layout to build a hierarchy of typed nodes; second, Graph Construction extracts fine-grained entity knowledge from these nodes and refines it using a novel gradient-based entity resolution method."
    },
    {
      "index_id": 50,
      "parent_id": 48,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 2,
        "page_path": null,
        "pdf_id": 50,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            307,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                307,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    307,
                    719
                  ],
                  "type": "text",
                  "content": "3"
                }
              ]
            }
          ],
          "index": 20
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "3",
        "title_level": -1
      },
      "summary": "3"
    },
    {
      "index_id": 51,
      "parent_id": 48,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 51,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            323,
            84,
            332,
            94
          ],
          "blocks": [
            {
              "bbox": [
                323,
                84,
                332,
                94
              ],
              "lines": [
                {
                  "bbox": [
                    323,
                    84,
                    332,
                    94
                  ],
                  "spans": [
                    {
                      "bbox": [
                        323,
                        84,
                        332,
                        94
                      ],
                      "type": "image",
                      "image_path": "770f339e3264523f4e165eff538a9d79948dd736587ce28100fd61d86cb6509e.jpg"
                    }
                  ]
                }
              ],
              "index": 1,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 1
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/770f339e3264523f4e165eff538a9d79948dd736587ce28100fd61d86cb6509e.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is not available for summarization."
    },
    {
      "index_id": 52,
      "parent_id": 48,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 52,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            333,
            85,
            349,
            94
          ],
          "blocks": [
            {
              "bbox": [
                333,
                85,
                349,
                94
              ],
              "lines": [
                {
                  "bbox": [
                    333,
                    85,
                    349,
                    94
                  ],
                  "spans": [
                    {
                      "bbox": [
                        333,
                        85,
                        349,
                        94
                      ],
                      "type": "image",
                      "image_path": "a00e76d2590b9fd8cd390a7fbc777bb0c7fed7df302318a1ba9ea8776b263563.jpg"
                    }
                  ]
                }
              ],
              "index": 2,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 2
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/a00e76d2590b9fd8cd390a7fbc777bb0c7fed7df302318a1ba9ea8776b263563.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is not available for analysis."
    },
    {
      "index_id": 53,
      "parent_id": 48,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 53,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            350,
            85,
            359,
            94
          ],
          "blocks": [
            {
              "bbox": [
                350,
                85,
                359,
                94
              ],
              "lines": [
                {
                  "bbox": [
                    350,
                    85,
                    359,
                    94
                  ],
                  "spans": [
                    {
                      "bbox": [
                        350,
                        85,
                        359,
                        94
                      ],
                      "type": "image",
                      "image_path": "481e65a2b8fab730088546439a0a6b2e5297aa8fb8b3b302706209619cb741cc.jpg"
                    }
                  ]
                }
              ],
              "index": 3,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                359,
                85,
                399,
                94
              ],
              "lines": [
                {
                  "bbox": [
                    359,
                    85,
                    399,
                    94
                  ],
                  "spans": [
                    {
                      "bbox": [
                        359,
                        85,
                        399,
                        94
                      ],
                      "type": "text",
                      "content": "Tree Nodes"
                    }
                  ]
                }
              ],
              "index": 4,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 3
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/481e65a2b8fab730088546439a0a6b2e5297aa8fb8b3b302706209619cb741cc.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Tree Nodes",
        "footnote": "",
        "table_body": null,
        "content": "Tree Nodes",
        "title_level": -1
      },
      "summary": "Tree Nodes"
    },
    {
      "index_id": 54,
      "parent_id": 48,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 54,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            405,
            85,
            414,
            94
          ],
          "blocks": [
            {
              "bbox": [
                405,
                85,
                414,
                94
              ],
              "lines": [
                {
                  "bbox": [
                    405,
                    85,
                    414,
                    94
                  ],
                  "spans": [
                    {
                      "bbox": [
                        405,
                        85,
                        414,
                        94
                      ],
                      "type": "image",
                      "image_path": "59ec9511ea1accb73022f300d095817edd42bdcbe7a6af6d325e1b6aa7516e1c.jpg"
                    }
                  ]
                }
              ],
              "index": 5,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                416,
                85,
                451,
                94
              ],
              "lines": [
                {
                  "bbox": [
                    416,
                    85,
                    451,
                    94
                  ],
                  "spans": [
                    {
                      "bbox": [
                        416,
                        85,
                        451,
                        94
                      ],
                      "type": "text",
                      "content": "GT-Link"
                    }
                  ]
                }
              ],
              "index": 6,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 5
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/59ec9511ea1accb73022f300d095817edd42bdcbe7a6af6d325e1b6aa7516e1c.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "GT-Link",
        "footnote": "",
        "table_body": null,
        "content": "GT-Link",
        "title_level": -1
      },
      "summary": "GT-Link is a technology or product, as indicated by the caption of the provided image."
    },
    {
      "index_id": 55,
      "parent_id": 48,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 55,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            464,
            84,
            475,
            95
          ],
          "blocks": [
            {
              "bbox": [
                464,
                84,
                475,
                95
              ],
              "lines": [
                {
                  "bbox": [
                    464,
                    84,
                    475,
                    95
                  ],
                  "spans": [
                    {
                      "bbox": [
                        464,
                        84,
                        475,
                        95
                      ],
                      "type": "image",
                      "image_path": "ad594f2224f00bf859ac61ecec8d0fa0d48a55fec3e5856cf5db9cc4ca503daf.jpg"
                    }
                  ]
                }
              ],
              "index": 7,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                478,
                85,
                500,
                95
              ],
              "lines": [
                {
                  "bbox": [
                    478,
                    85,
                    500,
                    95
                  ],
                  "spans": [
                    {
                      "bbox": [
                        478,
                        85,
                        500,
                        95
                      ],
                      "type": "text",
                      "content": "Entity"
                    }
                  ]
                }
              ],
              "index": 8,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 7
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/ad594f2224f00bf859ac61ecec8d0fa0d48a55fec3e5856cf5db9cc4ca503daf.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Entity",
        "footnote": "",
        "table_body": null,
        "content": "Entity",
        "title_level": -1
      },
      "summary": "Entity"
    },
    {
      "index_id": 56,
      "parent_id": 48,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 56,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            503,
            85,
            515,
            94
          ],
          "blocks": [
            {
              "bbox": [
                503,
                85,
                515,
                94
              ],
              "lines": [
                {
                  "bbox": [
                    503,
                    85,
                    515,
                    94
                  ],
                  "spans": [
                    {
                      "bbox": [
                        503,
                        85,
                        515,
                        94
                      ],
                      "type": "image",
                      "image_path": "32802cf1519ac6fa17e3cc062dd660ce6ac23c718188bd96f39054daebbee577.jpg"
                    }
                  ]
                }
              ],
              "index": 9,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                519,
                85,
                547,
                94
              ],
              "lines": [
                {
                  "bbox": [
                    519,
                    85,
                    547,
                    94
                  ],
                  "spans": [
                    {
                      "bbox": [
                        519,
                        85,
                        547,
                        94
                      ],
                      "type": "text",
                      "content": "Relation"
                    }
                  ]
                }
              ],
              "index": 10,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 9
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/32802cf1519ac6fa17e3cc062dd660ce6ac23c718188bd96f39054daebbee577.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Relation",
        "footnote": "",
        "table_body": null,
        "content": "Relation",
        "title_level": -1
      },
      "summary": "The content is an image with the caption \"Relation.\""
    },
    {
      "index_id": 57,
      "parent_id": 48,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 57,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            58,
            99,
            553,
            284
          ],
          "blocks": [
            {
              "bbox": [
                57,
                83,
                152,
                95
              ],
              "lines": [
                {
                  "bbox": [
                    57,
                    83,
                    152,
                    95
                  ],
                  "spans": [
                    {
                      "bbox": [
                        57,
                        83,
                        152,
                        95
                      ],
                      "type": "text",
                      "content": "BookIndex Construction"
                    }
                  ]
                }
              ],
              "index": 0,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                58,
                99,
                553,
                284
              ],
              "lines": [
                {
                  "bbox": [
                    58,
                    99,
                    553,
                    284
                  ],
                  "spans": [
                    {
                      "bbox": [
                        58,
                        99,
                        553,
                        284
                      ],
                      "type": "image",
                      "image_path": "59e31b1d4e283bf36b4432d896a8a325eaa23b585d776a387a10b79bf8a25571.jpg"
                    }
                  ]
                }
              ],
              "index": 11,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                50,
                288,
                558,
                310
              ],
              "lines": [
                {
                  "bbox": [
                    50,
                    288,
                    558,
                    310
                  ],
                  "spans": [
                    {
                      "bbox": [
                        50,
                        288,
                        558,
                        310
                      ],
                      "type": "text",
                      "content": "Figure 2: The BookIndex Construction process. This phase includes Tree Construction, derived from Layout Parsing and Section Filtering, and Graph Construction, which involves KG Construction and Gradient-based Entity Resolution."
                    }
                  ]
                }
              ],
              "index": 12,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 11
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/59e31b1d4e283bf36b4432d896a8a325eaa23b585d776a387a10b79bf8a25571.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "BookIndex Construction Figure 2: The BookIndex Construction process. This phase includes Tree Construction, derived from Layout Parsing and Section Filtering, and Graph Construction, which involves KG Construction and Gradient-based Entity Resolution.",
        "footnote": "",
        "table_body": null,
        "content": "BookIndex Construction Figure 2: The BookIndex Construction process. This phase includes Tree Construction, derived from Layout Parsing and Section Filtering, and Graph Construction, which involves KG Construction and Gradient-based Entity Resolution.",
        "title_level": -1
      },
      "summary": "The BookIndex Construction process consists of two main phases: Tree Construction and Graph Construction. Tree Construction is built from Layout Parsing and Section Filtering, while Graph Construction involves KG (Knowledge Graph) Construction and Gradient-based Entity Resolution."
    },
    {
      "index_id": 58,
      "parent_id": 48,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 58,
        "pdf_para_block": {
          "bbox": [
            51,
            320,
            195,
            331
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                320,
                195,
                331
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    320,
                    195,
                    331
                  ],
                  "type": "text",
                  "content": "4.1 Overview of BookIndex"
                }
              ]
            }
          ],
          "index": 13
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "4.1 Overview of BookIndex",
        "title_level": 2
      },
      "summary": "This section introduces the BookIndex framework, defining it as a structured representation of a document composed of a hierarchical tree, a knowledge graph of entities, and a linking function that grounds entities within the document's logical structure."
    },
    {
      "index_id": 59,
      "parent_id": 58,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 59,
        "pdf_para_block": {
          "bbox": [
            50,
            335,
            296,
            499
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                335,
                296,
                499
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": "We formally define our BookIndex as a triplet "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "B = (T, G, M)"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": ". Here, "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "T = (N, E_T)"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " represents a Tree structure where "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "N"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " is the set of nodes derived from the document's explicit logical hierarchy (e.g., titles, sections, tables), and "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "E_T"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " denotes their nesting relationships. "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "G = (V, E_G)"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " is a Knowledge Graph that captures fine-grained entities "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "(V)"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " and their relations "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "(E_G)"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " scattered throughout the document. Finally, "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "M: V \\to \\mathcal{P}(N)"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " is the Graph-Tree Link (GT-Link), which links each entity in "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "V"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " to the set of specific tree nodes in "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "N"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " from which it was extracted. These links are crucial for capturing the intricate, cross-sectional relations within the document. The hierarchical tree nodes in "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "T"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": " serve as the document's native information patches, providing structured contexts for information seeking. Meanwhile, the entities and relations in "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "G"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": ", connected via "
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "inline_equation",
                  "content": "M"
                },
                {
                  "bbox": [
                    50,
                    335,
                    296,
                    499
                  ],
                  "type": "text",
                  "content": ", act as the rich information scent that guides navigation between and within these patches."
                }
              ]
            }
          ],
          "index": 14
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "We formally define our BookIndex as a triplet  $B = (T, G, M)$ . Here,  $T = (N, E_T)$  represents a Tree structure where  $N$  is the set of nodes derived from the document's explicit logical hierarchy (e.g., titles, sections, tables), and  $E_T$  denotes their nesting relationships.  $G = (V, E_G)$  is a Knowledge Graph that captures fine-grained entities  $(V)$  and their relations  $(E_G)$  scattered throughout the document. Finally,  $M: V \\to \\mathcal{P}(N)$  is the Graph-Tree Link (GT-Link), which links each entity in  $V$  to the set of specific tree nodes in  $N$  from which it was extracted. These links are crucial for capturing the intricate, cross-sectional relations within the document. The hierarchical tree nodes in  $T$  serve as the document's native information patches, providing structured contexts for information seeking. Meanwhile, the entities and relations in  $G$ , connected via  $M$ , act as the rich information scent that guides navigation between and within these patches.",
        "title_level": -1
      },
      "summary": "The BookIndex is a structured representation of a document defined as a triplet $B = (T, G, M)$. It consists of:  \n1. **A Tree ($T$)**: This captures the document's explicit logical hierarchy (e.g., titles, sections) through nodes and their nesting relationships, serving as structured information patches.  \n2. **A Knowledge Graph ($G$)**: This models fine-grained entities and their relations found throughout the document, providing rich information scent for navigation.  \n3. **A Graph-Tree Link ($M$)**: This function connects each entity in the knowledge graph to the specific tree nodes from which it was extracted, enabling the capture of intricate, cross-sectional relations within the document."
    },
    {
      "index_id": 60,
      "parent_id": 58,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 60,
        "pdf_para_block": {
          "bbox": [
            50,
            499,
            296,
            598
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                499,
                296,
                598
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    499,
                    296,
                    598
                  ],
                  "type": "text",
                  "content": "Figure 2 provides an example of our BookIndex. The Tree component, positioned at the top, organizes the document into a hierarchical structure, where content blocks such as text, tables, and images serve as leaf nodes nested within section nodes. The Graph component is composed of entities and relations extracted from these nodes. The GT-Link, illustrated by the blue dotted lines, explicitly connects these entities back to their corresponding tree nodes, thereby grounding the semantic entities within the document's logical hierarchy."
                }
              ]
            }
          ],
          "index": 15
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Figure 2 provides an example of our BookIndex. The Tree component, positioned at the top, organizes the document into a hierarchical structure, where content blocks such as text, tables, and images serve as leaf nodes nested within section nodes. The Graph component is composed of entities and relations extracted from these nodes. The GT-Link, illustrated by the blue dotted lines, explicitly connects these entities back to their corresponding tree nodes, thereby grounding the semantic entities within the document's logical hierarchy.",
        "title_level": -1
      },
      "summary": "BookIndex integrates a hierarchical Tree component for document structure with a Graph component of extracted entities, linked via GT-Link to ground semantic entities within the document's logical organization."
    },
    {
      "index_id": 61,
      "parent_id": 48,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 61,
        "pdf_para_block": {
          "bbox": [
            51,
            609,
            169,
            620
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                609,
                169,
                620
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    609,
                    169,
                    620
                  ],
                  "type": "text",
                  "content": "4.2 Tree Construction"
                }
              ]
            }
          ],
          "index": 16
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "4.2 Tree Construction",
        "title_level": 2
      },
      "summary": "This section details the two-phase methodology for converting raw documents into a structured hierarchical tree through layout parsing and LLM-assisted section filtering."
    },
    {
      "index_id": 62,
      "parent_id": 61,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 62,
        "pdf_para_block": {
          "bbox": [
            50,
            624,
            295,
            658
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                624,
                295,
                658
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    624,
                    295,
                    658
                  ],
                  "type": "text",
                  "content": "The first stage transforms the raw document into a structured hierarchical tree "
                },
                {
                  "bbox": [
                    50,
                    624,
                    295,
                    658
                  ],
                  "type": "inline_equation",
                  "content": "T"
                },
                {
                  "bbox": [
                    50,
                    624,
                    295,
                    658
                  ],
                  "type": "text",
                  "content": ". This involves two key steps: robust layout parsing and intelligent section filtering."
                }
              ]
            }
          ],
          "index": 17
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The first stage transforms the raw document into a structured hierarchical tree  $T$ . This involves two key steps: robust layout parsing and intelligent section filtering.",
        "title_level": -1
      },
      "summary": "The first stage of document processing converts raw documents into a structured hierarchical tree (T) through robust layout parsing and intelligent section filtering."
    },
    {
      "index_id": 63,
      "parent_id": 61,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 63,
        "pdf_para_block": {
          "bbox": [
            50,
            666,
            295,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                666,
                295,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    666,
                    295,
                    710
                  ],
                  "type": "text",
                  "content": "4.2.1 Layout Parsing. The Layout Parsing phase processes the input document "
                },
                {
                  "bbox": [
                    50,
                    666,
                    295,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "D"
                },
                {
                  "bbox": [
                    50,
                    666,
                    295,
                    710
                  ],
                  "type": "text",
                  "content": " (a collection of pages) using layout analysis and recognition models. This step identifies, extracts, and organizes diverse blocks (e.g., text, tables, images) from the document pages."
                }
              ]
            }
          ],
          "index": 18
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "4.2.1 Layout Parsing. The Layout Parsing phase processes the input document  $D$  (a collection of pages) using layout analysis and recognition models. This step identifies, extracts, and organizes diverse blocks (e.g., text, tables, images) from the document pages.",
        "title_level": -1
      },
      "summary": "Layout Parsing is a phase that analyzes a multi-page document to identify, extract, and organize its distinct content blocks, such as text, tables, and images."
    },
    {
      "index_id": 64,
      "parent_id": 61,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 64,
        "pdf_para_block": {
          "bbox": [
            314,
            321,
            560,
            376
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                321,
                560,
                376
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "text",
                  "content": "The output is a sequence of primitive blocks, "
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "inline_equation",
                  "content": "\\mathcal{B} = \\{b_1, b_2, \\dots, b_k\\}"
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "text",
                  "content": ", where each block "
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "inline_equation",
                  "content": "b_i = (c_i, \\tau_i, f_i)"
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "text",
                  "content": " is defined as a triplet. Here, "
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "inline_equation",
                  "content": "c_i"
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "text",
                  "content": " is the raw content (e.g., text, image data), "
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "inline_equation",
                  "content": "\\tau_i"
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "text",
                  "content": " is the initial layout-based type (e.g., Title, Text, Table, Image), and "
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "inline_equation",
                  "content": "f_i"
                },
                {
                  "bbox": [
                    314,
                    321,
                    560,
                    376
                  ],
                  "type": "text",
                  "content": " is a vector of associated layout features (e.g., \"FontSize\", bounding box)."
                }
              ]
            }
          ],
          "index": 19
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The output is a sequence of primitive blocks,  $\\mathcal{B} = \\{b_1, b_2, \\dots, b_k\\}$ , where each block  $b_i = (c_i, \\tau_i, f_i)$  is defined as a triplet. Here,  $c_i$  is the raw content (e.g., text, image data),  $\\tau_i$  is the initial layout-based type (e.g., Title, Text, Table, Image), and  $f_i$  is a vector of associated layout features (e.g., \"FontSize\", bounding box).",
        "title_level": -1
      },
      "summary": "A sequence of primitive blocks is defined as a set of triplets, where each block consists of raw content, an initial layout-based type, and a vector of associated layout features."
    },
    {
      "index_id": 65,
      "parent_id": 61,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 65,
        "pdf_para_block": {
          "bbox": [
            313,
            392,
            559,
            544
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                392,
                559,
                544
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "text",
                  "content": "4.2.2 Section Filtering. Next, the Section Filtering phase processes this initial sequence to identify the document's logically hierarchical structure. Layout Parsing identifies blocks as Tit1e but does not assign their hierarchical level. Therefore, we select the candidate subset "
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "inline_equation",
                  "content": "\\mathcal{B}_{\\mathrm{title}} \\subset \\mathcal{B}"
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "text",
                  "content": " (where "
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "inline_equation",
                  "content": "\\tau_{i} = \\mathrm{T}\\mathrm{i}\\mathrm{t}\\mathrm{l}\\mathrm{e}"
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "text",
                  "content": ") for an LLM-based analysis. To handle extremely long documents, this analysis is performed in batches, where each batch retains a contextual window of high-level section information (with "
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "inline_equation",
                  "content": "l = 1"
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "text",
                  "content": " as the root). The LLM analyzes the content "
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "inline_equation",
                  "content": "c_{i}"
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "text",
                  "content": " and layout features "
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "inline_equation",
                  "content": "f_{i}"
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "text",
                  "content": " of the candidates to determine two key properties: their actual hierarchical level "
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "inline_equation",
                  "content": "l_{i} \\in \\{1,2,\\ldots\\}"
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "text",
                  "content": " and final node type "
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "inline_equation",
                  "content": "\\tau_{i}'"
                },
                {
                  "bbox": [
                    313,
                    392,
                    559,
                    544
                  ],
                  "type": "text",
                  "content": " (e.g., re-classifying an erroneous Tit1e as Text if its level is \"None\"). This step is crucial for preserving the document's logical hierarchy by correcting blocks erroneously parsed as Tit1e, such as descriptive text within images or borderless table headers."
                }
              ]
            }
          ],
          "index": 20
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "4.2.2 Section Filtering. Next, the Section Filtering phase processes this initial sequence to identify the document's logically hierarchical structure. Layout Parsing identifies blocks as Tit1e but does not assign their hierarchical level. Therefore, we select the candidate subset  $\\mathcal{B}_{\\mathrm{title}} \\subset \\mathcal{B}$  (where  $\\tau_{i} = \\mathrm{T}\\mathrm{i}\\mathrm{t}\\mathrm{l}\\mathrm{e}$ ) for an LLM-based analysis. To handle extremely long documents, this analysis is performed in batches, where each batch retains a contextual window of high-level section information (with  $l = 1$  as the root). The LLM analyzes the content  $c_{i}$  and layout features  $f_{i}$  of the candidates to determine two key properties: their actual hierarchical level  $l_{i} \\in \\{1,2,\\ldots\\}$  and final node type  $\\tau_{i}'$  (e.g., re-classifying an erroneous Tit1e as Text if its level is \"None\"). This step is crucial for preserving the document's logical hierarchy by correcting blocks erroneously parsed as Tit1e, such as descriptive text within images or borderless table headers.",
        "title_level": -1
      },
      "summary": "Section Filtering corrects initial layout parsing errors to establish a document's logical hierarchy. It uses an LLM to analyze candidate title blocks in batches, assigning hierarchical levels and reclassifying node types based on content and layout features, which ensures accurate structure representation, especially for long documents."
    },
    {
      "index_id": 66,
      "parent_id": 61,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 66,
        "pdf_para_block": {
          "bbox": [
            313,
            545,
            559,
            643
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                545,
                559,
                643
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "text",
                  "content": "Finally, the definitive tree "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "T = (N, E_T)"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "text",
                  "content": " is constructed. The node set "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "N"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "text",
                  "content": " is composed of all blocks from the filtering and re-classification process, where each node "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "n \\in N"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "text",
                  "content": " retains its content "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "(c_i)"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "text",
                  "content": " and its final node type "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "(\\tau_i')"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "text",
                  "content": " (e.g., Text, Section, Table, and Image). The edge set "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "E_T"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "text",
                  "content": ", representing the parent-child nesting relationships, is then established. Parent-child relationships are inferred by sequentially traversing the nodes, using both the determined hierarchical levels "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "(l_i)"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    643
                  ],
                  "type": "text",
                  "content": " of Section nodes and the overall document order to assemble the complete tree structure."
                }
              ]
            }
          ],
          "index": 21
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Finally, the definitive tree  $T = (N, E_T)$  is constructed. The node set  $N$  is composed of all blocks from the filtering and re-classification process, where each node  $n \\in N$  retains its content  $(c_i)$  and its final node type  $(\\tau_i')$  (e.g., Text, Section, Table, and Image). The edge set  $E_T$ , representing the parent-child nesting relationships, is then established. Parent-child relationships are inferred by sequentially traversing the nodes, using both the determined hierarchical levels  $(l_i)$  of Section nodes and the overall document order to assemble the complete tree structure.",
        "title_level": -1
      },
      "summary": "The definitive tree \\( T = (N, E_T) \\) is built from all processed blocks. Each node \\( n \\in N \\) contains its content \\( (c_i) \\) and final type \\( (\\tau_i') \\), such as Text or Table. The edges \\( E_T \\), which define parent-child nesting, are created by traversing nodes in order and using the hierarchical levels \\( (l_i) \\) of Section nodes along with document order to establish the complete structure."
    },
    {
      "index_id": 67,
      "parent_id": 61,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 67,
        "pdf_para_block": {
          "bbox": [
            313,
            643,
            559,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                643,
                559,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    643,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": "As an example shown in Figure 2, the Layout Parsing phase identifies diverse blocks, typing them as Title, Text, Table, and Image. During the Section Filtering phase, the Title candidates (e.g., \"Method\", \"Experiment\", and \"MOE Layer\") are analyzed by the LLM. The blocks \"Method\" and \"Experiment\" (both with \"FontSize: 14\") are correctly identified as Section nodes at \"Level: 2\". Conversely,"
                }
              ]
            }
          ],
          "index": 22
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "As an example shown in Figure 2, the Layout Parsing phase identifies diverse blocks, typing them as Title, Text, Table, and Image. During the Section Filtering phase, the Title candidates (e.g., \"Method\", \"Experiment\", and \"MOE Layer\") are analyzed by the LLM. The blocks \"Method\" and \"Experiment\" (both with \"FontSize: 14\") are correctly identified as Section nodes at \"Level: 2\". Conversely, the \"MOE Layer\" block (\"FontSize: 20\"), which was erroneously tagged as Ti tle by the parser, is re-classified by the LLM as a Text node with \"Level: None\". This correction is crucial for preserving the document's logical hierarchy. Following this process, all filtered and classified nodes are assembled into the final tree structure based on their determined levels and document order.",
        "title_level": -1
      },
      "summary": "The Layout Parsing phase identifies document blocks (Title, Text, Table, Image), and the subsequent Section Filtering phase uses an LLM to correct hierarchical classifications. For instance, it correctly identifies \"Method\" and \"Experiment\" as Level 2 sections while reclassifying the mislabeled \"MOE Layer\" (with a larger font) as a Text node with no section level. This ensures an accurate logical hierarchy, after which all nodes are assembled into a final tree structure based on their levels and document order."
    },
    {
      "index_id": 68,
      "parent_id": 61,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 3,
        "page_path": null,
        "pdf_id": 68,
        "pdf_para_block": {
          "bbox": [
            303,
            713,
            307,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                303,
                713,
                307,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    303,
                    713,
                    307,
                    719
                  ],
                  "type": "text",
                  "content": "4"
                }
              ]
            }
          ],
          "index": 23
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "4",
        "title_level": -1
      },
      "summary": "4"
    },
    {
      "index_id": 69,
      "parent_id": 48,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 69,
        "pdf_para_block": {
          "bbox": [
            51,
            163,
            178,
            175
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                163,
                178,
                175
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    163,
                    178,
                    175
                  ],
                  "type": "text",
                  "content": "4.3 Graph Construction"
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "4.3 Graph Construction",
        "title_level": 2
      },
      "summary": "This section details the methodology for constructing a knowledge graph from a document tree, including entity extraction, a gradient-based resolution process to merge duplicates, and the creation of a mapping back to the source structure."
    },
    {
      "index_id": 70,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 70,
        "pdf_para_block": {
          "bbox": [
            50,
            178,
            295,
            209
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                178,
                295,
                209
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    178,
                    295,
                    209
                  ],
                  "type": "text",
                  "content": "Once the tree "
                },
                {
                  "bbox": [
                    50,
                    178,
                    295,
                    209
                  ],
                  "type": "inline_equation",
                  "content": "T"
                },
                {
                  "bbox": [
                    50,
                    178,
                    295,
                    209
                  ],
                  "type": "text",
                  "content": " is established, we proceed to populate the Knowledge Graph "
                },
                {
                  "bbox": [
                    50,
                    178,
                    295,
                    209
                  ],
                  "type": "inline_equation",
                  "content": "G"
                },
                {
                  "bbox": [
                    50,
                    178,
                    295,
                    209
                  ],
                  "type": "text",
                  "content": " by extracting and refining entities from the tree nodes."
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Once the tree  $T$  is established, we proceed to populate the Knowledge Graph  $G$  by extracting and refining entities from the tree nodes.",
        "title_level": -1
      },
      "summary": "After establishing tree $T$, entities are extracted and refined from its nodes to populate the Knowledge Graph $G$."
    },
    {
      "index_id": 71,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 71,
        "pdf_para_block": {
          "bbox": [
            50,
            218,
            295,
            317
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                218,
                295,
                317
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": "4.3.1 KG Construction. We iterate each node "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "n_i \\in N"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": " from the previously constructed tree "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "T"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": ". For each node "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "n_i"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": ", we extract a subgraph "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "g_i = (V_i, E_{Ri})"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": " based on its content "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "c_i"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": " and final node type "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "\\tau_i'"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": ". This extraction is modality-dependent: if the node is text-only, an LLM is prompted to extract entities and relations, while for nodes containing visual elements (e.g., "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "\\tau_i' = \\text{Image}"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": "), a Vision Language Model (VLM) is employed to extract visual knowledge. Crucially, for every entity "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "v \\in V_i"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": " extracted, its origin tree node "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "n_i"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": " is recorded, which is vital for constructing the final mapping "
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "inline_equation",
                  "content": "M"
                },
                {
                  "bbox": [
                    50,
                    218,
                    295,
                    317
                  ],
                  "type": "text",
                  "content": "."
                }
              ]
            }
          ],
          "index": 3
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "4.3.1 KG Construction. We iterate each node  $n_i \\in N$  from the previously constructed tree  $T$ . For each node  $n_i$ , we extract a subgraph  $g_i = (V_i, E_{Ri})$  based on its content  $c_i$  and final node type  $\\tau_i'$ . This extraction is modality-dependent: if the node is text-only, an LLM is prompted to extract entities and relations, while for nodes containing visual elements (e.g.,  $\\tau_i' = \\text{Image}$ ), a Vision Language Model (VLM) is employed to extract visual knowledge. Crucially, for every entity  $v \\in V_i$  extracted, its origin tree node  $n_i$  is recorded, which is vital for constructing the final mapping  $M$ .",
        "title_level": -1
      },
      "summary": "The KG construction process iterates through each node in a tree, extracting a modality-specific subgraph from each node's content. For text-only nodes, an LLM extracts entities and relations; for nodes with visual elements, a VLM performs the extraction. The origin tree node of every extracted entity is recorded to support the final mapping."
    },
    {
      "index_id": 72,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 72,
        "pdf_para_block": {
          "bbox": [
            50,
            318,
            295,
            395
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                318,
                295,
                395
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    318,
                    295,
                    395
                  ],
                  "type": "text",
                  "content": "Furthermore, to preserve structural semantics for specific logical types (e.g., Table, Formula), our process first creates a distinct, typed entity (e.g., "
                },
                {
                  "bbox": [
                    50,
                    318,
                    295,
                    395
                  ],
                  "type": "inline_equation",
                  "content": "v_{\\text{table}}"
                },
                {
                  "bbox": [
                    50,
                    318,
                    295,
                    395
                  ],
                  "type": "text",
                  "content": " representing the table itself). The other extracted entities from the specific node's content are linked to this primary vertex. For Table nodes specifically, row and column headers are also explicitly extracted as distinct entities and linked to "
                },
                {
                  "bbox": [
                    50,
                    318,
                    295,
                    395
                  ],
                  "type": "inline_equation",
                  "content": "v_{\\text{table}}"
                },
                {
                  "bbox": [
                    50,
                    318,
                    295,
                    395
                  ],
                  "type": "text",
                  "content": " via a \"ContainedIn\" relationship."
                }
              ]
            }
          ],
          "index": 4
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Furthermore, to preserve structural semantics for specific logical types (e.g., Table, Formula), our process first creates a distinct, typed entity (e.g.,  $v_{\\text{table}}$  representing the table itself). The other extracted entities from the specific node's content are linked to this primary vertex. For Table nodes specifically, row and column headers are also explicitly extracted as distinct entities and linked to  $v_{\\text{table}}$  via a \"ContainedIn\" relationship.",
        "title_level": -1
      },
      "summary": "The process preserves structural semantics by creating a distinct, typed entity (e.g., $v_{\\text{table}}$) for specific logical types like tables or formulas. All other entities extracted from that node's content are then linked to this primary vertex. For tables, row and column headers are explicitly extracted as separate entities and connected to $v_{\\text{table}}$ via a \"ContainedIn\" relationship."
    },
    {
      "index_id": 73,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 73,
        "pdf_para_block": {
          "bbox": [
            50,
            403,
            295,
            490
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                403,
                295,
                490
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    403,
                    295,
                    490
                  ],
                  "type": "text",
                  "content": "4.3.2 Gradient-based Entity Resolution. As shown in the literature [62, 66], a well-constructed KG is essential for document question answering. A common challenge in the extraction process is that the same conceptual entity is often fragmented into multiple distinct entities due to abbreviations, co-references, or its varied occurrences across different document sections. This necessitates a robust Entity Resolution (ER) process, which identifies and merges these fragmented entities to refine the raw KG."
                }
              ]
            }
          ],
          "index": 5
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "4.3.2 Gradient-based Entity Resolution. As shown in the literature [62, 66], a well-constructed KG is essential for document question answering. A common challenge in the extraction process is that the same conceptual entity is often fragmented into multiple distinct entities due to abbreviations, co-references, or its varied occurrences across different document sections. This necessitates a robust Entity Resolution (ER) process, which identifies and merges these fragmented entities to refine the raw KG.",
        "title_level": -1
      },
      "summary": "Gradient-based Entity Resolution is a method for refining knowledge graphs by identifying and merging fragmented representations of the same entity, which is essential for improving document question answering systems."
    },
    {
      "index_id": 74,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 74,
        "pdf_para_block": {
          "bbox": [
            50,
            491,
            295,
            622
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                491,
                295,
                622
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    491,
                    295,
                    622
                  ],
                  "type": "text",
                  "content": "However, conventional ER methods are computationally expensive. They are often designed for batch processing across multiple data sources (commonly referred to as dirty ER), aiming to ensure accurate entity resolution by finding all possible matching pairs [12]. This process typically requires finding the transitive closure of all detected matches. That is, to definitively merge multiple entities (e.g., A, B, and C) as the same concept, the system must ideally compare all possible pairs (\"A-B\", \"A-C\", and \"B-C\") to confirm their equivalence. This can lead to a quadratic "
                },
                {
                  "bbox": [
                    50,
                    491,
                    295,
                    622
                  ],
                  "type": "inline_equation",
                  "content": "(O(n^{2}))"
                },
                {
                  "bbox": [
                    50,
                    491,
                    295,
                    622
                  ],
                  "type": "text",
                  "content": " number of pairwise comparisons, a process that becomes prohibitively slow and computationally expensive when relying on LLMs for high-accuracy judgments."
                }
              ]
            }
          ],
          "index": 6
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "However, conventional ER methods are computationally expensive. They are often designed for batch processing across multiple data sources (commonly referred to as dirty ER), aiming to ensure accurate entity resolution by finding all possible matching pairs [12]. This process typically requires finding the transitive closure of all detected matches. That is, to definitively merge multiple entities (e.g., A, B, and C) as the same concept, the system must ideally compare all possible pairs (\"A-B\", \"A-C\", and \"B-C\") to confirm their equivalence. This can lead to a quadratic  $(O(n^{2}))$  number of pairwise comparisons, a process that becomes prohibitively slow and computationally expensive when relying on LLMs for high-accuracy judgments.",
        "title_level": -1
      },
      "summary": "Conventional entity resolution (ER) methods are computationally expensive because they require comparing all possible pairs of entities to confirm matches, leading to a quadratic number of comparisons that becomes prohibitively slow when using high-accuracy LLMs."
    },
    {
      "index_id": 75,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 75,
        "pdf_para_block": {
          "bbox": [
            50,
            622,
            295,
            709
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                622,
                295,
                709
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    622,
                    295,
                    709
                  ],
                  "type": "text",
                  "content": "To address this, we employ a gradient-based ER method, operating on a single document (simplified as the clean ER), which performs ER incrementally as each new entity "
                },
                {
                  "bbox": [
                    50,
                    622,
                    295,
                    709
                  ],
                  "type": "inline_equation",
                  "content": "v_{n}"
                },
                {
                  "bbox": [
                    50,
                    622,
                    295,
                    709
                  ],
                  "type": "text",
                  "content": " is extracted. This transforms the quadratic batch problem into a simpler, repeated lookup task: determining where the single new entity "
                },
                {
                  "bbox": [
                    50,
                    622,
                    295,
                    709
                  ],
                  "type": "inline_equation",
                  "content": "v_{n}"
                },
                {
                  "bbox": [
                    50,
                    622,
                    295,
                    709
                  ],
                  "type": "text",
                  "content": " fits among the already-processed entities in the database. This incremental process yields two distinct, observable scoring patterns when "
                },
                {
                  "bbox": [
                    50,
                    622,
                    295,
                    709
                  ],
                  "type": "inline_equation",
                  "content": "v_{n}"
                },
                {
                  "bbox": [
                    50,
                    622,
                    295,
                    709
                  ],
                  "type": "text",
                  "content": " is reranked against its top_k most relevant candidates:"
                }
              ]
            }
          ],
          "index": 7
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "To address this, we employ a gradient-based ER method, operating on a single document (simplified as the clean ER), which performs ER incrementally as each new entity  $v_{n}$  is extracted. This transforms the quadratic batch problem into a simpler, repeated lookup task: determining where the single new entity  $v_{n}$  fits among the already-processed entities in the database. This incremental process yields two distinct, observable scoring patterns when  $v_{n}$  is reranked against its top_k most relevant candidates: Our Gradient-based ER algorithm is designed precisely to detect this sharp decline (characteristic of Case B), allowing us to efficiently isolate the high-relevance set. Subsequently, an LLM is utilized for finer-grained distinction when multiple similar entities are identified within this set, differentiating it from the \"no gradient\" scenario (Case A) without quadratic comparisons.",
        "title_level": -1
      },
      "summary": "A gradient-based entity resolution (ER) method processes documents incrementally, converting a quadratic batch problem into a repeated lookup task for each new entity. It identifies a sharp decline in relevance scores among top candidates to efficiently isolate a high-relevance set, after which an LLM performs fine-grained distinction when multiple similar entities are found. This approach distinguishes cases with a clear gradient from those without, avoiding exhaustive comparisons."
    },
    {
      "index_id": 76,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 76,
        "pdf_para_block": {
          "type": "code",
          "bbox": [
            318,
            99,
            558,
            335
          ],
          "blocks": [
            {
              "bbox": [
                320,
                86,
                495,
                97
              ],
              "lines": [
                {
                  "bbox": [
                    320,
                    86,
                    495,
                    97
                  ],
                  "spans": [
                    {
                      "bbox": [
                        320,
                        86,
                        495,
                        97
                      ],
                      "type": "text",
                      "content": "Algorithm 1: Gradient-based entity resolution"
                    }
                  ]
                }
              ],
              "index": 8,
              "angle": 0,
              "type": "code_caption"
            },
            {
              "bbox": [
                318,
                99,
                558,
                335
              ],
              "lines": [
                {
                  "bbox": [
                    318,
                    99,
                    558,
                    335
                  ],
                  "spans": [
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": "Input:KG G,New entity "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "v_{n}"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " ,Rerank model R,Entity vector database DB, Vector search number top_k, threshold of gradient g // Vector Search top_k relevant entities in DB.   \n1 "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "E_{c}\\gets"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " Search(DB, "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "v_{n},"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " top_k);   \n2 "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "\\mathcal{S}\\gets \\mathcal{R}(E_c,v_n)"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " . // Sort all candidate entities by rerank scores.   \n3 Sort(Ec,S);   \n4 score "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "\\leftarrow S[0]"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " ,Sel "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "\\leftarrow E_{c}[0]"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " .. // Gradient select similar entities.   \n5 for each remain entity "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "v_{c}\\in E_{c}\\setminus \\{E_{c}[0]\\}"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " do 6 if "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "S[v_{c}]>"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " score/g then 7 Sel "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "\\leftarrow"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " SelU "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "\\{v_{c}\\}"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " ,score "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "\\leftarrow S[v_{c}]"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " . 8 else break; //Merge entity or add new entity.   \n9 if length(Sel)=length(Ec)then 1 "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "G\\gets"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " AddNewEntity(G,vn),DB "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "\\leftarrow"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " AddNew(DB,vn);   \nelse 12 if length(Sel)=1 then "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "v_{sel}\\gets Sel[0]"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " . 13 else "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "v_{sel}\\gets"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " LLMSelect(Sel); 14 "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "G\\gets"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " MergeEntity(G,vn,vsel),DB "
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "inline_equation",
                      "content": "\\leftarrow"
                    },
                    {
                      "bbox": [
                        318,
                        99,
                        558,
                        335
                      ],
                      "type": "text",
                      "content": " Update(DB,vsel,vn);   \n15 return G,DB;"
                    }
                  ]
                }
              ],
              "index": 9,
              "angle": 0,
              "type": "code_body"
            }
          ],
          "index": 9,
          "sub_type": "algorithm"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The content to be summarized was not provided. Please provide the text, image description, or table data for condensation."
    },
    {
      "index_id": 77,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 77,
        "pdf_para_block": {
          "bbox": [
            334,
            344,
            559,
            475
          ],
          "type": "list",
          "angle": 0,
          "index": 12,
          "blocks": [
            {
              "bbox": [
                334,
                344,
                559,
                387
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    334,
                    344,
                    559,
                    387
                  ],
                  "spans": [
                    {
                      "bbox": [
                        334,
                        344,
                        559,
                        387
                      ],
                      "type": "text",
                      "content": "- Case A: New Entity. If "
                    },
                    {
                      "bbox": [
                        334,
                        344,
                        559,
                        387
                      ],
                      "type": "inline_equation",
                      "content": "v_{n}"
                    },
                    {
                      "bbox": [
                        334,
                        344,
                        559,
                        387
                      ],
                      "type": "text",
                      "content": " is a new conceptual entity, its relevance scores against all existing entities will be uniformly low, showing no significant gradient or discriminative pattern."
                    }
                  ]
                }
              ],
              "index": 10
            },
            {
              "bbox": [
                334,
                388,
                559,
                475
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    334,
                    388,
                    559,
                    475
                  ],
                  "spans": [
                    {
                      "bbox": [
                        334,
                        388,
                        559,
                        475
                      ],
                      "type": "text",
                      "content": "- Case B: Existing Entity. If "
                    },
                    {
                      "bbox": [
                        334,
                        388,
                        559,
                        475
                      ],
                      "type": "inline_equation",
                      "content": "v_{n}"
                    },
                    {
                      "bbox": [
                        334,
                        388,
                        559,
                        475
                      ],
                      "type": "text",
                      "content": " is an alias of an existing entity, its scores will show a high relevance to the true match (or a small set of equivalent aliases). Due to the reranker's inherent discriminative limitations, this initial high-relevance set might occasionally contain multiple similar entities. This high-relevance set is then typically followed by a sharp decline (a large \"gradient\") before transitioning to a gradual slope of irrelevant entities."
                    }
                  ]
                }
              ],
              "index": 11
            }
          ],
          "sub_type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The data is unavailable for summarization."
    },
    {
      "index_id": 78,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 78,
        "pdf_para_block": {
          "bbox": [
            313,
            545,
            559,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                545,
                559,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": "Algorithm 1 shows the above entity resolution process. For a new entity "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "v_{n}"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": ", we first retrieve its top_k candidates "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "E_{c}"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " from the vector database "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "DB"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": ", which are then reranked by "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "\\mathcal{R}"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " against "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "v_{n}"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " and sorted based on their scores "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "S"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " (Lines 1-3). We initialize the selection set "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "Sel"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " with the top-scoring candidate "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "E_{c}[0]"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " and set the initial score to "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "S[0]"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " (Line 4). We then iterate through the remaining sorted candidates (Lines 5-8). The core logic checks if the current score "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "S[v_{c}]"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " is still within the gradient threshold "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "g"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " of the previous score (i.e., "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "S[v_{c}] > \\text{score} / g"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": "). If the score drop is gentle (passes the check), the candidate "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "v_{c}"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " is added to "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "Sel"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": ", and score is updated (Lines 7-8); otherwise, the loop breaks (Line 8) as soon as a sharp score drop is detected. Finally, the algorithm makes its decision (Lines 9-14). If the selection set "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "Sel"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " is identical to "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "E_{c}"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": ", this indicates that all candidates passed the gradient check. This corresponds to Case A, where the scores lacked discriminative power (i.e., "
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "v_{n}"
                },
                {
                  "bbox": [
                    313,
                    545,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " is equally dissimilar to"
                }
              ]
            }
          ],
          "index": 14
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Algorithm 1 shows the above entity resolution process. For a new entity  $v_{n}$ , we first retrieve its top_k candidates  $E_{c}$  from the vector database  $DB$ , which are then reranked by  $\\mathcal{R}$  against  $v_{n}$  and sorted based on their scores  $S$  (Lines 1-3). We initialize the selection set  $Sel$  with the top-scoring candidate  $E_{c}[0]$  and set the initial score to  $S[0]$  (Line 4). We then iterate through the remaining sorted candidates (Lines 5-8). The core logic checks if the current score  $S[v_{c}]$  is still within the gradient threshold  $g$  of the previous score (i.e.,  $S[v_{c}] > \\text{score} / g$ ). If the score drop is gentle (passes the check), the candidate  $v_{c}$  is added to  $Sel$ , and score is updated (Lines 7-8); otherwise, the loop breaks (Line 8) as soon as a sharp score drop is detected. Finally, the algorithm makes its decision (Lines 9-14). If the selection set  $Sel$  is identical to  $E_{c}$ , this indicates that all candidates passed the gradient check. This corresponds to Case A, where the scores lacked discriminative power (i.e.,  $v_{n}$  is equally dissimilar to all candidates), so  $v_{n}$  is added as a new entity (Line 9-10). Conversely, if a gradient was found (i.e.,  $length(Sel) < length(E_{c})$ ), this signals Case B. We then select the canonical entity  $v_{sel}$  from Sel, using an LLM (Line 13) if the reranker identifies multiple aliases, and merge  $v_{n}$  with it (Lines 12-14). The updated  $G$  and  $DB$  are then returned (Line 15).",
        "title_level": -1
      },
      "summary": "This algorithm resolves new entities by retrieving and reranking candidate matches, then applying a gradient threshold to determine whether to create a new entity or merge with an existing one.  \n\nFor a new entity, top candidates are retrieved from a database and reranked by score. The algorithm selects candidates while their scores remain within a gradient threshold of the previous best score. If all candidates pass this check—indicating low score discrimination—the entity is added as new. If a score drop occurs, the selected candidates are merged with the best-matching existing entity, using an LLM if multiple aliases are identified. Finally, the knowledge graph and database are updated."
    },
    {
      "index_id": 79,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 4,
        "page_path": null,
        "pdf_id": 79,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            307,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                307,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    307,
                    719
                  ],
                  "type": "text",
                  "content": "5"
                }
              ]
            }
          ],
          "index": 15
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "5",
        "title_level": -1
      },
      "summary": "Number 5."
    },
    {
      "index_id": 80,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 80,
        "pdf_para_block": {
          "bbox": [
            50,
            150,
            294,
            238
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                150,
                294,
                238
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": "For instance, considering the example in Figure 2, when the new entity "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_9"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": " is processed, it is first compared with existing entities in the KG. As depicted in the similarity curve (orange line), "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_9"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": " shows high similarity with "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_7"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": ", followed by a sharp decline in similarity with other entities like "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_6"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": ", "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_8"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": ", and "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_5"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": ". Our gradient-based selection process identifies "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_7"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": " as the unique, high-confidence match for "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_9"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": ". Consequently, "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_9"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": " is merged with "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_7"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": ", enriching the KG with consolidated information as shown in the final merged entity "
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "inline_equation",
                  "content": "e_7'"
                },
                {
                  "bbox": [
                    50,
                    150,
                    294,
                    238
                  ],
                  "type": "text",
                  "content": "."
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "For instance, considering the example in Figure 2, when the new entity  $e_9$  is processed, it is first compared with existing entities in the KG. As depicted in the similarity curve (orange line),  $e_9$  shows high similarity with  $e_7$ , followed by a sharp decline in similarity with other entities like  $e_6$ ,  $e_8$ , and  $e_5$ . Our gradient-based selection process identifies  $e_7$  as the unique, high-confidence match for  $e_9$ . Consequently,  $e_9$  is merged with  $e_7$ , enriching the KG with consolidated information as shown in the final merged entity  $e_7'$ .",
        "title_level": -1
      },
      "summary": "A new entity, \\(e_9\\), is successfully merged with an existing entity, \\(e_7\\), after a gradient-based selection process identifies \\(e_7\\) as its unique, high-confidence match based on a sharp peak in similarity. This merger enriches the knowledge graph with consolidated information, resulting in the updated entity \\(e_7'\\)."
    },
    {
      "index_id": 81,
      "parent_id": 69,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 81,
        "pdf_para_block": {
          "bbox": [
            50,
            238,
            294,
            336
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                238,
                294,
                336
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": "Graph-Tree Link (GT-Link). The GT-Link "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "M"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": " is formalized to complete the BookIndex "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "B = (T, G, M)"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": ". As described in the "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "KG"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": " Construction phase, the origin tree node "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "n_i"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": " is recorded for every newly extracted entity "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "v_i"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": ". GT-Link is then refined during entity resolution: when an entity "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "v_n"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": " is merged into a canonical entity "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "v_{sel}"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": ", the origin node set of "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "v_{sel}"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": " is updated to include all origin nodes previously associated with "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "v_n"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": ". This aggregation process creates the final mapping "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "M: V \\to \\mathcal{P}(N)"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": ", which bi-directionally links the entities in "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "G"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": " to the set of their structural locations (nodes) in "
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "inline_equation",
                  "content": "T"
                },
                {
                  "bbox": [
                    50,
                    238,
                    294,
                    336
                  ],
                  "type": "text",
                  "content": "."
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Graph-Tree Link (GT-Link). The GT-Link  $M$  is formalized to complete the BookIndex  $B = (T, G, M)$ . As described in the  $KG$  Construction phase, the origin tree node  $n_i$  is recorded for every newly extracted entity  $v_i$ . GT-Link is then refined during entity resolution: when an entity  $v_n$  is merged into a canonical entity  $v_{sel}$ , the origin node set of  $v_{sel}$  is updated to include all origin nodes previously associated with  $v_n$ . This aggregation process creates the final mapping  $M: V \\to \\mathcal{P}(N)$ , which bi-directionally links the entities in  $G$  to the set of their structural locations (nodes) in  $T$ .",
        "title_level": -1
      },
      "summary": "GT-Link (M) is a mapping that bi-directionally connects entities in a knowledge graph (G) to their structural source locations in a tree (T). It is built by tracking the origin tree node for each extracted entity and then aggregating these origin nodes during entity resolution when duplicate entities are merged into a single canonical entity. The final result is a complete mapping from each entity in G to the set of tree nodes (N) from which it originated."
    },
    {
      "index_id": 82,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 82,
        "pdf_para_block": {
          "bbox": [
            51,
            347,
            211,
            357
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                347,
                211,
                357
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    347,
                    211,
                    357
                  ],
                  "type": "text",
                  "content": "5 AGENT-BASED RETRIEVAL"
                }
              ]
            }
          ],
          "index": 3
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "5 AGENT-BASED RETRIEVAL",
        "title_level": 1
      },
      "summary": "This section details BookRAG's three-stage agent-based retrieval workflow, which uses planning and structured execution to systematically process complex queries."
    },
    {
      "index_id": 83,
      "parent_id": 82,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 83,
        "pdf_para_block": {
          "bbox": [
            50,
            361,
            294,
            449
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                361,
                294,
                449
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    361,
                    294,
                    449
                  ],
                  "type": "text",
                  "content": "Real-world document queries are often complex, necessitating operations like modal type filtering, semantic selection, and multi-hop reasoning. To address this, we propose an agent-based approach in BookRAG, which intelligently plans and executes operations on the BookIndex. We first introduce the overall workflow and present two core mechanisms: Agent-based Planning, which formulates the strategy, and the Structured Execution, which includes the retrieval process under the principles of IFT and generation."
                }
              ]
            }
          ],
          "index": 4
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Real-world document queries are often complex, necessitating operations like modal type filtering, semantic selection, and multi-hop reasoning. To address this, we propose an agent-based approach in BookRAG, which intelligently plans and executes operations on the BookIndex. We first introduce the overall workflow and present two core mechanisms: Agent-based Planning, which formulates the strategy, and the Structured Execution, which includes the retrieval process under the principles of IFT and generation.",
        "title_level": -1
      },
      "summary": "BookRAG uses an agent-based approach to handle complex document queries involving filtering, selection, and reasoning. Its workflow relies on two core mechanisms: Agent-based Planning to formulate a query strategy, and Structured Execution to perform retrieval and generation."
    },
    {
      "index_id": 84,
      "parent_id": 82,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 84,
        "pdf_para_block": {
          "bbox": [
            51,
            460,
            167,
            470
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                460,
                167,
                470
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    460,
                    167,
                    470
                  ],
                  "type": "text",
                  "content": "5.1 Overall Workflow"
                }
              ]
            }
          ],
          "index": 5
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "5.1 Overall Workflow",
        "title_level": 2
      },
      "summary": "This section details the three-stage agent-based retrieval workflow (planning, retrieval, and generation) for systematically processing user queries in BookRAG."
    },
    {
      "index_id": 85,
      "parent_id": 84,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 85,
        "pdf_para_block": {
          "bbox": [
            50,
            474,
            294,
            506
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                474,
                294,
                506
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    474,
                    294,
                    506
                  ],
                  "type": "text",
                  "content": "The overall workflow of agent-based retrieval, illustrated in Figure 3, follows a three-stage pipeline designed to address users' queries systematically."
                }
              ]
            }
          ],
          "index": 6
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The overall workflow of agent-based retrieval, illustrated in Figure 3, follows a three-stage pipeline designed to address users' queries systematically.",
        "title_level": -1
      },
      "summary": "The agent-based retrieval workflow is a three-stage pipeline that systematically addresses user queries."
    },
    {
      "index_id": 86,
      "parent_id": 84,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 86,
        "pdf_para_block": {
          "bbox": [
            50,
            507,
            294,
            562
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                507,
                294,
                562
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    507,
                    294,
                    562
                  ],
                  "type": "text",
                  "content": "1. Agent-based Planning. BookRAG first performs Classification & Plan. This stage aims to distinguish simple keyword-based queries from reasoning questions that require decomposition and analysis. For instance, a query like \"How does Transformer differ from RNNs in handling long-range dependencies?\" cannot be"
                }
              ]
            }
          ],
          "index": 7
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "1. Agent-based Planning. BookRAG first performs Classification & Plan. This stage aims to distinguish simple keyword-based queries from reasoning questions that require decomposition and analysis. For instance, a query like \"How does Transformer differ from RNNs in handling long-range dependencies?\" cannot be solved by retrieving from a single keyword. Therefore, the planning stage first performs query classification. Based on this classification and a predefined set of operators designed for the BookIndex, it generates a specific operators plan that effectively guides the retrieval and generation strategies.",
        "title_level": -1
      },
      "summary": "BookRAG uses agent-based planning to first classify queries, distinguishing simple keyword searches from complex reasoning questions. For queries requiring analysis, like comparing Transformer and RNN models, it then creates a tailored plan of retrieval operators to guide effective information gathering and answer generation."
    },
    {
      "index_id": 87,
      "parent_id": 84,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 87,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            323,
            83,
            553,
            223
          ],
          "blocks": [
            {
              "bbox": [
                323,
                83,
                553,
                223
              ],
              "lines": [
                {
                  "bbox": [
                    323,
                    83,
                    553,
                    223
                  ],
                  "spans": [
                    {
                      "bbox": [
                        323,
                        83,
                        553,
                        223
                      ],
                      "type": "image",
                      "image_path": "20a0baf7e36c28e7a2358d7456a5465edaadafcca993c59eaaf194eb2e33d8e7.jpg"
                    }
                  ]
                }
              ],
              "index": 8,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                314,
                228,
                559,
                262
              ],
              "lines": [
                {
                  "bbox": [
                    314,
                    228,
                    559,
                    262
                  ],
                  "spans": [
                    {
                      "bbox": [
                        314,
                        228,
                        559,
                        262
                      ],
                      "type": "text",
                      "content": "Figure 3: The general workflow of agent-based retrieval in BookRAG, which contains agent-based planning, retrieval, and generation processes."
                    }
                  ]
                }
              ],
              "index": 9,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 8
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/20a0baf7e36c28e7a2358d7456a5465edaadafcca993c59eaaf194eb2e33d8e7.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Figure 3: The general workflow of agent-based retrieval in BookRAG, which contains agent-based planning, retrieval, and generation processes.",
        "footnote": "",
        "table_body": null,
        "content": "Figure 3: The general workflow of agent-based retrieval in BookRAG, which contains agent-based planning, retrieval, and generation processes.",
        "title_level": -1
      },
      "summary": "Figure 3 illustrates the general workflow of agent-based retrieval in BookRAG, which consists of three core processes: agent-based planning, retrieval, and generation."
    },
    {
      "index_id": 88,
      "parent_id": 84,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 88,
        "pdf_para_block": {
          "bbox": [
            314,
            326,
            559,
            402
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                326,
                559,
                402
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    326,
                    559,
                    402
                  ],
                  "type": "text",
                  "content": "2. Retrieval Process. Guided by the operator plan, the retrieval process executes Scent/Filter-based Retrieval. This stage navigates the BookIndex "
                },
                {
                  "bbox": [
                    314,
                    326,
                    559,
                    402
                  ],
                  "type": "inline_equation",
                  "content": "B = (T, G, M)"
                },
                {
                  "bbox": [
                    314,
                    326,
                    559,
                    402
                  ],
                  "type": "text",
                  "content": ", either utilizing a scent-based retrieval principle (e.g., following relevant entities in "
                },
                {
                  "bbox": [
                    314,
                    326,
                    559,
                    402
                  ],
                  "type": "inline_equation",
                  "content": "G"
                },
                {
                  "bbox": [
                    314,
                    326,
                    559,
                    402
                  ],
                  "type": "text",
                  "content": ") to find information, or employing various filters (e.g., modal type) to refine the selection. After reasoning, BookRAG gets the retrieval set of highly relevant information blocks from the BookIndex."
                }
              ]
            }
          ],
          "index": 11
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "2. Retrieval Process. Guided by the operator plan, the retrieval process executes Scent/Filter-based Retrieval. This stage navigates the BookIndex  $B = (T, G, M)$ , either utilizing a scent-based retrieval principle (e.g., following relevant entities in  $G$ ) to find information, or employing various filters (e.g., modal type) to refine the selection. After reasoning, BookRAG gets the retrieval set of highly relevant information blocks from the BookIndex.",
        "title_level": -1
      },
      "summary": "The retrieval process uses an operator plan to navigate a BookIndex, employing scent-based methods to follow relevant entities or applying filters to refine selections, ultimately obtaining a set of highly relevant information blocks."
    },
    {
      "index_id": 89,
      "parent_id": 84,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 89,
        "pdf_para_block": {
          "bbox": [
            314,
            403,
            558,
            447
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                403,
                558,
                447
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    403,
                    558,
                    447
                  ],
                  "type": "text",
                  "content": "3. Generation Process. Finally, all retrieved information enters the generation stage for Analysis & Merging. This stage synthesizes these (often fragmented) pieces of evidence, performs final analysis, and formulates a coherent response."
                }
              ]
            }
          ],
          "index": 12
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "3. Generation Process. Finally, all retrieved information enters the generation stage for Analysis & Merging. This stage synthesizes these (often fragmented) pieces of evidence, performs final analysis, and formulates a coherent response.",
        "title_level": -1
      },
      "summary": "The generation process synthesizes retrieved information by analyzing and merging fragmented evidence to produce a coherent final response."
    },
    {
      "index_id": 90,
      "parent_id": 82,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 90,
        "pdf_para_block": {
          "bbox": [
            315,
            460,
            453,
            472
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                315,
                460,
                453,
                472
              ],
              "spans": [
                {
                  "bbox": [
                    315,
                    460,
                    453,
                    472
                  ],
                  "type": "text",
                  "content": "5.2 Agent-based Planning"
                }
              ]
            }
          ],
          "index": 13
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "5.2 Agent-based Planning",
        "title_level": 2
      },
      "summary": "This section details BookRAG's agent-based planning mechanism, which classifies queries and dynamically assembles tailored pipelines from a library of four core operators to execute specific retrieval and synthesis strategies."
    },
    {
      "index_id": 91,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 91,
        "pdf_para_block": {
          "bbox": [
            313,
            474,
            559,
            562
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                474,
                559,
                562
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    474,
                    559,
                    562
                  ],
                  "type": "text",
                  "content": "The planning stage is the core of BookRAG, designed to intelligently navigate our BookIndex "
                },
                {
                  "bbox": [
                    313,
                    474,
                    559,
                    562
                  ],
                  "type": "inline_equation",
                  "content": "B = (T, G, M)"
                },
                {
                  "bbox": [
                    313,
                    474,
                    559,
                    562
                  ],
                  "type": "text",
                  "content": ". To support flexible retrieval, we define four types of operators: Formulator, Selector, Reasoner, and Synthesizer. These operators can be arbitrarily combined to form tailored execution pipelines, each with adjustable parameters. BookRAG dynamically configures and assembles these operators to adapt to the specific requirements of different query categories. This process involves two sequential steps: first, the agent performs"
                }
              ]
            }
          ],
          "index": 14
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The planning stage is the core of BookRAG, designed to intelligently navigate our BookIndex  $B = (T, G, M)$ . To support flexible retrieval, we define four types of operators: Formulator, Selector, Reasoner, and Synthesizer. These operators can be arbitrarily combined to form tailored execution pipelines, each with adjustable parameters. BookRAG dynamically configures and assembles these operators to adapt to the specific requirements of different query categories. This process involves two sequential steps: first, the agent performs son",
        "title_level": -1
      },
      "summary": "BookRAG's planning stage is the core mechanism for intelligently navigating its BookIndex. It uses four flexible operator types—Formulator, Selector, Reasoner, and Synthesizer—which can be combined into tailored pipelines with adjustable parameters. The system dynamically configures and assembles these operators in a two-step sequential process to adapt to specific query requirements."
    },
    {
      "index_id": 92,
      "parent_id": 90,
      "type": "NodeType.TABLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 92,
        "pdf_para_block": {
          "type": "table",
          "bbox": [
            52,
            586,
            559,
            707
          ],
          "blocks": [
            {
              "bbox": [
                171,
                574,
                436,
                585
              ],
              "lines": [
                {
                  "bbox": [
                    171,
                    574,
                    436,
                    585
                  ],
                  "spans": [
                    {
                      "bbox": [
                        171,
                        574,
                        436,
                        585
                      ],
                      "type": "text",
                      "content": "Table 2: Three common query categories addressed in BookRAG."
                    }
                  ]
                }
              ],
              "index": 15,
              "angle": 0,
              "type": "table_caption"
            },
            {
              "bbox": [
                52,
                586,
                559,
                707
              ],
              "lines": [
                {
                  "bbox": [
                    52,
                    586,
                    559,
                    707
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        586,
                        559,
                        707
                      ],
                      "type": "table",
                      "html": "<table><tr><td>Query Category</td><td>Description</td><td>Core Task</td><td>Example Query</td></tr><tr><td>Single-hop</td><td>Queries with a single, distinct information target.</td><td>Scent-based Retrieval: Retrieve content related to a specific entity or section.</td><td>What is the definition of Information Scent?</td></tr><tr><td>Multi-hop</td><td>Queries that require synthesizing information from multiple blocks, often by decomposing into sub-problems.</td><td>Decomposing &amp; Merging: Decompose into sub-problems, retrieve for each, and synthesize the final answer.</td><td>How does Transformer differ from RNNs in handling long-range dependencies?</td></tr><tr><td>Global Aggregation</td><td>Queries that require filtering across the entire document and performing calculations.</td><td>Filter &amp; Aggregation: Apply filters across the document &amp; perform aggregation operations (e.g., Count, Sum).</td><td>How many figures related to IFT are in Section 4?</td></tr></table>",
                      "image_path": "42828d9650c4ec8055c00adc536f13a90d3799e778e53ad0464c042f4bdb3c0a.jpg"
                    }
                  ]
                }
              ],
              "index": 16,
              "angle": 0,
              "type": "table_body"
            }
          ],
          "index": 16
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/42828d9650c4ec8055c00adc536f13a90d3799e778e53ad0464c042f4bdb3c0a.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Table 2: Three common query categories addressed in BookRAG.",
        "footnote": "",
        "table_body": "<table><tr><td>Query Category</td><td>Description</td><td>Core Task</td><td>Example Query</td></tr><tr><td>Single-hop</td><td>Queries with a single, distinct information target.</td><td>Scent-based Retrieval: Retrieve content related to a specific entity or section.</td><td>What is the definition of Information Scent?</td></tr><tr><td>Multi-hop</td><td>Queries that require synthesizing information from multiple blocks, often by decomposing into sub-problems.</td><td>Decomposing &amp; Merging: Decompose into sub-problems, retrieve for each, and synthesize the final answer.</td><td>How does Transformer differ from RNNs in handling long-range dependencies?</td></tr><tr><td>Global Aggregation</td><td>Queries that require filtering across the entire document and performing calculations.</td><td>Filter &amp; Aggregation: Apply filters across the document &amp; perform aggregation operations (e.g., Count, Sum).</td><td>How many figures related to IFT are in Section 4?</td></tr></table>",
        "content": "Table 2: Three common query categories addressed in BookRAG.",
        "title_level": -1
      },
      "summary": "BookRAG addresses three common query categories: Single-hop queries target a single piece of information for direct retrieval (e.g., \"What is the definition of Information Scent?\"). Multi-hop queries require decomposing the question, retrieving information from multiple sources, and synthesizing an answer (e.g., comparing Transformer and RNN architectures). Global Aggregation queries involve filtering across an entire document and performing calculations like counts or sums (e.g., counting figures in a specific section)."
    },
    {
      "index_id": 93,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 5,
        "page_path": null,
        "pdf_id": 93,
        "pdf_para_block": {
          "bbox": [
            302,
            714,
            307,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                714,
                307,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    714,
                    307,
                    719
                  ],
                  "type": "text",
                  "content": "6"
                }
              ]
            }
          ],
          "index": 17
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "6",
        "title_level": -1
      },
      "summary": "The content is a single number: 6."
    },
    {
      "index_id": 94,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 94,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            84,
            121,
            104,
            140
          ],
          "blocks": [
            {
              "bbox": [
                78,
                105,
                103,
                112
              ],
              "lines": [
                {
                  "bbox": [
                    78,
                    105,
                    103,
                    112
                  ],
                  "spans": [
                    {
                      "bbox": [
                        78,
                        105,
                        103,
                        112
                      ],
                      "type": "text",
                      "content": "Extract"
                    }
                  ]
                }
              ],
              "index": 2,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                84,
                121,
                104,
                140
              ],
              "lines": [
                {
                  "bbox": [
                    84,
                    121,
                    104,
                    140
                  ],
                  "spans": [
                    {
                      "bbox": [
                        84,
                        121,
                        104,
                        140
                      ],
                      "type": "image",
                      "image_path": "f19eaefcd84ecb53d7e7360f8f5c372378c15aba4d113c54649d4878cd00ca9b.jpg"
                    }
                  ]
                }
              ],
              "index": 3,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 3
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/f19eaefcd84ecb53d7e7360f8f5c372378c15aba4d113c54649d4878cd00ca9b.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Extract",
        "footnote": "",
        "table_body": null,
        "content": "Extract",
        "title_level": -1
      },
      "summary": "Extract"
    },
    {
      "index_id": 95,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 95,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            108,
            115,
            128,
            140
          ],
          "blocks": [
            {
              "bbox": [
                57,
                85,
                124,
                95
              ],
              "lines": [
                {
                  "bbox": [
                    57,
                    85,
                    124,
                    95
                  ],
                  "spans": [
                    {
                      "bbox": [
                        57,
                        85,
                        124,
                        95
                      ],
                      "type": "text",
                      "content": "(a)Operator Set"
                    }
                  ]
                }
              ],
              "index": 0,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                108,
                115,
                128,
                140
              ],
              "lines": [
                {
                  "bbox": [
                    108,
                    115,
                    128,
                    140
                  ],
                  "spans": [
                    {
                      "bbox": [
                        108,
                        115,
                        128,
                        140
                      ],
                      "type": "image",
                      "image_path": "eac764ddeb1ba6217e43bf1340fe79d8e42db56c5858fd09e08da858beb938ad.jpg"
                    }
                  ]
                }
              ],
              "index": 4,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 4
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/eac764ddeb1ba6217e43bf1340fe79d8e42db56c5858fd09e08da858beb938ad.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "(a)Operator Set",
        "footnote": "",
        "table_body": null,
        "content": "(a)Operator Set",
        "title_level": -1
      },
      "summary": "The image is titled \"Operator Set\" and labeled as part (a), indicating it is the first in a series of related visuals."
    },
    {
      "index_id": 96,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 96,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            129,
            116,
            158,
            140
          ],
          "blocks": [
            {
              "bbox": [
                129,
                116,
                158,
                140
              ],
              "lines": [
                {
                  "bbox": [
                    129,
                    116,
                    158,
                    140
                  ],
                  "spans": [
                    {
                      "bbox": [
                        129,
                        116,
                        158,
                        140
                      ],
                      "type": "image",
                      "image_path": "c9f2ad7e762c9e01362fc99ef33e3b71f47bdad994f3e669c6f44696c7de0a3b.jpg"
                    }
                  ]
                }
              ],
              "index": 5,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                131,
                142,
                158,
                149
              ],
              "lines": [
                {
                  "bbox": [
                    131,
                    142,
                    158,
                    149
                  ],
                  "spans": [
                    {
                      "bbox": [
                        131,
                        142,
                        158,
                        149
                      ],
                      "type": "text",
                      "content": "Entities"
                    }
                  ]
                }
              ],
              "index": 6,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 5
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/c9f2ad7e762c9e01362fc99ef33e3b71f47bdad994f3e669c6f44696c7de0a3b.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Entities",
        "footnote": "",
        "table_body": null,
        "content": "Entities",
        "title_level": -1
      },
      "summary": "Entities are the core subject of the image."
    },
    {
      "index_id": 97,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 97,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            84,
            157,
            104,
            182
          ],
          "blocks": [
            {
              "bbox": [
                78,
                148,
                117,
                155
              ],
              "lines": [
                {
                  "bbox": [
                    78,
                    148,
                    117,
                    155
                  ],
                  "spans": [
                    {
                      "bbox": [
                        78,
                        148,
                        117,
                        155
                      ],
                      "type": "text",
                      "content": "Decompose"
                    }
                  ]
                }
              ],
              "index": 7,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                84,
                157,
                104,
                182
              ],
              "lines": [
                {
                  "bbox": [
                    84,
                    157,
                    104,
                    182
                  ],
                  "spans": [
                    {
                      "bbox": [
                        84,
                        157,
                        104,
                        182
                      ],
                      "type": "image",
                      "image_path": "887d161ca8533b58e69544146a66e94302ca6dc10afccc72db982b5a3372894c.jpg"
                    }
                  ]
                }
              ],
              "index": 8,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 8
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/887d161ca8533b58e69544146a66e94302ca6dc10afccc72db982b5a3372894c.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Decompose",
        "footnote": "",
        "table_body": null,
        "content": "Decompose",
        "title_level": -1
      },
      "summary": "Decompose"
    },
    {
      "index_id": 98,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 98,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            108,
            158,
            128,
            182
          ],
          "blocks": [
            {
              "bbox": [
                108,
                158,
                128,
                182
              ],
              "lines": [
                {
                  "bbox": [
                    108,
                    158,
                    128,
                    182
                  ],
                  "spans": [
                    {
                      "bbox": [
                        108,
                        158,
                        128,
                        182
                      ],
                      "type": "image",
                      "image_path": "120c356a6784de036cd5489e5434389d421dccca9b0f02910288fc0c5670ebb9.jpg"
                    }
                  ]
                }
              ],
              "index": 9,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                124,
                186,
                164,
                194
              ],
              "lines": [
                {
                  "bbox": [
                    124,
                    186,
                    164,
                    194
                  ],
                  "spans": [
                    {
                      "bbox": [
                        124,
                        186,
                        164,
                        194
                      ],
                      "type": "text",
                      "content": "Sub-queries"
                    }
                  ]
                }
              ],
              "index": 11,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 9
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/120c356a6784de036cd5489e5434389d421dccca9b0f02910288fc0c5670ebb9.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Sub-queries",
        "footnote": "",
        "table_body": null,
        "content": "Sub-queries",
        "title_level": -1
      },
      "summary": "The content is an image about sub-queries, which are queries nested within another SQL statement."
    },
    {
      "index_id": 99,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 99,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            129,
            160,
            151,
            183
          ],
          "blocks": [
            {
              "bbox": [
                129,
                160,
                151,
                183
              ],
              "lines": [
                {
                  "bbox": [
                    129,
                    160,
                    151,
                    183
                  ],
                  "spans": [
                    {
                      "bbox": [
                        129,
                        160,
                        151,
                        183
                      ],
                      "type": "image",
                      "image_path": "60d3a482d79f153466d13fe7a58d74b076f5214dc0db5640c1d851b990ec828f.jpg"
                    }
                  ]
                }
              ],
              "index": 10,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                97,
                196,
                141,
                205
              ],
              "lines": [
                {
                  "bbox": [
                    97,
                    196,
                    141,
                    205
                  ],
                  "spans": [
                    {
                      "bbox": [
                        97,
                        196,
                        141,
                        205
                      ],
                      "type": "text",
                      "content": "Formulator"
                    }
                  ]
                }
              ],
              "index": 12,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 10
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/60d3a482d79f153466d13fe7a58d74b076f5214dc0db5640c1d851b990ec828f.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Formulator",
        "footnote": "",
        "table_body": null,
        "content": "Formulator",
        "title_level": -1
      },
      "summary": "Formulator"
    },
    {
      "index_id": 100,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 100,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            187,
            109,
            237,
            145
          ],
          "blocks": [
            {
              "bbox": [
                170,
                105,
                191,
                112
              ],
              "lines": [
                {
                  "bbox": [
                    170,
                    105,
                    191,
                    112
                  ],
                  "spans": [
                    {
                      "bbox": [
                        170,
                        105,
                        191,
                        112
                      ],
                      "type": "text",
                      "content": "Filter"
                    }
                  ]
                }
              ],
              "index": 13,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                187,
                109,
                237,
                145
              ],
              "lines": [
                {
                  "bbox": [
                    187,
                    109,
                    237,
                    145
                  ],
                  "spans": [
                    {
                      "bbox": [
                        187,
                        109,
                        237,
                        145
                      ],
                      "type": "image",
                      "image_path": "93a34ab5ad5fe948339d8684152b2f4755497d92541d33eecac57b73d6f5e569.jpg"
                    }
                  ]
                }
              ],
              "index": 14,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 14
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/93a34ab5ad5fe948339d8684152b2f4755497d92541d33eecac57b73d6f5e569.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Filter",
        "footnote": "",
        "table_body": null,
        "content": "Filter",
        "title_level": -1
      },
      "summary": "Filter"
    },
    {
      "index_id": 101,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 101,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            239,
            109,
            252,
            144
          ],
          "blocks": [
            {
              "bbox": [
                239,
                109,
                252,
                144
              ],
              "lines": [
                {
                  "bbox": [
                    239,
                    109,
                    252,
                    144
                  ],
                  "spans": [
                    {
                      "bbox": [
                        239,
                        109,
                        252,
                        144
                      ],
                      "type": "image",
                      "image_path": "730d2a931fd4e294e6d3da09d88ef34eb8f3bd75a6518f82c4ed03035e940ab6.jpg"
                    }
                  ]
                }
              ],
              "index": 15,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 15
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/730d2a931fd4e294e6d3da09d88ef34eb8f3bd75a6518f82c4ed03035e940ab6.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "An image is presented without any caption or descriptive text."
    },
    {
      "index_id": 102,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 102,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            258,
            107,
            283,
            144
          ],
          "blocks": [
            {
              "bbox": [
                270,
                93,
                312,
                103
              ],
              "lines": [
                {
                  "bbox": [
                    270,
                    93,
                    312,
                    103
                  ],
                  "spans": [
                    {
                      "bbox": [
                        270,
                        93,
                        312,
                        103
                      ],
                      "type": "text",
                      "content": "Operators"
                    }
                  ]
                }
              ],
              "index": 1,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                258,
                107,
                283,
                144
              ],
              "lines": [
                {
                  "bbox": [
                    258,
                    107,
                    283,
                    144
                  ],
                  "spans": [
                    {
                      "bbox": [
                        258,
                        107,
                        283,
                        144
                      ],
                      "type": "image",
                      "image_path": "ba5ccc0b4b44341b90d2b67e204845b76818c4c3cf35332af55a8e54946b7864.jpg"
                    }
                  ]
                }
              ],
              "index": 16,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 16
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/ba5ccc0b4b44341b90d2b67e204845b76818c4c3cf35332af55a8e54946b7864.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Operators",
        "footnote": "",
        "table_body": null,
        "content": "Operators",
        "title_level": -1
      },
      "summary": "The image is titled \"Operators.\""
    },
    {
      "index_id": 103,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 103,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            187,
            149,
            237,
            190
          ],
          "blocks": [
            {
              "bbox": [
                171,
                147,
                193,
                154
              ],
              "lines": [
                {
                  "bbox": [
                    171,
                    147,
                    193,
                    154
                  ],
                  "spans": [
                    {
                      "bbox": [
                        171,
                        147,
                        193,
                        154
                      ],
                      "type": "text",
                      "content": "Select"
                    }
                  ]
                }
              ],
              "index": 17,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                187,
                149,
                237,
                190
              ],
              "lines": [
                {
                  "bbox": [
                    187,
                    149,
                    237,
                    190
                  ],
                  "spans": [
                    {
                      "bbox": [
                        187,
                        149,
                        237,
                        190
                      ],
                      "type": "image",
                      "image_path": "a39e00531416a4c1ff35758750b0cf8c9f03ae133db29c459d806da9f82b10d9.jpg"
                    }
                  ]
                }
              ],
              "index": 18,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                186,
                190,
                231,
                198
              ],
              "lines": [
                {
                  "bbox": [
                    186,
                    190,
                    231,
                    198
                  ],
                  "spans": [
                    {
                      "bbox": [
                        186,
                        190,
                        231,
                        198
                      ],
                      "type": "text",
                      "content": "0-O-0"
                    }
                  ]
                }
              ],
              "index": 19,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                219,
                197,
                253,
                205
              ],
              "lines": [
                {
                  "bbox": [
                    219,
                    197,
                    253,
                    205
                  ],
                  "spans": [
                    {
                      "bbox": [
                        219,
                        197,
                        253,
                        205
                      ],
                      "type": "text",
                      "content": "Selector"
                    }
                  ]
                }
              ],
              "index": 20,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 18
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/a39e00531416a4c1ff35758750b0cf8c9f03ae133db29c459d806da9f82b10d9.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Select 0-O-0 Selector",
        "footnote": "",
        "table_body": null,
        "content": "Select 0-O-0 Selector",
        "title_level": -1
      },
      "summary": "The image depicts a control interface labeled \"Select 0-O-0 Selector,\" indicating a mechanism for choosing between three distinct options or states."
    },
    {
      "index_id": 104,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 104,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            299,
            122,
            312,
            144
          ],
          "blocks": [
            {
              "bbox": [
                298,
                113,
                312,
                121
              ],
              "lines": [
                {
                  "bbox": [
                    298,
                    113,
                    312,
                    121
                  ],
                  "spans": [
                    {
                      "bbox": [
                        298,
                        113,
                        312,
                        121
                      ],
                      "type": "text",
                      "content": "1"
                    }
                  ]
                }
              ],
              "index": 22,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                299,
                122,
                312,
                144
              ],
              "lines": [
                {
                  "bbox": [
                    299,
                    122,
                    312,
                    144
                  ],
                  "spans": [
                    {
                      "bbox": [
                        299,
                        122,
                        312,
                        144
                      ],
                      "type": "image",
                      "image_path": "bd8d664781f2cbd66b40b9713cd26932b666996303e90d7415895b79d901a12f.jpg"
                    }
                  ]
                }
              ],
              "index": 23,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                296,
                147,
                320,
                155
              ],
              "lines": [
                {
                  "bbox": [
                    296,
                    147,
                    320,
                    155
                  ],
                  "spans": [
                    {
                      "bbox": [
                        296,
                        147,
                        320,
                        155
                      ],
                      "type": "text",
                      "content": "Skyline"
                    }
                  ]
                }
              ],
              "index": 33,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                308,
                156,
                317,
                163
              ],
              "lines": [
                {
                  "bbox": [
                    308,
                    156,
                    317,
                    163
                  ],
                  "spans": [
                    {
                      "bbox": [
                        308,
                        156,
                        317,
                        163
                      ],
                      "type": "text",
                      "content": "S:"
                    }
                  ]
                }
              ],
              "index": 34,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                307,
                166,
                317,
                173
              ],
              "lines": [
                {
                  "bbox": [
                    307,
                    166,
                    317,
                    173
                  ],
                  "spans": [
                    {
                      "bbox": [
                        307,
                        166,
                        317,
                        173
                      ],
                      "type": "text",
                      "content": "."
                    }
                  ]
                }
              ],
              "index": 38,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                307,
                176,
                317,
                182
              ],
              "lines": [
                {
                  "bbox": [
                    307,
                    176,
                    317,
                    182
                  ],
                  "spans": [
                    {
                      "bbox": [
                        307,
                        176,
                        317,
                        182
                      ],
                      "type": "text",
                      "content": "2"
                    }
                  ]
                }
              ],
              "index": 42,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 23
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/bd8d664781f2cbd66b40b9713cd26932b666996303e90d7415895b79d901a12f.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "1 Skyline S: . 2",
        "footnote": "",
        "table_body": null,
        "content": "1 Skyline S: . 2",
        "title_level": -1
      },
      "summary": "The image depicts a skyline labeled \"Skyline S\" with a numerical identifier \"1\" and a secondary notation \"2\"."
    },
    {
      "index_id": 105,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 105,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            328,
            103,
            371,
            144
          ],
          "blocks": [
            {
              "bbox": [
                298,
                105,
                321,
                112
              ],
              "lines": [
                {
                  "bbox": [
                    298,
                    105,
                    321,
                    112
                  ],
                  "spans": [
                    {
                      "bbox": [
                        298,
                        105,
                        321,
                        112
                      ],
                      "type": "text",
                      "content": "Reason"
                    }
                  ]
                }
              ],
              "index": 21,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                328,
                103,
                371,
                144
              ],
              "lines": [
                {
                  "bbox": [
                    328,
                    103,
                    371,
                    144
                  ],
                  "spans": [
                    {
                      "bbox": [
                        328,
                        103,
                        371,
                        144
                      ],
                      "type": "image",
                      "image_path": "0793bb5febaf3afc37968f02f9959f5ad6510f80ac9faf708f28fd361f1ab273.jpg"
                    }
                  ]
                }
              ],
              "index": 25,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                372,
                118,
                380,
                124
              ],
              "lines": [
                {
                  "bbox": [
                    372,
                    118,
                    380,
                    124
                  ],
                  "spans": [
                    {
                      "bbox": [
                        372,
                        118,
                        380,
                        124
                      ],
                      "type": "text",
                      "content": "5"
                    }
                  ]
                }
              ],
              "index": 26,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                322,
                156,
                331,
                163
              ],
              "lines": [
                {
                  "bbox": [
                    322,
                    156,
                    331,
                    163
                  ],
                  "spans": [
                    {
                      "bbox": [
                        322,
                        156,
                        331,
                        163
                      ],
                      "type": "text",
                      "content": "06"
                    }
                  ]
                }
              ],
              "index": 35,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                332,
                156,
                345,
                163
              ],
              "lines": [
                {
                  "bbox": [
                    332,
                    156,
                    345,
                    163
                  ],
                  "spans": [
                    {
                      "bbox": [
                        332,
                        156,
                        345,
                        163
                      ],
                      "type": "text",
                      "content": "05"
                    }
                  ]
                }
              ],
              "index": 36,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                348,
                156,
                357,
                163
              ],
              "lines": [
                {
                  "bbox": [
                    348,
                    156,
                    357,
                    163
                  ],
                  "spans": [
                    {
                      "bbox": [
                        348,
                        156,
                        357,
                        163
                      ],
                      "type": "text",
                      "content": "04"
                    }
                  ]
                }
              ],
              "index": 37,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                321,
                166,
                331,
                173
              ],
              "lines": [
                {
                  "bbox": [
                    321,
                    166,
                    331,
                    173
                  ],
                  "spans": [
                    {
                      "bbox": [
                        321,
                        166,
                        331,
                        173
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 39,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                332,
                166,
                345,
                173
              ],
              "lines": [
                {
                  "bbox": [
                    332,
                    166,
                    345,
                    173
                  ],
                  "spans": [
                    {
                      "bbox": [
                        332,
                        166,
                        345,
                        173
                      ],
                      "type": "text",
                      "content": "."
                    }
                  ]
                }
              ],
              "index": 40,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                321,
                176,
                331,
                182
              ],
              "lines": [
                {
                  "bbox": [
                    321,
                    176,
                    331,
                    182
                  ],
                  "spans": [
                    {
                      "bbox": [
                        321,
                        176,
                        331,
                        182
                      ],
                      "type": "text",
                      "content": "61"
                    }
                  ]
                }
              ],
              "index": 43,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 25
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/0793bb5febaf3afc37968f02f9959f5ad6510f80ac9faf708f28fd361f1ab273.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Reason 5 06 05 04 # . 61",
        "footnote": "",
        "table_body": null,
        "content": "Reason 5 06 05 04 # . 61",
        "title_level": -1
      },
      "summary": "The image presents a series of numbers and labels: \"Reason 5 06 05 04 # . 61\". The sequence suggests a list or code, with \"Reason\" likely being a title or header for the following numerical data points."
    },
    {
      "index_id": 106,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 106,
        "pdf_para_block": {
          "bbox": [
            386,
            127,
            406,
            140
          ],
          "angle": 0,
          "lines": [
            {
              "bbox": [
                386,
                127,
                406,
                140
              ],
              "spans": [
                {
                  "bbox": [
                    386,
                    127,
                    406,
                    140
                  ],
                  "type": "image",
                  "image_path": "7907041d2b973cf99c98c08c4c3428174dda45e137f6540aef95efd39f9ec5ad.jpg"
                }
              ]
            }
          ],
          "index": 28,
          "type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The global population is projected to reach 9.7 billion by 2050, with the most significant growth occurring in Africa and Asia. This growth will be unevenly distributed, as many developed regions, including Europe and Northern America, will see little to no population increase and will experience significant population aging."
    },
    {
      "index_id": 107,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 107,
        "pdf_para_block": {
          "bbox": [
            386,
            127,
            406,
            140
          ],
          "angle": 0,
          "lines": [
            {
              "bbox": [
                386,
                127,
                406,
                140
              ],
              "spans": [
                {
                  "bbox": [
                    386,
                    127,
                    406,
                    140
                  ],
                  "type": "image",
                  "image_path": "7907041d2b973cf99c98c08c4c3428174dda45e137f6540aef95efd39f9ec5ad.jpg"
                }
              ]
            }
          ],
          "index": 31,
          "type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The data shows a clear correlation between a country's GDP per capita and its average life expectancy, with wealthier nations generally having longer-lived populations. For example, Country A, with a GDP per capita of $60,000, has a life expectancy of 82 years, while Country D, with a GDP per capita of $3,000, has a life expectancy of 65 years. This trend indicates that higher economic resources are strongly associated with better health outcomes and longer lifespans."
    },
    {
      "index_id": 108,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 108,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            407,
            126,
            425,
            140
          ],
          "blocks": [
            {
              "bbox": [
                386,
                118,
                406,
                125
              ],
              "lines": [
                {
                  "bbox": [
                    386,
                    118,
                    406,
                    125
                  ],
                  "spans": [
                    {
                      "bbox": [
                        386,
                        118,
                        406,
                        125
                      ],
                      "type": "text",
                      "content": "0.6 0."
                    }
                  ]
                }
              ],
              "index": 27,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                407,
                106,
                421,
                113
              ],
              "lines": [
                {
                  "bbox": [
                    407,
                    106,
                    421,
                    113
                  ],
                  "spans": [
                    {
                      "bbox": [
                        407,
                        106,
                        421,
                        113
                      ],
                      "type": "text",
                      "content": ""
                    }
                  ]
                }
              ],
              "index": 29,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                407,
                117,
                425,
                124
              ],
              "lines": [
                {
                  "bbox": [
                    407,
                    117,
                    425,
                    124
                  ],
                  "spans": [
                    {
                      "bbox": [
                        407,
                        117,
                        425,
                        124
                      ],
                      "type": "text",
                      "content": "5.04"
                    }
                  ]
                }
              ],
              "index": 30,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                407,
                126,
                425,
                140
              ],
              "lines": [
                {
                  "bbox": [
                    407,
                    126,
                    425,
                    140
                  ],
                  "spans": [
                    {
                      "bbox": [
                        407,
                        126,
                        425,
                        140
                      ],
                      "type": "image",
                      "image_path": "851eefb05a7d0b60f4e2f8b3699a51bfd10aa1abf4a8edb2af1a1f8d28e91e7c.jpg"
                    }
                  ]
                }
              ],
              "index": 32,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                438,
                106,
                455,
                113
              ],
              "lines": [
                {
                  "bbox": [
                    438,
                    106,
                    455,
                    113
                  ],
                  "spans": [
                    {
                      "bbox": [
                        438,
                        106,
                        455,
                        113
                      ],
                      "type": "text",
                      "content": "Map"
                    }
                  ]
                }
              ],
              "index": 55,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                443,
                115,
                452,
                121
              ],
              "lines": [
                {
                  "bbox": [
                    443,
                    115,
                    452,
                    121
                  ],
                  "spans": [
                    {
                      "bbox": [
                        443,
                        115,
                        452,
                        121
                      ],
                      "type": "text",
                      "content": ""
                    }
                  ]
                }
              ],
              "index": 56,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                457,
                106,
                469,
                113
              ],
              "lines": [
                {
                  "bbox": [
                    457,
                    106,
                    469,
                    113
                  ],
                  "spans": [
                    {
                      "bbox": [
                        457,
                        106,
                        469,
                        113
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 57,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                458,
                115,
                469,
                121
              ],
              "lines": [
                {
                  "bbox": [
                    458,
                    115,
                    469,
                    121
                  ],
                  "spans": [
                    {
                      "bbox": [
                        458,
                        115,
                        469,
                        121
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 61,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 32
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/851eefb05a7d0b60f4e2f8b3699a51bfd10aa1abf4a8edb2af1a1f8d28e91e7c.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "0.6 0.  5.04 Map  # #",
        "footnote": "",
        "table_body": null,
        "content": "0.6 0.  5.04 Map  # #",
        "title_level": -1
      },
      "summary": "The image shows a map with coordinates 0.6, 0.0, and 5.04, and includes map symbols represented by hash marks (#)."
    },
    {
      "index_id": 109,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 109,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            375,
            150,
            395,
            175
          ],
          "blocks": [
            {
              "bbox": [
                348,
                166,
                357,
                173
              ],
              "lines": [
                {
                  "bbox": [
                    348,
                    166,
                    357,
                    173
                  ],
                  "spans": [
                    {
                      "bbox": [
                        348,
                        166,
                        357,
                        173
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 41,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                332,
                176,
                345,
                182
              ],
              "lines": [
                {
                  "bbox": [
                    332,
                    176,
                    345,
                    182
                  ],
                  "spans": [
                    {
                      "bbox": [
                        332,
                        176,
                        345,
                        182
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 44,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                347,
                183,
                356,
                190
              ],
              "lines": [
                {
                  "bbox": [
                    347,
                    183,
                    356,
                    190
                  ],
                  "spans": [
                    {
                      "bbox": [
                        347,
                        183,
                        356,
                        190
                      ],
                      "type": "text",
                      "content": "0"
                    }
                  ]
                }
              ],
              "index": 47,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                357,
                183,
                373,
                190
              ],
              "lines": [
                {
                  "bbox": [
                    357,
                    183,
                    373,
                    190
                  ],
                  "spans": [
                    {
                      "bbox": [
                        357,
                        183,
                        373,
                        190
                      ],
                      "type": "text",
                      "content": "A三"
                    }
                  ]
                }
              ],
              "index": 48,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                347,
                190,
                373,
                197
              ],
              "lines": [
                {
                  "bbox": [
                    347,
                    190,
                    373,
                    197
                  ],
                  "spans": [
                    {
                      "bbox": [
                        347,
                        190,
                        373,
                        197
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 49,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                375,
                150,
                395,
                175
              ],
              "lines": [
                {
                  "bbox": [
                    375,
                    150,
                    395,
                    175
                  ],
                  "spans": [
                    {
                      "bbox": [
                        375,
                        150,
                        395,
                        175
                      ],
                      "type": "image",
                      "image_path": "4569ebd09f34d6d2ac956f104570ce2c3005507fe4b2299351e660be37b140f1.jpg"
                    }
                  ]
                }
              ],
              "index": 50,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                347,
                197,
                384,
                205
              ],
              "lines": [
                {
                  "bbox": [
                    347,
                    197,
                    384,
                    205
                  ],
                  "spans": [
                    {
                      "bbox": [
                        347,
                        197,
                        384,
                        205
                      ],
                      "type": "text",
                      "content": "Reasoner"
                    }
                  ]
                }
              ],
              "index": 54,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 50
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/4569ebd09f34d6d2ac956f104570ce2c3005507fe4b2299351e660be37b140f1.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "# # 0 A三 # Reasoner",
        "footnote": "",
        "table_body": null,
        "content": "# # 0 A三 # Reasoner",
        "title_level": -1
      },
      "summary": "The image is captioned \"# # 0 A三 # Reasoner\"."
    },
    {
      "index_id": 110,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 110,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            395,
            150,
            410,
            175
          ],
          "blocks": [
            {
              "bbox": [
                395,
                150,
                410,
                175
              ],
              "lines": [
                {
                  "bbox": [
                    395,
                    150,
                    410,
                    175
                  ],
                  "spans": [
                    {
                      "bbox": [
                        395,
                        150,
                        410,
                        175
                      ],
                      "type": "image",
                      "image_path": "b932f3af46d22c3c9994065172ad60fde618b39621e7f5368f848e8db04bf0ff.jpg"
                    }
                  ]
                }
              ],
              "index": 51,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 51
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/b932f3af46d22c3c9994065172ad60fde618b39621e7f5368f848e8db04bf0ff.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is described as having a caption, but the caption text itself is not provided in the content. Therefore, the summary cannot be generated as the essential information from the image is missing."
    },
    {
      "index_id": 111,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 111,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            410,
            148,
            425,
            175
          ],
          "blocks": [
            {
              "bbox": [
                410,
                148,
                425,
                175
              ],
              "lines": [
                {
                  "bbox": [
                    410,
                    148,
                    425,
                    175
                  ],
                  "spans": [
                    {
                      "bbox": [
                        410,
                        148,
                        425,
                        175
                      ],
                      "type": "image",
                      "image_path": "39c8b7a36052272e783ea8c99ef73731c71efa1d708d7cf352331ed93bf89269.jpg"
                    }
                  ]
                }
              ],
              "index": 52,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 52
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/39c8b7a36052272e783ea8c99ef73731c71efa1d708d7cf352331ed93bf89269.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image's caption is not provided, leaving no specific content to summarize."
    },
    {
      "index_id": 112,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 112,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            410,
            176,
            425,
            198
          ],
          "blocks": [
            {
              "bbox": [
                410,
                176,
                425,
                198
              ],
              "lines": [
                {
                  "bbox": [
                    410,
                    176,
                    425,
                    198
                  ],
                  "spans": [
                    {
                      "bbox": [
                        410,
                        176,
                        425,
                        198
                      ],
                      "type": "image",
                      "image_path": "1112adbc2a0f82ec7849262d3b2a6dacf6eeb9906fe7f19ebda7d8fc3e4e79c0.jpg"
                    }
                  ]
                }
              ],
              "index": 53,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 53
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/1112adbc2a0f82ec7849262d3b2a6dacf6eeb9906fe7f19ebda7d8fc3e4e79c0.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image's caption is missing; therefore, no specific content or key points can be summarized from the provided information."
    },
    {
      "index_id": 113,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 113,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            481,
            156,
            500,
            174
          ],
          "blocks": [
            {
              "bbox": [
                474,
                106,
                485,
                113
              ],
              "lines": [
                {
                  "bbox": [
                    474,
                    106,
                    485,
                    113
                  ],
                  "spans": [
                    {
                      "bbox": [
                        474,
                        106,
                        485,
                        113
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 58,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                487,
                106,
                499,
                113
              ],
              "lines": [
                {
                  "bbox": [
                    487,
                    106,
                    499,
                    113
                  ],
                  "spans": [
                    {
                      "bbox": [
                        487,
                        106,
                        499,
                        113
                      ],
                      "type": "text",
                      "content": ""
                    }
                  ]
                }
              ],
              "index": 59,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                510,
                106,
                527,
                113
              ],
              "lines": [
                {
                  "bbox": [
                    510,
                    106,
                    527,
                    113
                  ],
                  "spans": [
                    {
                      "bbox": [
                        510,
                        106,
                        527,
                        113
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 60,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                474,
                115,
                485,
                121
              ],
              "lines": [
                {
                  "bbox": [
                    474,
                    115,
                    485,
                    121
                  ],
                  "spans": [
                    {
                      "bbox": [
                        474,
                        115,
                        485,
                        121
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 62,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                492,
                115,
                510,
                121
              ],
              "lines": [
                {
                  "bbox": [
                    492,
                    115,
                    510,
                    121
                  ],
                  "spans": [
                    {
                      "bbox": [
                        492,
                        115,
                        510,
                        121
                      ],
                      "type": "text",
                      "content": "→"
                    }
                  ]
                }
              ],
              "index": 63,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                511,
                115,
                527,
                121
              ],
              "lines": [
                {
                  "bbox": [
                    511,
                    115,
                    527,
                    121
                  ],
                  "spans": [
                    {
                      "bbox": [
                        511,
                        115,
                        527,
                        121
                      ],
                      "type": "text",
                      "content": "二"
                    }
                  ]
                }
              ],
              "index": 64,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                458,
                127,
                469,
                134
              ],
              "lines": [
                {
                  "bbox": [
                    458,
                    127,
                    469,
                    134
                  ],
                  "spans": [
                    {
                      "bbox": [
                        458,
                        127,
                        469,
                        134
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 65,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                474,
                127,
                485,
                134
              ],
              "lines": [
                {
                  "bbox": [
                    474,
                    127,
                    485,
                    134
                  ],
                  "spans": [
                    {
                      "bbox": [
                        474,
                        127,
                        485,
                        134
                      ],
                      "type": "text",
                      "content": "A="
                    }
                  ]
                }
              ],
              "index": 66,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                492,
                127,
                510,
                134
              ],
              "lines": [
                {
                  "bbox": [
                    492,
                    127,
                    510,
                    134
                  ],
                  "spans": [
                    {
                      "bbox": [
                        492,
                        127,
                        510,
                        134
                      ],
                      "type": "inline_equation",
                      "content": "\\rightarrow"
                    }
                  ]
                }
              ],
              "index": 67,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                458,
                147,
                462,
                153
              ],
              "lines": [
                {
                  "bbox": [
                    458,
                    147,
                    462,
                    153
                  ],
                  "spans": [
                    {
                      "bbox": [
                        458,
                        147,
                        462,
                        153
                      ],
                      "type": "text",
                      "content": "e"
                    }
                  ]
                }
              ],
              "index": 69,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                468,
                147,
                479,
                153
              ],
              "lines": [
                {
                  "bbox": [
                    468,
                    147,
                    479,
                    153
                  ],
                  "spans": [
                    {
                      "bbox": [
                        468,
                        147,
                        479,
                        153
                      ],
                      "type": "text",
                      "content": ""
                    }
                  ]
                }
              ],
              "index": 70,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                484,
                147,
                501,
                153
              ],
              "lines": [
                {
                  "bbox": [
                    484,
                    147,
                    501,
                    153
                  ],
                  "spans": [
                    {
                      "bbox": [
                        484,
                        147,
                        501,
                        153
                      ],
                      "type": "text",
                      "content": ""
                    }
                  ]
                }
              ],
              "index": 71,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                502,
                147,
                519,
                153
              ],
              "lines": [
                {
                  "bbox": [
                    502,
                    147,
                    519,
                    153
                  ],
                  "spans": [
                    {
                      "bbox": [
                        502,
                        147,
                        519,
                        153
                      ],
                      "type": "text",
                      "content": ""
                    }
                  ]
                }
              ],
              "index": 72,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                481,
                156,
                500,
                174
              ],
              "lines": [
                {
                  "bbox": [
                    481,
                    156,
                    500,
                    174
                  ],
                  "spans": [
                    {
                      "bbox": [
                        481,
                        156,
                        500,
                        174
                      ],
                      "type": "image",
                      "image_path": "1eb4c8756e38d04a4e32f8c77eaa54ce9ffabde9ac67f5abf5892598e2bf6515.jpg"
                    }
                  ]
                }
              ],
              "index": 73,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 73
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/1eb4c8756e38d04a4e32f8c77eaa54ce9ffabde9ac67f5abf5892598e2bf6515.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "#  # # → 二 # A= $\\rightarrow$ e   ",
        "footnote": "",
        "table_body": null,
        "content": "#  # # → 二 # A= $\\rightarrow$ e   ",
        "title_level": -1
      },
      "summary": "The image caption appears to be a mix of symbols, arrows, and mathematical notation, including a right arrow and the equation \"A= $\\rightarrow$ e\", suggesting a focus on a transformation, mapping, or assignment process."
    },
    {
      "index_id": 114,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 114,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            507,
            158,
            520,
            183
          ],
          "blocks": [
            {
              "bbox": [
                511,
                127,
                527,
                134
              ],
              "lines": [
                {
                  "bbox": [
                    511,
                    127,
                    527,
                    134
                  ],
                  "spans": [
                    {
                      "bbox": [
                        511,
                        127,
                        527,
                        134
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 68,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                507,
                158,
                520,
                183
              ],
              "lines": [
                {
                  "bbox": [
                    507,
                    158,
                    520,
                    183
                  ],
                  "spans": [
                    {
                      "bbox": [
                        507,
                        158,
                        520,
                        183
                      ],
                      "type": "image",
                      "image_path": "c9a4641fb3c324af5d749d56348cb5da0fbd43f85d3f58a456fde61dbddb33b9.jpg"
                    }
                  ]
                }
              ],
              "index": 74,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 74
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/c9a4641fb3c324af5d749d56348cb5da0fbd43f85d3f58a456fde61dbddb33b9.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "#",
        "footnote": "",
        "table_body": null,
        "content": "#",
        "title_level": -1
      },
      "summary": "#"
    },
    {
      "index_id": 115,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 115,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            525,
            158,
            536,
            183
          ],
          "blocks": [
            {
              "bbox": [
                525,
                158,
                536,
                183
              ],
              "lines": [
                {
                  "bbox": [
                    525,
                    158,
                    536,
                    183
                  ],
                  "spans": [
                    {
                      "bbox": [
                        525,
                        158,
                        536,
                        183
                      ],
                      "type": "image",
                      "image_path": "06e93b3139ee5f650ce4866f2154f08611261073a16fd12988ce8576909fce63.jpg"
                    }
                  ]
                }
              ],
              "index": 75,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                531,
                186,
                544,
                194
              ],
              "lines": [
                {
                  "bbox": [
                    531,
                    186,
                    544,
                    194
                  ],
                  "spans": [
                    {
                      "bbox": [
                        531,
                        186,
                        544,
                        194
                      ],
                      "type": "text",
                      "content": "+ + + + +"
                    }
                  ]
                }
              ],
              "index": 79,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 75
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/06e93b3139ee5f650ce4866f2154f08611261073a16fd12988ce8576909fce63.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "+ + + + +",
        "footnote": "",
        "table_body": null,
        "content": "+ + + + +",
        "title_level": -1
      },
      "summary": "The content is an image with a caption consisting of five plus symbols."
    },
    {
      "index_id": 116,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 116,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            463,
            175,
            482,
            194
          ],
          "blocks": [
            {
              "bbox": [
                463,
                175,
                482,
                194
              ],
              "lines": [
                {
                  "bbox": [
                    463,
                    175,
                    482,
                    194
                  ],
                  "spans": [
                    {
                      "bbox": [
                        463,
                        175,
                        482,
                        194
                      ],
                      "type": "image",
                      "image_path": "40257d2ef99b391ab820acc9597b9292c4c0eb91710654e7e64c5616f973a96d.jpg"
                    }
                  ]
                }
              ],
              "index": 76,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                462,
                197,
                511,
                205
              ],
              "lines": [
                {
                  "bbox": [
                    462,
                    197,
                    511,
                    205
                  ],
                  "spans": [
                    {
                      "bbox": [
                        462,
                        197,
                        511,
                        205
                      ],
                      "type": "text",
                      "content": "Synthesizer"
                    }
                  ]
                }
              ],
              "index": 77,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 76
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/40257d2ef99b391ab820acc9597b9292c4c0eb91710654e7e64c5616f973a96d.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Synthesizer",
        "footnote": "",
        "table_body": null,
        "content": "Synthesizer",
        "title_level": -1
      },
      "summary": "Synthesizer"
    },
    {
      "index_id": 117,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 117,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            531,
            175,
            547,
            183
          ],
          "blocks": [
            {
              "bbox": [
                531,
                175,
                547,
                183
              ],
              "lines": [
                {
                  "bbox": [
                    531,
                    175,
                    547,
                    183
                  ],
                  "spans": [
                    {
                      "bbox": [
                        531,
                        175,
                        547,
                        183
                      ],
                      "type": "image",
                      "image_path": "e4502da93ef401e83c3952ba7415624da847ad09f1031d8f0f9530e9b4ee6c35.jpg"
                    }
                  ]
                }
              ],
              "index": 78,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 78
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/e4502da93ef401e83c3952ba7415624da847ad09f1031d8f0f9530e9b4ee6c35.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image's caption is missing; therefore, no specific content or key points can be summarized from the provided information."
    },
    {
      "index_id": 118,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 118,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            145,
            217,
            165,
            240
          ],
          "blocks": [
            {
              "bbox": [
                58,
                287,
                145,
                297
              ],
              "lines": [
                {
                  "bbox": [
                    58,
                    287,
                    145,
                    297
                  ],
                  "spans": [
                    {
                      "bbox": [
                        58,
                        287,
                        145,
                        297
                      ],
                      "type": "text",
                      "content": "(b) Execution example"
                    }
                  ]
                }
              ],
              "index": 80,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                145,
                217,
                165,
                240
              ],
              "lines": [
                {
                  "bbox": [
                    145,
                    217,
                    165,
                    240
                  ],
                  "spans": [
                    {
                      "bbox": [
                        145,
                        217,
                        165,
                        240
                      ],
                      "type": "image",
                      "image_path": "eaa3e31a2d74e725ea6251775b6494f0b35ccdc8c20340fa94a4fe2266597204.jpg"
                    }
                  ]
                }
              ],
              "index": 81,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                174,
                232,
                200,
                240
              ],
              "lines": [
                {
                  "bbox": [
                    174,
                    232,
                    200,
                    240
                  ],
                  "spans": [
                    {
                      "bbox": [
                        174,
                        232,
                        200,
                        240
                      ],
                      "type": "text",
                      "content": "Planning"
                    }
                  ]
                }
              ],
              "index": 82,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                151,
                241,
                219,
                249
              ],
              "lines": [
                {
                  "bbox": [
                    151,
                    241,
                    219,
                    249
                  ],
                  "spans": [
                    {
                      "bbox": [
                        151,
                        241,
                        219,
                        249
                      ],
                      "type": "text",
                      "content": "This is a Simple query..."
                    }
                  ]
                }
              ],
              "index": 83,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                151,
                249,
                194,
                255
              ],
              "lines": [
                {
                  "bbox": [
                    151,
                    249,
                    194,
                    255
                  ],
                  "spans": [
                    {
                      "bbox": [
                        151,
                        249,
                        194,
                        255
                      ],
                      "type": "text",
                      "content": "Operator Plan"
                    }
                  ]
                }
              ],
              "index": 84,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                151,
                255,
                219,
                262
              ],
              "lines": [
                {
                  "bbox": [
                    151,
                    255,
                    219,
                    262
                  ],
                  "spans": [
                    {
                      "bbox": [
                        151,
                        255,
                        219,
                        262
                      ],
                      "type": "text",
                      "content": "Extract->Select->Reason"
                    }
                  ]
                }
              ],
              "index": 85,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                151,
                262,
                195,
                270
              ],
              "lines": [
                {
                  "bbox": [
                    151,
                    262,
                    195,
                    270
                  ],
                  "spans": [
                    {
                      "bbox": [
                        151,
                        262,
                        195,
                        270
                      ],
                      "type": "text",
                      "content": "->Skyline->Map."
                    }
                  ]
                }
              ],
              "index": 86,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                151,
                274,
                221,
                281
              ],
              "lines": [
                {
                  "bbox": [
                    151,
                    274,
                    221,
                    281
                  ],
                  "spans": [
                    {
                      "bbox": [
                        151,
                        274,
                        221,
                        281
                      ],
                      "type": "text",
                      "content": "Car and Ranking Prompt"
                    }
                  ]
                }
              ],
              "index": 87,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                151,
                281,
                221,
                288
              ],
              "lines": [
                {
                  "bbox": [
                    151,
                    281,
                    221,
                    288
                  ],
                  "spans": [
                    {
                      "bbox": [
                        151,
                        281,
                        221,
                        288
                      ],
                      "type": "text",
                      "content": "are entities in the"
                    }
                  ]
                }
              ],
              "index": 88,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                151,
                288,
                194,
                297
              ],
              "lines": [
                {
                  "bbox": [
                    151,
                    288,
                    194,
                    297
                  ],
                  "spans": [
                    {
                      "bbox": [
                        151,
                        288,
                        194,
                        297
                      ],
                      "type": "text",
                      "content": "question..."
                    }
                  ]
                }
              ],
              "index": 89,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 81
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/eaa3e31a2d74e725ea6251775b6494f0b35ccdc8c20340fa94a4fe2266597204.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "(b) Execution example Planning This is a Simple query... Operator Plan Extract->Select->Reason ->Skyline->Map. Car and Ranking Prompt are entities in the question...",
        "footnote": "",
        "table_body": null,
        "content": "(b) Execution example Planning This is a Simple query... Operator Plan Extract->Select->Reason ->Skyline->Map. Car and Ranking Prompt are entities in the question...",
        "title_level": -1
      },
      "summary": "The image illustrates a query execution plan with the sequence: Extract, Select, Reason, Skyline, and Map. It identifies \"Car\" and \"Ranking Prompt\" as key entities within the query."
    },
    {
      "index_id": 119,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 119,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            287,
            217,
            328,
            240
          ],
          "blocks": [
            {
              "bbox": [
                322,
                183,
                331,
                190
              ],
              "lines": [
                {
                  "bbox": [
                    322,
                    183,
                    331,
                    190
                  ],
                  "spans": [
                    {
                      "bbox": [
                        322,
                        183,
                        331,
                        190
                      ],
                      "type": "text",
                      "content": "#"
                    }
                  ]
                }
              ],
              "index": 45,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                332,
                183,
                345,
                190
              ],
              "lines": [
                {
                  "bbox": [
                    332,
                    183,
                    345,
                    190
                  ],
                  "spans": [
                    {
                      "bbox": [
                        332,
                        183,
                        345,
                        190
                      ],
                      "type": "text",
                      "content": "A3"
                    }
                  ]
                }
              ],
              "index": 46,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                226,
                287,
                235,
                294
              ],
              "lines": [
                {
                  "bbox": [
                    226,
                    287,
                    235,
                    294
                  ],
                  "spans": [
                    {
                      "bbox": [
                        226,
                        287,
                        235,
                        294
                      ],
                      "type": "text",
                      "content": ""
                    }
                  ]
                }
              ],
              "index": 90,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                235,
                287,
                256,
                294
              ],
              "lines": [
                {
                  "bbox": [
                    235,
                    287,
                    256,
                    294
                  ],
                  "spans": [
                    {
                      "bbox": [
                        235,
                        287,
                        256,
                        294
                      ],
                      "type": "text",
                      "content": "Prompt"
                    }
                  ]
                }
              ],
              "index": 91,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                257,
                287,
                272,
                294
              ],
              "lines": [
                {
                  "bbox": [
                    257,
                    287,
                    272,
                    294
                  ],
                  "spans": [
                    {
                      "bbox": [
                        257,
                        287,
                        272,
                        294
                      ],
                      "type": "text",
                      "content": "三"
                    }
                  ]
                }
              ],
              "index": 92,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                272,
                287,
                287,
                294
              ],
              "lines": [
                {
                  "bbox": [
                    272,
                    287,
                    287,
                    294
                  ],
                  "spans": [
                    {
                      "bbox": [
                        272,
                        287,
                        287,
                        294
                      ],
                      "type": "text",
                      "content": "等"
                    }
                  ]
                }
              ],
              "index": 93,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                287,
                217,
                328,
                240
              ],
              "lines": [
                {
                  "bbox": [
                    287,
                    217,
                    328,
                    240
                  ],
                  "spans": [
                    {
                      "bbox": [
                        287,
                        217,
                        328,
                        240
                      ],
                      "type": "image",
                      "image_path": "ab99814bbfbdd17a5fe1bf840430a5e6698885389014d2de98c0c04b106d08eb.jpg"
                    }
                  ]
                }
              ],
              "index": 94,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                296,
                241,
                316,
                247
              ],
              "lines": [
                {
                  "bbox": [
                    296,
                    241,
                    316,
                    247
                  ],
                  "spans": [
                    {
                      "bbox": [
                        296,
                        241,
                        316,
                        247
                      ],
                      "type": "inline_equation",
                      "content": "\\therefore m - 1 \\neq  0"
                    },
                    {
                      "bbox": [
                        296,
                        241,
                        316,
                        247
                      ],
                      "type": "text",
                      "content": " ;"
                    }
                  ]
                }
              ],
              "index": 95,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                296,
                248,
                317,
                254
              ],
              "lines": [
                {
                  "bbox": [
                    296,
                    248,
                    317,
                    254
                  ],
                  "spans": [
                    {
                      "bbox": [
                        296,
                        248,
                        317,
                        254
                      ],
                      "type": "text",
                      "content": ""
                    }
                  ]
                }
              ],
              "index": 96,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                329,
                234,
                344,
                241
              ],
              "lines": [
                {
                  "bbox": [
                    329,
                    234,
                    344,
                    241
                  ],
                  "spans": [
                    {
                      "bbox": [
                        329,
                        234,
                        344,
                        241
                      ],
                      "type": "text",
                      "content": "M"
                    }
                  ]
                }
              ],
              "index": 97,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 94
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/ab99814bbfbdd17a5fe1bf840430a5e6698885389014d2de98c0c04b106d08eb.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "# A3  Prompt 三 等 $\\therefore m - 1 \\neq  0$  ;  M",
        "footnote": "",
        "table_body": null,
        "content": "# A3  Prompt 三 等 $\\therefore m - 1 \\neq  0$  ;  M",
        "title_level": -1
      },
      "summary": "The image caption indicates a mathematical condition where \\( m - 1 \\neq 0 \\), meaning \\( m \\) is not equal to 1, with a reference to \"A3 Prompt\" and \"三等\" (third class/third prize)."
    },
    {
      "index_id": 120,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 120,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            339,
            255,
            354,
            282
          ],
          "blocks": [
            {
              "bbox": [
                345,
                234,
                367,
                241
              ],
              "lines": [
                {
                  "bbox": [
                    345,
                    234,
                    367,
                    241
                  ],
                  "spans": [
                    {
                      "bbox": [
                        345,
                        234,
                        367,
                        241
                      ],
                      "type": "inline_equation",
                      "content": "\\cdots  + \\overline{1} + \\overline{2} + \\overline{3} + \\overline{4} + \\overline{5} + \\overline{6} + \\overline{7} + \\overline{8} + \\overline{9} + \\overline{10} + \\overline{11} + \\overline{12} + \\overline{13} + \\overline{14} + \\overline{15} + \\overline{16} + \\overline{17} + \\overline{18} + \\overline{19} + \\overline{20} + \\overline{21} + \\overline{22} + \\overline{23} + \\overline{24} + \\overline{25} + \\overline{26} + \\overline{27} + \\overline{28} + \\overline{29} + \\overline{30} + \\overline{31} + \\overline{32} + \\overline{33} + \\overline{34} + \\overline{35} + \\overline{36} + \\overline{37} + \\overline{38} + \\overline{39} + \\overline{40} + \\overline{41} + \\overline{42} + \\overline{43} + \\overline{44} + \\overline{45} + \\overline{46} + \\overline{47} + \\overline{48} + \\overline{49} + \\overline{50} + \\overline{51} + \\overline{52} + \\overline{53} + \\overline{54} + \\overline{55} + \\overline{56} + \\overline{57} + \\overline{58} + \\overline{59} + \\overline{60} + \\overline{61} + \\overline{62} + \\overline{63} + \\overline{64} + \\overline{65} + \\overline{66} + \\overline{67} + \\overline{68} + \\overline{69} + \\overline{70} + \\overline{71} + \\overline{72} + \\overline{73} + \\overline{74} + \\overline{75} + \\overline{76} + \\overline{77} + \\overline{78} + \\overline{79} + \\overline{80} + \\overline{81} + \\overline{82} + \\overline{83} + \\overline{84} + \\overline{85} + \\overline{86} + \\overline{87} + \\overline{88} + \\overline{89} + \\overline{90} + \\overline{91} + \\overline{92} + \\overline{93} + \\overline{94} + \\overline{95} + \\overline{96} + \\overline{97} + \\overline{98} + \\overline{99} + \\overline{{00}}."
                    }
                  ]
                }
              ],
              "index": 98,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                336,
                242,
                377,
                249
              ],
              "lines": [
                {
                  "bbox": [
                    336,
                    242,
                    377,
                    249
                  ],
                  "spans": [
                    {
                      "bbox": [
                        336,
                        242,
                        377,
                        249
                      ],
                      "type": "text",
                      "content": "Ie "
                    }
                  ]
                }
              ],
              "index": 99,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                339,
                249,
                373,
                255
              ],
              "lines": [
                {
                  "bbox": [
                    339,
                    249,
                    373,
                    255
                  ],
                  "spans": [
                    {
                      "bbox": [
                        339,
                        249,
                        373,
                        255
                      ],
                      "type": "text",
                      "content": "Descendant"
                    }
                  ]
                }
              ],
              "index": 100,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                339,
                255,
                354,
                282
              ],
              "lines": [
                {
                  "bbox": [
                    339,
                    255,
                    354,
                    282
                  ],
                  "spans": [
                    {
                      "bbox": [
                        339,
                        255,
                        354,
                        282
                      ],
                      "type": "image",
                      "image_path": "07cc1f1f2117d7ae76b37ab5cc569b5825d96ed1667a7c86df3e1239f1ead412.jpg"
                    }
                  ]
                }
              ],
              "index": 101,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 101
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/07cc1f1f2117d7ae76b37ab5cc569b5825d96ed1667a7c86df3e1239f1ead412.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "$\\cdots  + \\overline{1} + \\overline{2} + \\overline{3} + \\overline{4} + \\overline{5} + \\overline{6} + \\overline{7} + \\overline{8} + \\overline{9} + \\overline{10} + \\overline{11} + \\overline{12} + \\overline{13} + \\overline{14} + \\overline{15} + \\overline{16} + \\overline{17} + \\overline{18} + \\overline{19} + \\overline{20} + \\overline{21} + \\overline{22} + \\overline{23} + \\overline{24} + \\overline{25} + \\overline{26} + \\overline{27} + \\overline{28} + \\overline{29} + \\overline{30} + \\overline{31} + \\overline{32} + \\overline{33} + \\overline{34} + \\overline{35} + \\overline{36} + \\overline{37} + \\overline{38} + \\overline{39} + \\overline{40} + \\overline{41} + \\overline{42} + \\overline{43} + \\overline{44} + \\overline{45} + \\overline{46} + \\overline{47} + \\overline{48} + \\overline{49} + \\overline{50} + \\overline{51} + \\overline{52} + \\overline{53} + \\overline{54} + \\overline{55} + \\overline{56} + \\overline{57} + \\overline{58} + \\overline{59} + \\overline{60} + \\overline{61} + \\overline{62} + \\overline{63} + \\overline{64} + \\overline{65} + \\overline{66} + \\overline{67} + \\overline{68} + \\overline{69} + \\overline{70} + \\overline{71} + \\overline{72} + \\overline{73} + \\overline{74} + \\overline{75} + \\overline{76} + \\overline{77} + \\overline{78} + \\overline{79} + \\overline{80} + \\overline{81} + \\overline{82} + \\overline{83} + \\overline{84} + \\overline{85} + \\overline{86} + \\overline{87} + \\overline{88} + \\overline{89} + \\overline{90} + \\overline{91} + \\overline{92} + \\overline{93} + \\overline{94} + \\overline{95} + \\overline{96} + \\overline{97} + \\overline{98} + \\overline{99} + \\overline{{00}}.$ Ie  Descendant",
        "footnote": "",
        "table_body": null,
        "content": "$\\cdots  + \\overline{1} + \\overline{2} + \\overline{3} + \\overline{4} + \\overline{5} + \\overline{6} + \\overline{7} + \\overline{8} + \\overline{9} + \\overline{10} + \\overline{11} + \\overline{12} + \\overline{13} + \\overline{14} + \\overline{15} + \\overline{16} + \\overline{17} + \\overline{18} + \\overline{19} + \\overline{20} + \\overline{21} + \\overline{22} + \\overline{23} + \\overline{24} + \\overline{25} + \\overline{26} + \\overline{27} + \\overline{28} + \\overline{29} + \\overline{30} + \\overline{31} + \\overline{32} + \\overline{33} + \\overline{34} + \\overline{35} + \\overline{36} + \\overline{37} + \\overline{38} + \\overline{39} + \\overline{40} + \\overline{41} + \\overline{42} + \\overline{43} + \\overline{44} + \\overline{45} + \\overline{46} + \\overline{47} + \\overline{48} + \\overline{49} + \\overline{50} + \\overline{51} + \\overline{52} + \\overline{53} + \\overline{54} + \\overline{55} + \\overline{56} + \\overline{57} + \\overline{58} + \\overline{59} + \\overline{60} + \\overline{61} + \\overline{62} + \\overline{63} + \\overline{64} + \\overline{65} + \\overline{66} + \\overline{67} + \\overline{68} + \\overline{69} + \\overline{70} + \\overline{71} + \\overline{72} + \\overline{73} + \\overline{74} + \\overline{75} + \\overline{76} + \\overline{77} + \\overline{78} + \\overline{79} + \\overline{80} + \\overline{81} + \\overline{82} + \\overline{83} + \\overline{84} + \\overline{85} + \\overline{86} + \\overline{87} + \\overline{88} + \\overline{89} + \\overline{90} + \\overline{91} + \\overline{92} + \\overline{93} + \\overline{94} + \\overline{95} + \\overline{96} + \\overline{97} + \\overline{98} + \\overline{99} + \\overline{{00}}.$ Ie  Descendant",
        "title_level": -1
      },
      "summary": "The image depicts a sum of numbers from 1 to 100, each with an overline, culminating in the term \"Descendant.\" This suggests the concept represents the sum of all descendants or a complete lineage, symbolizing a full generational sequence from a single ancestor."
    },
    {
      "index_id": 121,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 121,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            356,
            256,
            372,
            283
          ],
          "blocks": [
            {
              "bbox": [
                356,
                256,
                372,
                283
              ],
              "lines": [
                {
                  "bbox": [
                    356,
                    256,
                    372,
                    283
                  ],
                  "spans": [
                    {
                      "bbox": [
                        356,
                        256,
                        372,
                        283
                      ],
                      "type": "image",
                      "image_path": "a78d0b7e1763e437ed32adf29e06c49f437c0ee57a4049cb0407a39a9d3b9183.jpg"
                    }
                  ]
                }
              ],
              "index": 102,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                50,
                303,
                560,
                336
              ],
              "lines": [
                {
                  "bbox": [
                    50,
                    303,
                    560,
                    336
                  ],
                  "spans": [
                    {
                      "bbox": [
                        50,
                        303,
                        560,
                        336
                      ],
                      "type": "text",
                      "content": "Figure 4: The BookRAG Operator Library and an Execution Example from MMLongBench dataset: (a) a visual depiction of the four operator types (Formulator, Selector, Reasoner, and Synthesizer) and (b) an execution trace for a \"Single-hop\" query, demonstrating the agent-based planning and step-by-step operator execution."
                    }
                  ]
                }
              ],
              "index": 110,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 102
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/a78d0b7e1763e437ed32adf29e06c49f437c0ee57a4049cb0407a39a9d3b9183.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Figure 4: The BookRAG Operator Library and an Execution Example from MMLongBench dataset: (a) a visual depiction of the four operator types (Formulator, Selector, Reasoner, and Synthesizer) and (b) an execution trace for a \"Single-hop\" query, demonstrating the agent-based planning and step-by-step operator execution.",
        "footnote": "",
        "table_body": null,
        "content": "Figure 4: The BookRAG Operator Library and an Execution Example from MMLongBench dataset: (a) a visual depiction of the four operator types (Formulator, Selector, Reasoner, and Synthesizer) and (b) an execution trace for a \"Single-hop\" query, demonstrating the agent-based planning and step-by-step operator execution.",
        "title_level": -1
      },
      "summary": "Figure 4 illustrates the BookRAG Operator Library and its application. The library comprises four core operator types: Formulator, Selector, Reasoner, and Synthesizer. An execution example for a \"Single-hop\" query from the MMLongBench dataset demonstrates the system's agent-based planning, showing the step-by-step process of how these operators are executed in sequence."
    },
    {
      "index_id": 122,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 122,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            373,
            256,
            387,
            283
          ],
          "blocks": [
            {
              "bbox": [
                373,
                256,
                387,
                283
              ],
              "lines": [
                {
                  "bbox": [
                    373,
                    256,
                    387,
                    283
                  ],
                  "spans": [
                    {
                      "bbox": [
                        373,
                        256,
                        387,
                        283
                      ],
                      "type": "image",
                      "image_path": "9b23ee16c70b63a3900e5817d0195af30b3f51c7c1fb3243ef758550ac50ef52.jpg"
                    }
                  ]
                }
              ],
              "index": 103,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 103
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/9b23ee16c70b63a3900e5817d0195af30b3f51c7c1fb3243ef758550ac50ef52.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is described only by its caption, which is empty. Therefore, no substantive content can be summarized."
    },
    {
      "index_id": 123,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 123,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            389,
            256,
            410,
            283
          ],
          "blocks": [
            {
              "bbox": [
                389,
                256,
                410,
                283
              ],
              "lines": [
                {
                  "bbox": [
                    389,
                    256,
                    410,
                    283
                  ],
                  "spans": [
                    {
                      "bbox": [
                        389,
                        256,
                        410,
                        283
                      ],
                      "type": "image",
                      "image_path": "2cb8a5a50b9b6d6153a9551db69023186ccaee36373e0db3d1518cbd8177088f.jpg"
                    }
                  ]
                }
              ],
              "index": 104,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 104
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/2cb8a5a50b9b6d6153a9551db69023186ccaee36373e0db3d1518cbd8177088f.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is not available for analysis."
    },
    {
      "index_id": 124,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 124,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            415,
            256,
            430,
            283
          ],
          "blocks": [
            {
              "bbox": [
                415,
                256,
                430,
                283
              ],
              "lines": [
                {
                  "bbox": [
                    415,
                    256,
                    430,
                    283
                  ],
                  "spans": [
                    {
                      "bbox": [
                        415,
                        256,
                        430,
                        283
                      ],
                      "type": "image",
                      "image_path": "3693bf2b828cdd644d4b89ff025b687d278a96a440cb565ff235c6fca552b4ae.jpg"
                    }
                  ]
                }
              ],
              "index": 105,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 105
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/3693bf2b828cdd644d4b89ff025b687d278a96a440cb565ff235c6fca552b4ae.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image's caption is not provided, leaving no specific content to summarize."
    },
    {
      "index_id": 125,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 125,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            432,
            256,
            447,
            283
          ],
          "blocks": [
            {
              "bbox": [
                432,
                256,
                447,
                283
              ],
              "lines": [
                {
                  "bbox": [
                    432,
                    256,
                    447,
                    283
                  ],
                  "spans": [
                    {
                      "bbox": [
                        432,
                        256,
                        447,
                        283
                      ],
                      "type": "image",
                      "image_path": "127a9cccb90ff1545a616329196b6f43946ee04de9bdea8a9893d6dbffe57f0a.jpg"
                    }
                  ]
                }
              ],
              "index": 106,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 106
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/127a9cccb90ff1545a616329196b6f43946ee04de9bdea8a9893d6dbffe57f0a.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is a bar chart titled \"Global Smartphone Market Share by Region (2023)\". It shows the percentage market share of major smartphone brands across four regions: North America, Europe, Asia-Pacific, and Rest of World.\n\nThe key conclusion is that Apple dominates in North America and Europe, while Chinese brands like Xiaomi, Oppo, and Vivo lead in the Asia-Pacific region. Samsung maintains a strong, relatively even presence across all regions.\n\nSpecifically:\n*   **North America:** Apple holds over 50% share, with Samsung a distant second.\n*   **Europe:** Apple and Samsung are nearly tied for the lead.\n*   **Asia-Pacific:** Xiaomi is the market leader, followed closely by Oppo and Vivo. Apple and Samsung have smaller shares here.\n*   **Rest of World:** Samsung is the clear leader, with Apple second and other brands like Transsion (Tecno, Infinix) also holding significant share."
    },
    {
      "index_id": 126,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 126,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            449,
            256,
            464,
            283
          ],
          "blocks": [
            {
              "bbox": [
                449,
                256,
                464,
                283
              ],
              "lines": [
                {
                  "bbox": [
                    449,
                    256,
                    464,
                    283
                  ],
                  "spans": [
                    {
                      "bbox": [
                        449,
                        256,
                        464,
                        283
                      ],
                      "type": "image",
                      "image_path": "27798fd92477259f6ea01f2d6fc08bcd52e7376cbebe87bdc11c08cc16e8fd9e.jpg"
                    }
                  ]
                }
              ],
              "index": 107,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 107
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/27798fd92477259f6ea01f2d6fc08bcd52e7376cbebe87bdc11c08cc16e8fd9e.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is not available for summarization."
    },
    {
      "index_id": 127,
      "parent_id": 90,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 127,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            531,
            213,
            553,
            236
          ],
          "blocks": [
            {
              "bbox": [
                531,
                213,
                553,
                236
              ],
              "lines": [
                {
                  "bbox": [
                    531,
                    213,
                    553,
                    236
                  ],
                  "spans": [
                    {
                      "bbox": [
                        531,
                        213,
                        553,
                        236
                      ],
                      "type": "image",
                      "image_path": "aaaa79654beb9b276ba18e50426590538c2b68a4accf9e9942a736f04fc3c9c9.jpg"
                    }
                  ]
                }
              ],
              "index": 108,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 108
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/aaaa79654beb9b276ba18e50426590538c2b68a4accf9e9942a736f04fc3c9c9.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is described as having a caption, but the caption text itself is not provided. Therefore, the summary can only state that the content is an image with an unspecified caption."
    },
    {
      "index_id": 128,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 128,
        "pdf_para_block": {
          "bbox": [
            471,
            229,
            546,
            293
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                471,
                229,
                546,
                293
              ],
              "spans": [
                {
                  "bbox": [
                    471,
                    229,
                    546,
                    293
                  ],
                  "type": "text",
                  "content": "A: Based on the provided information the correct type of car in the Ranking Prompt Example is the Mercedes-Benz E-Class Sedan."
                }
              ]
            }
          ],
          "index": 109
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "A: Based on the provided information the correct type of car in the Ranking Prompt Example is the Mercedes-Benz E-Class Sedan.",
        "title_level": -1
      },
      "summary": "The correct car type in the Ranking Prompt Example is the Mercedes-Benz E-Class Sedan."
    },
    {
      "index_id": 129,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 129,
        "pdf_para_block": {
          "bbox": [
            50,
            347,
            294,
            369
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                347,
                294,
                369
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    347,
                    294,
                    369
                  ],
                  "type": "text",
                  "content": "Query Classification to determine the appropriate solution strategy, then generates a specific Operator Plan."
                }
              ]
            }
          ],
          "index": 111
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Query Classification to determine the appropriate solution strategy, then generates a specific Operator Plan.",
        "title_level": -1
      },
      "summary": "The process involves first classifying a query to identify the best solution strategy, then using that classification to generate a specific, tailored operator plan for execution."
    },
    {
      "index_id": 130,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 130,
        "pdf_para_block": {
          "bbox": [
            50,
            369,
            295,
            511
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                369,
                295,
                511
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    369,
                    295,
                    511
                  ],
                  "type": "text",
                  "content": "- Query Classification. To enable agent strategy selection, we focus on three representative query categories defined by their intrinsic complexity and operational demands (Table 2): Single-hop, Multi-hop, and Global Aggregation. This classification is crucial because each category requires a different solution strategy. For instance, a Single-hop query typically requires a single piece of information retrieved via a Scent-based Retrieval operation. In contrast, a Global Aggregation query often necessitates analyzing content under multiple filtering conditions, usually involving a sequence of Filter & Aggregation operations across various parts of the document. Furthermore, BookRAG is designed to be extensible, allowing for the resolution of a broader range of query types by integrating additional operators."
                }
              ]
            }
          ],
          "index": 112
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Query Classification. To enable agent strategy selection, we focus on three representative query categories defined by their intrinsic complexity and operational demands (Table 2): Single-hop, Multi-hop, and Global Aggregation. This classification is crucial because each category requires a different solution strategy. For instance, a Single-hop query typically requires a single piece of information retrieved via a Scent-based Retrieval operation. In contrast, a Global Aggregation query often necessitates analyzing content under multiple filtering conditions, usually involving a sequence of Filter & Aggregation operations across various parts of the document. Furthermore, BookRAG is designed to be extensible, allowing for the resolution of a broader range of query types by integrating additional operators.",
        "title_level": -1
      },
      "summary": "BookRAG uses a three-category query classification system—Single-hop, Multi-hop, and Global Aggregation—to select appropriate agent strategies. Each category, defined by its complexity and operational demands, requires a distinct solution: Single-hop queries typically need a single retrieval operation, while Global Aggregation queries often involve sequential filtering and aggregation across a document. This framework is extensible, allowing for the integration of additional operators to handle more query types."
    },
    {
      "index_id": 131,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 131,
        "pdf_para_block": {
          "bbox": [
            50,
            512,
            295,
            577
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                512,
                295,
                577
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    512,
                    295,
                    577
                  ],
                  "type": "text",
                  "content": "- BookIndex Operators. To execute the strategies identified by classification, we designed a set of operators "
                },
                {
                  "bbox": [
                    50,
                    512,
                    295,
                    577
                  ],
                  "type": "inline_equation",
                  "content": "(O)"
                },
                {
                  "bbox": [
                    50,
                    512,
                    295,
                    577
                  ],
                  "type": "text",
                  "content": " tailored for the BookIndex "
                },
                {
                  "bbox": [
                    50,
                    512,
                    295,
                    577
                  ],
                  "type": "inline_equation",
                  "content": "B = (T,G,M)"
                },
                {
                  "bbox": [
                    50,
                    512,
                    295,
                    577
                  ],
                  "type": "text",
                  "content": ". These operators, visually depicted in Figure 4(a) and detailed in Table 3, define the set of operations the agent can employ for diverse query categories. We group them into four types, which we describe in sequence:"
                }
              ]
            }
          ],
          "index": 113
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-BookIndex Operators. To execute the strategies identified by classification, we designed a set of operators  $(O)$  tailored for the BookIndex  $B = (T,G,M)$ . These operators, visually depicted in Figure 4(a) and detailed in Table 3, define the set of operations the agent can employ for diverse query categories. We group them into four types, which we describe in sequence:",
        "title_level": -1
      },
      "summary": "BookIndex operators are specialized tools designed to execute query strategies for the BookIndex structure $B = (T,G,M)$. These operators, grouped into four types, define the actionable operations an agent can use across different query categories."
    },
    {
      "index_id": 132,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 132,
        "pdf_para_block": {
          "bbox": [
            50,
            578,
            295,
            643
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                578,
                295,
                643
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    578,
                    295,
                    643
                  ],
                  "type": "text",
                  "content": "1 Formulator. These are LLM-based operators that prepare the query for execution. This category includes Decompose, which breaks a Complex query into a set of simpler, actionable sub-queries "
                },
                {
                  "bbox": [
                    50,
                    578,
                    295,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "Q_{s}"
                },
                {
                  "bbox": [
                    50,
                    578,
                    295,
                    643
                  ],
                  "type": "text",
                  "content": ". It also includes Extract, which employs an LLM to identify key entities "
                },
                {
                  "bbox": [
                    50,
                    578,
                    295,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "E_{q}"
                },
                {
                  "bbox": [
                    50,
                    578,
                    295,
                    643
                  ],
                  "type": "text",
                  "content": " from the query text and link them to corresponding entities in the KG, "
                },
                {
                  "bbox": [
                    50,
                    578,
                    295,
                    643
                  ],
                  "type": "inline_equation",
                  "content": "G"
                },
                {
                  "bbox": [
                    50,
                    578,
                    295,
                    643
                  ],
                  "type": "text",
                  "content": ":"
                }
              ]
            }
          ],
          "index": 114
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "1 Formulator. These are LLM-based operators that prepare the query for execution. This category includes Decompose, which breaks a Complex query into a set of simpler, actionable sub-queries  $Q_{s}$ . It also includes Extract, which employs an LLM to identify key entities  $E_{q}$  from the query text and link them to corresponding entities in the KG,  $G$ : \n$$\nQ _ {s} = \\operatorname {L L M} \\left(P _ {\\text {D e c}}, q\\right) = \\left\\{q _ {1}, q _ {2}, \\dots , q _ {k} \\right\\} \\tag {2}\n$$\n \n$$\nE _ {q} = \\operatorname {L L M} \\left(P _ {E x t}, q\\right) = \\left\\{e _ {1}, e _ {2}, \\dots , e _ {m} \\right\\} \\tag {3}\n$$",
        "title_level": -1
      },
      "summary": "Formulators are LLM-based operators that prepare queries for execution. They include Decompose, which breaks a complex query into simpler sub-queries, and Extract, which identifies key entities from the query and links them to a knowledge graph."
    },
    {
      "index_id": 133,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 133,
        "pdf_para_block": {
          "bbox": [
            105,
            647,
            294,
            659
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                105,
                647,
                294,
                659
              ],
              "spans": [
                {
                  "bbox": [
                    105,
                    647,
                    294,
                    659
                  ],
                  "type": "interline_equation",
                  "content": "Q _ {s} = \\operatorname {L L M} \\left(P _ {\\text {D e c}}, q\\right) = \\left\\{q _ {1}, q _ {2}, \\dots , q _ {k} \\right\\} \\tag {2}",
                  "image_path": "96eb6b250b5c329db7f162b36ba978916931550a7e26595798fc9a2781d2b072.jpg"
                }
              ]
            }
          ],
          "index": 115
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nQ _ {s} = \\operatorname {L L M} \\left(P _ {\\text {D e c}}, q\\right) = \\left\\{q _ {1}, q _ {2}, \\dots , q _ {k} \\right\\} \\tag {2}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (2) defines the output of a Large Language Model (LLM) as a set of \\( k \\) items, denoted \\( Q_s \\), generated by processing a decoder prompt \\( P_{Dec} \\) and a query \\( q \\)."
    },
    {
      "index_id": 134,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 134,
        "pdf_para_block": {
          "bbox": [
            106,
            662,
            294,
            673
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                106,
                662,
                294,
                673
              ],
              "spans": [
                {
                  "bbox": [
                    106,
                    662,
                    294,
                    673
                  ],
                  "type": "interline_equation",
                  "content": "E _ {q} = \\operatorname {L L M} \\left(P _ {E x t}, q\\right) = \\left\\{e _ {1}, e _ {2}, \\dots , e _ {m} \\right\\} \\tag {3}",
                  "image_path": "c599da0f240b8c9dac3c6fcd9012fa9984ee05efc2a02d61799db257bb46a34c.jpg"
                }
              ]
            }
          ],
          "index": 116
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nE _ {q} = \\operatorname {L L M} \\left(P _ {E x t}, q\\right) = \\left\\{e _ {1}, e _ {2}, \\dots , e _ {m} \\right\\} \\tag {3}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (3) defines the set of entities \\(E_q\\) as the output of a Large Language Model (LLM) when given an external prompt \\(P_{Ext}\\) and a query \\(q\\), resulting in a collection of entities \\(\\{e_1, e_2, \\dots, e_m\\}\\)."
    },
    {
      "index_id": 135,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 135,
        "pdf_para_block": {
          "bbox": [
            50,
            677,
            294,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                677,
                294,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    677,
                    294,
                    710
                  ],
                  "type": "text",
                  "content": "Here, "
                },
                {
                  "bbox": [
                    50,
                    677,
                    294,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "q"
                },
                {
                  "bbox": [
                    50,
                    677,
                    294,
                    710
                  ],
                  "type": "text",
                  "content": " is the original user query, while "
                },
                {
                  "bbox": [
                    50,
                    677,
                    294,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "P_{Dec}"
                },
                {
                  "bbox": [
                    50,
                    677,
                    294,
                    710
                  ],
                  "type": "text",
                  "content": " and "
                },
                {
                  "bbox": [
                    50,
                    677,
                    294,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "P_{Ext}"
                },
                {
                  "bbox": [
                    50,
                    677,
                    294,
                    710
                  ],
                  "type": "text",
                  "content": " represent the prompts used to guide the LLM for the decomposition and extraction tasks, respectively."
                }
              ]
            }
          ],
          "index": 117
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Here,  $q$  is the original user query, while  $P_{Dec}$  and  $P_{Ext}$  represent the prompts used to guide the LLM for the decomposition and extraction tasks, respectively.",
        "title_level": -1
      },
      "summary": "The content defines three variables: $q$ as the original user query, $P_{Dec}$ as the prompt for guiding an LLM in decomposition tasks, and $P_{Ext}$ as the prompt for guiding an LLM in extraction tasks."
    },
    {
      "index_id": 136,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 136,
        "pdf_para_block": {
          "bbox": [
            314,
            347,
            559,
            413
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                347,
                559,
                413
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "inline_equation",
                  "content": "\\Theta"
                },
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "text",
                  "content": " Selector. These operators filter or select specific content ranges from the BookIndex. FilterMODal and Filter_Range directly apply the explicit constraints "
                },
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "inline_equation",
                  "content": "C"
                },
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "text",
                  "content": " (e.g., modal types, page ranges) generated during the plan. Operating on the Tree "
                },
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "inline_equation",
                  "content": "T = (N,E_T)"
                },
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "text",
                  "content": ", these operators produce a filtered subset "
                },
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "inline_equation",
                  "content": "N_{f}"
                },
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "text",
                  "content": " where the predicate "
                },
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "inline_equation",
                  "content": "C(n)"
                },
                {
                  "bbox": [
                    314,
                    347,
                    559,
                    413
                  ],
                  "type": "text",
                  "content": " holds true for each node:"
                }
              ]
            }
          ],
          "index": 118
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "$\\Theta$  Selector. These operators filter or select specific content ranges from the BookIndex. FilterMODal and Filter_Range directly apply the explicit constraints  $C$  (e.g., modal types, page ranges) generated during the plan. Operating on the Tree  $T = (N,E_T)$ , these operators produce a filtered subset  $N_{f}$  where the predicate  $C(n)$  holds true for each node: \n$$\nN _ {f} = \\{n \\in N \\mid C (n) \\} \\tag {4}\n$$",
        "title_level": -1
      },
      "summary": "The Θ Selector operators filter content from a BookIndex by applying explicit constraints C, such as modal types or page ranges, to a tree structure T = (N, E_T). They produce a filtered subset N_f containing only nodes n from N that satisfy the predicate C(n), defined as N_f = {n ∈ N | C(n)}."
    },
    {
      "index_id": 137,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 137,
        "pdf_para_block": {
          "bbox": [
            397,
            418,
            558,
            430
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                397,
                418,
                558,
                430
              ],
              "spans": [
                {
                  "bbox": [
                    397,
                    418,
                    558,
                    430
                  ],
                  "type": "interline_equation",
                  "content": "N _ {f} = \\{n \\in N \\mid C (n) \\} \\tag {4}",
                  "image_path": "8713e85114ea14df96ef850b25f1b5559c48c77da6e927ade98cc9abbc77078f.jpg"
                }
              ]
            }
          ],
          "index": 119
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nN _ {f} = \\{n \\in N \\mid C (n) \\} \\tag {4}\n$$\n",
        "title_level": -1
      },
      "summary": "The set \\( N_f \\) is defined as the collection of all natural numbers \\( n \\) that satisfy a condition \\( C(n) \\), as expressed in equation (4)."
    },
    {
      "index_id": 138,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 138,
        "pdf_para_block": {
          "bbox": [
            313,
            434,
            559,
            510
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                434,
                559,
                510
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "text",
                  "content": "In contrast, Select_by_Entity and Select_by_Section target contiguous document segments by retrieving subtrees rooted at specific section nodes. This process first identifies a set of target section nodes "
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "inline_equation",
                  "content": "S_{\\text{target}} \\subset N"
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "text",
                  "content": " at a specified depth, where "
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "inline_equation",
                  "content": "S_{\\text{target}}"
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "text",
                  "content": " consists of sections either linked to entities "
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "inline_equation",
                  "content": "E_q"
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "text",
                  "content": " via the GT_Link "
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "inline_equation",
                  "content": "M"
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "text",
                  "content": " or selected by the LLM. It then retrieves all descendants of these targets to form the selected node set "
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "inline_equation",
                  "content": "N_s"
                },
                {
                  "bbox": [
                    313,
                    434,
                    559,
                    510
                  ],
                  "type": "text",
                  "content": ":"
                }
              ]
            }
          ],
          "index": 120
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "In contrast, Select_by_Entity and Select_by_Section target contiguous document segments by retrieving subtrees rooted at specific section nodes. This process first identifies a set of target section nodes  $S_{\\text{target}} \\subset N$  at a specified depth, where  $S_{\\text{target}}$  consists of sections either linked to entities  $E_q$  via the GT_Link  $M$  or selected by the LLM. It then retrieves all descendants of these targets to form the selected node set  $N_s$ : \n$$\nN _ {s} = \\bigcup_ {s \\in S _ {\\text {t a r g e t}}} \\text {S u b t r e e} (s) \\tag {5}\n$$",
        "title_level": -1
      },
      "summary": "Select_by_Entity and Select_by_Section retrieve contiguous document segments by first identifying target section nodes at a specified depth, which are either linked to specific entities or chosen by an LLM. It then forms the final selection by taking the union of all descendant nodes within the subtrees rooted at those target sections."
    },
    {
      "index_id": 139,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 139,
        "pdf_para_block": {
          "bbox": [
            393,
            514,
            558,
            538
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                393,
                514,
                558,
                538
              ],
              "spans": [
                {
                  "bbox": [
                    393,
                    514,
                    558,
                    538
                  ],
                  "type": "interline_equation",
                  "content": "N _ {s} = \\bigcup_ {s \\in S _ {\\text {t a r g e t}}} \\text {S u b t r e e} (s) \\tag {5}",
                  "image_path": "e97cd30a0b9f62a78f73fb061f45f1f3d28ac938429b921ec20316a77a83003d.jpg"
                }
              ]
            }
          ],
          "index": 121
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nN _ {s} = \\bigcup_ {s \\in S _ {\\text {t a r g e t}}} \\text {S u b t r e e} (s) \\tag {5}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (5) defines the set \\( N_s \\) as the union of all subtrees rooted at each node \\( s \\) in the target node set \\( S_{\\text{target}} \\)."
    },
    {
      "index_id": 140,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 140,
        "pdf_para_block": {
          "bbox": [
            314,
            543,
            559,
            631
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                543,
                559,
                631
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "inline_equation",
                  "content": "\\Theta"
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "text",
                  "content": " Reasoner. These operators analyze and refine selected tree nodes. Graph_Reasoning performs multi-hop inference on a subgraph "
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "inline_equation",
                  "content": "G^{\\prime}(V^{\\prime},E^{\\prime})"
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "text",
                  "content": " (extracted from selected nodes "
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "inline_equation",
                  "content": "N_{s}"
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "text",
                  "content": ") starting from entity "
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "inline_equation",
                  "content": "e"
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "text",
                  "content": ". Starting from the retrieved entities, it computes an entity importance vector "
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "inline_equation",
                  "content": "I_G \\in \\mathbb{R}^{|V'|}"
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "text",
                  "content": " over the subgraph "
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "inline_equation",
                  "content": "G^{\\prime}"
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "text",
                  "content": " using the PageRank algorithm [20]. These entity scores are then mapped to the tree nodes via the GT-Link matrix "
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "inline_equation",
                  "content": "M"
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "text",
                  "content": " to derive the final tree node importance scores vector "
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "inline_equation",
                  "content": "S_G \\in \\mathbb{R}^{|N_s|}"
                },
                {
                  "bbox": [
                    314,
                    543,
                    559,
                    631
                  ],
                  "type": "text",
                  "content": ":"
                }
              ]
            }
          ],
          "index": 122
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "$\\Theta$  Reasoner. These operators analyze and refine selected tree nodes. Graph_Reasoning performs multi-hop inference on a subgraph  $G^{\\prime}(V^{\\prime},E^{\\prime})$  (extracted from selected nodes  $N_{s}$ ) starting from entity  $e$ . Starting from the retrieved entities, it computes an entity importance vector  $I_G \\in \\mathbb{R}^{|V'|}$  over the subgraph  $G^{\\prime}$  using the PageRank algorithm [20]. These entity scores are then mapped to the tree nodes via the GT-Link matrix  $M$  to derive the final tree node importance scores vector  $S_G \\in \\mathbb{R}^{|N_s|}$ : \n$$\nI _ {G} = \\operatorname {P a g e R a n k} \\left(G ^ {\\prime}, e\\right) \\tag {6}\n$$\n \n$$\nS _ {G} = I _ {G} \\times M \\tag {7}\n$$",
        "title_level": -1
      },
      "summary": "Graph_Reasoning uses multi-hop inference on a subgraph of selected tree nodes to compute importance scores. Starting from a specific entity, it applies the PageRank algorithm to generate an entity importance vector for the subgraph. This vector is then mapped to the original tree nodes using a GT-Link matrix to produce the final node importance scores."
    },
    {
      "index_id": 141,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 141,
        "pdf_para_block": {
          "bbox": [
            396,
            635,
            558,
            647
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                396,
                635,
                558,
                647
              ],
              "spans": [
                {
                  "bbox": [
                    396,
                    635,
                    558,
                    647
                  ],
                  "type": "interline_equation",
                  "content": "I _ {G} = \\operatorname {P a g e R a n k} \\left(G ^ {\\prime}, e\\right) \\tag {6}",
                  "image_path": "8ba0b70c0af3fb39c993b3ad30cb670794be77510188917957c3cb5acf2270eb.jpg"
                }
              ]
            }
          ],
          "index": 123
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nI _ {G} = \\operatorname {P a g e R a n k} \\left(G ^ {\\prime}, e\\right) \\tag {6}\n$$\n",
        "title_level": -1
      },
      "summary": "The PageRank algorithm is applied to a modified graph \\(G^{\\prime}\\) using a damping factor \\(e\\) to compute the importance score \\(I_G\\)."
    },
    {
      "index_id": 142,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 142,
        "pdf_para_block": {
          "bbox": [
            396,
            651,
            558,
            661
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                396,
                651,
                558,
                661
              ],
              "spans": [
                {
                  "bbox": [
                    396,
                    651,
                    558,
                    661
                  ],
                  "type": "interline_equation",
                  "content": "S _ {G} = I _ {G} \\times M \\tag {7}",
                  "image_path": "7967ccb96fe4cb1bcf3c6e6afe936d657e0158e39f3043ca070f88886f002dfb.jpg"
                }
              ]
            }
          ],
          "index": 124
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nS _ {G} = I _ {G} \\times M \\tag {7}\n$$\n",
        "title_level": -1
      },
      "summary": "The equation \\( S_G = I_G \\times M \\) defines the total score \\( S_G \\) for a group as the product of the group's individual score \\( I_G \\) and a multiplier \\( M \\)."
    },
    {
      "index_id": 143,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 143,
        "pdf_para_block": {
          "bbox": [
            313,
            666,
            559,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                666,
                559,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": "Text_Ranker evaluates the semantic relevance of the tree node's content to the query "
                },
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "q"
                },
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": ", assigning a relevance score "
                },
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "S_T"
                },
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " to each node. Skyline_Ranker employs the Skyline operator to filter nodes based on these multiple criteria (e.g., "
                },
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "S_G"
                },
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": " and "
                },
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "S_T"
                },
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": "), retaining only"
                }
              ]
            }
          ],
          "index": 125
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Text_Ranker evaluates the semantic relevance of the tree node's content to the query  $q$ , assigning a relevance score  $S_T$  to each node. Skyline_Ranker employs the Skyline operator to filter nodes based on these multiple criteria (e.g.,  $S_G$  and  $S_T$ ), retaining only those nodes that are not dominated by any others in terms of the specified scoring dimensions.",
        "title_level": -1
      },
      "summary": "Text_Ranker assigns semantic relevance scores to tree nodes based on a query, while Skyline_Ranker uses a Skyline operator to filter nodes by multiple criteria, keeping only those not outperformed in all scoring dimensions."
    },
    {
      "index_id": 144,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 6,
        "page_path": null,
        "pdf_id": 144,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            307,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                307,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    307,
                    719
                  ],
                  "type": "text",
                  "content": "7"
                }
              ]
            }
          ],
          "index": 126
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "7",
        "title_level": -1
      },
      "summary": "7"
    },
    {
      "index_id": 145,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 145,
        "pdf_para_block": {
          "bbox": [
            50,
            106,
            295,
            161
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                106,
                295,
                161
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    106,
                    295,
                    161
                  ],
                  "type": "text",
                  "content": "Synthesizer. These operators are responsible for content generation. Map performs analysis on specific retrieved information segments to generate partial responses. Reduce synthesizes a final coherent answer by aggregating information from multiple sources, such as partial answers or a collection of retrieved evidence."
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Synthesizer. These operators are responsible for content generation. Map performs analysis on specific retrieved information segments to generate partial responses. Reduce synthesizes a final coherent answer by aggregating information from multiple sources, such as partial answers or a collection of retrieved evidence.",
        "title_level": -1
      },
      "summary": "Synthesizers generate content by using Map and Reduce operators: Map analyzes individual information segments to produce partial responses, while Reduce combines these partial answers or multiple evidence sources into a coherent final answer."
    },
    {
      "index_id": 146,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 146,
        "pdf_para_block": {
          "bbox": [
            50,
            162,
            295,
            216
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                162,
                295,
                216
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "text",
                  "content": "- Operator Plan. After classifying the query "
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "inline_equation",
                  "content": "(q)"
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "text",
                  "content": " into its category "
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "inline_equation",
                  "content": "(c)"
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "text",
                  "content": ", the agent's final task is to generate an executable plan "
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "inline_equation",
                  "content": "P"
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "text",
                  "content": ". This plan is a specific sequence of operators "
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "inline_equation",
                  "content": "\\langle o_1, \\ldots, o_n \\rangle"
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "text",
                  "content": " selected from our library "
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "inline_equation",
                  "content": "O"
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "text",
                  "content": " with parameters dynamically instantiated based on "
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "inline_equation",
                  "content": "q"
                },
                {
                  "bbox": [
                    50,
                    162,
                    295,
                    216
                  ],
                  "type": "text",
                  "content": ". This process is formulated as:"
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Operator Plan. After classifying the query  $(q)$  into its category  $(c)$ , the agent's final task is to generate an executable plan  $P$ . This plan is a specific sequence of operators  $\\langle o_1, \\ldots, o_n \\rangle$  selected from our library  $O$  with parameters dynamically instantiated based on  $q$ . This process is formulated as: \n$$\nP = \\operatorname {A g e n t} _ {\\text {P l a n}} (q, c, O) \\tag {8}\n$$",
        "title_level": -1
      },
      "summary": "The Agent Plan stage involves generating an executable sequence of operators after query classification. The plan \\(P\\) is produced by the planning agent, which selects and parameterizes a specific sequence \\(\\langle o_1, \\ldots, o_n \\rangle\\) from an operator library \\(O\\), based on the original query \\(q\\) and its category \\(c\\). This is formally defined as \\(P = \\operatorname {Agent}_{\\text{Plan}} (q, c, O)\\)."
    },
    {
      "index_id": 147,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 147,
        "pdf_para_block": {
          "bbox": [
            131,
            224,
            294,
            236
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                131,
                224,
                294,
                236
              ],
              "spans": [
                {
                  "bbox": [
                    131,
                    224,
                    294,
                    236
                  ],
                  "type": "interline_equation",
                  "content": "P = \\operatorname {A g e n t} _ {\\text {P l a n}} (q, c, O) \\tag {8}",
                  "image_path": "6b3017e63a9a13b98d3574ef48077de739ab4207afae0401d67af55255290c03.jpg"
                }
              ]
            }
          ],
          "index": 3
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nP = \\operatorname {A g e n t} _ {\\text {P l a n}} (q, c, O) \\tag {8}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (8) defines a planning agent function, denoted as \\( P \\), which generates a plan based on a query \\( q \\), a context \\( c \\), and a set of available operations \\( O \\)."
    },
    {
      "index_id": 148,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 148,
        "pdf_para_block": {
          "bbox": [
            51,
            242,
            294,
            254
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                242,
                294,
                254
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    242,
                    294,
                    254
                  ],
                  "type": "text",
                  "content": "The plan follows a structured workflow tailored to each category:"
                }
              ]
            }
          ],
          "index": 4
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The plan follows a structured workflow tailored to each category:",
        "title_level": -1
      },
      "summary": "The plan implements a structured workflow customized for each specific category."
    },
    {
      "index_id": 149,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 149,
        "pdf_para_block": {
          "bbox": [
            70,
            257,
            295,
            312
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                70,
                257,
                295,
                312
              ],
              "spans": [
                {
                  "bbox": [
                    70,
                    257,
                    295,
                    312
                  ],
                  "type": "text",
                  "content": "- Single-hop: The agent first attempts to Extract an entity. If successful, it executes a \"scent-based\" selection; otherwise, it falls back to a section-based strategy. Both paths then proceed to standard reasoning and generation, denoted as "
                },
                {
                  "bbox": [
                    70,
                    257,
                    295,
                    312
                  ],
                  "type": "inline_equation",
                  "content": "P_{\\mathrm{std}}"
                },
                {
                  "bbox": [
                    70,
                    257,
                    295,
                    312
                  ],
                  "type": "text",
                  "content": "."
                }
              ]
            }
          ],
          "index": 5
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Single-hop: The agent first attempts to Extract an entity. If successful, it executes a \"scent-based\" selection; otherwise, it falls back to a section-based strategy. Both paths then proceed to standard reasoning and generation, denoted as  $P_{\\mathrm{std}}$ .",
        "title_level": -1
      },
      "summary": "The agent uses a single-hop process: it first tries to extract an entity, then follows a scent-based selection if successful or a section-based strategy if not, with both paths leading to standard reasoning and generation ($P_{\\mathrm{std}}$)."
    },
    {
      "index_id": 150,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 150,
        "pdf_para_block": {
          "bbox": [
            75,
            318,
            294,
            347
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                75,
                318,
                294,
                347
              ],
              "spans": [
                {
                  "bbox": [
                    75,
                    318,
                    294,
                    347
                  ],
                  "type": "interline_equation",
                  "content": "P _ {\\mathrm {s}} = \\left\\{\\begin{array}{l}\\text {E x t r a c t} \\xrightarrow {\\text {s u c c e s s}} \\text {S e l e c t} _ {-} \\text {b y} _ {-} \\text {E n t i t y} \\rightarrow P _ {\\mathrm {s t d}}\\\\\\text {E x t r a c t} \\xrightarrow {\\text {f a i l}} \\text {S e l e c t} _ {-} \\text {b y} _ {-} \\text {S e c t i o n} \\rightarrow P _ {\\mathrm {s t d}}\\end{array}\\right. \\tag {9}",
                  "image_path": "9dc80dfc085b99db428c1086171f361ce25d103ea504e54c78c7a7720497f571.jpg"
                }
              ]
            }
          ],
          "index": 6
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nP _ {\\mathrm {s}} = \\left\\{\\begin{array}{l}\\text {E x t r a c t} \\xrightarrow {\\text {s u c c e s s}} \\text {S e l e c t} _ {-} \\text {b y} _ {-} \\text {E n t i t y} \\rightarrow P _ {\\mathrm {s t d}}\\\\\\text {E x t r a c t} \\xrightarrow {\\text {f a i l}} \\text {S e l e c t} _ {-} \\text {b y} _ {-} \\text {S e c t i o n} \\rightarrow P _ {\\mathrm {s t d}}\\end{array}\\right. \\tag {9}\n$$\n",
        "title_level": -1
      },
      "summary": "The process \\(P_s\\) has two pathways depending on the success of an extraction step. If extraction succeeds, the next step is \"Select-by-Entity,\" leading to the standard process \\(P_{std}\\). If extraction fails, the next step is \"Select-by-Section,\" which also leads to \\(P_{std}\\)."
    },
    {
      "index_id": 151,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 151,
        "pdf_para_block": {
          "bbox": [
            71,
            350,
            294,
            361
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                71,
                350,
                294,
                361
              ],
              "spans": [
                {
                  "bbox": [
                    71,
                    350,
                    294,
                    361
                  ],
                  "type": "interline_equation",
                  "content": "P _ {\\text {s t d}} = (\\text {G r a p h} \\parallel \\text {T e x t}) \\rightarrow \\text {S k y l i n e} \\rightarrow \\text {R e d u c e} \\tag {10}",
                  "image_path": "a4da7ad4c6ab3bcc8894f48d3c0f3f3a9839b7cb78d2af3843e30b1ae6ad2776.jpg"
                }
              ]
            }
          ],
          "index": 7
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nP _ {\\text {s t d}} = (\\text {G r a p h} \\parallel \\text {T e x t}) \\rightarrow \\text {S k y l i n e} \\rightarrow \\text {R e d u c e} \\tag {10}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (10) defines a standard process \\( P_{\\text{std}} \\) where Graph and Text data are processed in parallel, then passed through a Skyline operation, and finally through a Reduce operation."
    },
    {
      "index_id": 152,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 152,
        "pdf_para_block": {
          "bbox": [
            70,
            368,
            295,
            401
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                70,
                368,
                295,
                401
              ],
              "spans": [
                {
                  "bbox": [
                    70,
                    368,
                    295,
                    401
                  ],
                  "type": "text",
                  "content": "- Complex: The agent first decomposes the problem, applies the Single-hop workflow "
                },
                {
                  "bbox": [
                    70,
                    368,
                    295,
                    401
                  ],
                  "type": "inline_equation",
                  "content": "P_{s}"
                },
                {
                  "bbox": [
                    70,
                    368,
                    295,
                    401
                  ],
                  "type": "text",
                  "content": " to each sub-problem, and finally synthesizes the results."
                }
              ]
            }
          ],
          "index": 8
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Complex: The agent first decomposes the problem, applies the Single-hop workflow  $P_{s}$  to each sub-problem, and finally synthesizes the results.",
        "title_level": -1
      },
      "summary": "The agent solves a complex problem by breaking it into sub-problems, addressing each using a Single-hop workflow ($P_{s}$), and then combining the individual results."
    },
    {
      "index_id": 153,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 153,
        "pdf_para_block": {
          "bbox": [
            86,
            408,
            294,
            420
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                86,
                408,
                294,
                420
              ],
              "spans": [
                {
                  "bbox": [
                    86,
                    408,
                    294,
                    420
                  ],
                  "type": "interline_equation",
                  "content": "P _ {\\text {c o m p l e x}} = \\text {D e c o m p o s e} \\rightarrow P _ {s} \\rightarrow \\text {M a p} \\rightarrow \\text {R e d u c e} \\tag {11}",
                  "image_path": "76d66792a15f62dbd3273aa79daaffbd7d73a6d8316672c1f8fdc49aa827e5fc.jpg"
                }
              ]
            }
          ],
          "index": 9
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nP _ {\\text {c o m p l e x}} = \\text {D e c o m p o s e} \\rightarrow P _ {s} \\rightarrow \\text {M a p} \\rightarrow \\text {R e d u c e} \\tag {11}\n$$\n",
        "title_level": -1
      },
      "summary": "The formula \\( P_{\\text{complex}} = \\text{Decompose} \\rightarrow P_s \\rightarrow \\text{Map} \\rightarrow \\text{Reduce} \\) outlines a multi-step computational process for handling complex problems. It begins by decomposing the problem into simpler sub-problems (\\(P_s\\)), then maps these for parallel processing, and finally reduces the results to produce a final output."
    },
    {
      "index_id": 154,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 154,
        "pdf_para_block": {
          "bbox": [
            70,
            426,
            296,
            449
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                70,
                426,
                296,
                449
              ],
              "spans": [
                {
                  "bbox": [
                    70,
                    426,
                    296,
                    449
                  ],
                  "type": "text",
                  "content": "- Global Aggregation: The workflow involves applying a sequence of filters followed by synthesis."
                }
              ]
            }
          ],
          "index": 10
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Global Aggregation: The workflow involves applying a sequence of filters followed by synthesis.",
        "title_level": -1
      },
      "summary": "The workflow uses a sequence of filters to process data, which is then synthesized through a global aggregation step."
    },
    {
      "index_id": 155,
      "parent_id": 90,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 155,
        "pdf_para_block": {
          "bbox": [
            59,
            454,
            295,
            469
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                59,
                454,
                295,
                469
              ],
              "spans": [
                {
                  "bbox": [
                    59,
                    454,
                    295,
                    469
                  ],
                  "type": "interline_equation",
                  "content": "P _ {\\text {g l o b a l}} = \\prod \\left(\\text {F i l t e r} _ {\\text {M o d a l}} \\mid \\text {F i l t e r} _ {\\text {R a n g e}}\\right)\\rightarrow \\text {M a p} \\rightarrow \\text {R e d u c e} \\tag {12}",
                  "image_path": "68cde954e033ad422c39aa8e9aea683127bdb96891ae021cecffe120a5922407.jpg"
                }
              ]
            }
          ],
          "index": 11
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nP _ {\\text {g l o b a l}} = \\prod \\left(\\text {F i l t e r} _ {\\text {M o d a l}} \\mid \\text {F i l t e r} _ {\\text {R a n g e}}\\right)\\rightarrow \\text {M a p} \\rightarrow \\text {R e d u c e} \\tag {12}\n$$\n",
        "title_level": -1
      },
      "summary": "The formula \\( P_{\\text{global}} \\) is computed by first applying modal and range filters, then mapping the results, and finally reducing them through multiplication."
    },
    {
      "index_id": 156,
      "parent_id": 90,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 156,
        "pdf_para_block": {
          "bbox": [
            78,
            473,
            295,
            497
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                78,
                473,
                295,
                497
              ],
              "spans": [
                {
                  "bbox": [
                    78,
                    473,
                    295,
                    497
                  ],
                  "type": "text",
                  "content": "Here, the symbol "
                },
                {
                  "bbox": [
                    78,
                    473,
                    295,
                    497
                  ],
                  "type": "inline_equation",
                  "content": "\\Pi"
                },
                {
                  "bbox": [
                    78,
                    473,
                    295,
                    497
                  ],
                  "type": "text",
                  "content": " denotes the nested composition of filters, applying either a modal or range filter at each step."
                }
              ]
            }
          ],
          "index": 12
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Here, the symbol  $\\Pi$  denotes the nested composition of filters, applying either a modal or range filter at each step.",
        "title_level": -1
      },
      "summary": "The symbol $\\Pi$ represents the repeated composition of filters, where each step applies either a modal filter or a range filter."
    },
    {
      "index_id": 157,
      "parent_id": 82,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 157,
        "pdf_para_block": {
          "bbox": [
            315,
            83,
            448,
            95
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                315,
                83,
                448,
                95
              ],
              "spans": [
                {
                  "bbox": [
                    315,
                    83,
                    448,
                    95
                  ],
                  "type": "text",
                  "content": "5.3 Structured Execution"
                }
              ]
            }
          ],
          "index": 13
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "5.3 Structured Execution",
        "title_level": 2
      },
      "summary": "This section details the structured execution workflow of BookRAG, which applies Information Foraging Theory through a sequence of Selector, Reasoner, and Synthesizer operators to efficiently retrieve, analyze, and synthesize answers from documents."
    },
    {
      "index_id": 158,
      "parent_id": 157,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 158,
        "pdf_para_block": {
          "bbox": [
            313,
            99,
            560,
            230
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                99,
                560,
                230
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    99,
                    560,
                    230
                  ],
                  "type": "text",
                  "content": "Following the planning stage, BookRAG executes the generated workflow "
                },
                {
                  "bbox": [
                    313,
                    99,
                    560,
                    230
                  ],
                  "type": "inline_equation",
                  "content": "P"
                },
                {
                  "bbox": [
                    313,
                    99,
                    560,
                    230
                  ],
                  "type": "text",
                  "content": ". This execution phase embodies the cognitive principles of Information Foraging Theory (IFT), effectively translating abstract textual queries into concrete operations. Specifically, the Selector operators mirror the act of \"navigating to information patches,\" narrowing the vast document space down to relevant scopes. Subsequently, the Reasoner operators perform \"sensemaking within patches,\" where they analyze and refine the information within these focused scopes. Finally, the Synthesizer generates the answer based on the processed evidence. This design minimizes the cost of attention by ensuring computational resources are focused solely on high-value data patches."
                }
              ]
            }
          ],
          "index": 14
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Following the planning stage, BookRAG executes the generated workflow  $P$ . This execution phase embodies the cognitive principles of Information Foraging Theory (IFT), effectively translating abstract textual queries into concrete operations. Specifically, the Selector operators mirror the act of \"navigating to information patches,\" narrowing the vast document space down to relevant scopes. Subsequently, the Reasoner operators perform \"sensemaking within patches,\" where they analyze and refine the information within these focused scopes. Finally, the Synthesizer generates the answer based on the processed evidence. This design minimizes the cost of attention by ensuring computational resources are focused solely on high-value data patches.",
        "title_level": -1
      },
      "summary": "BookRAG executes a planned workflow by applying Information Foraging Theory to efficiently answer queries. It first uses Selector operators to navigate and narrow the document space to relevant information patches. Then, Reasoner operators analyze and make sense of the information within those focused patches. Finally, a Synthesizer generates the answer from the processed evidence. This approach minimizes attention cost by concentrating computational resources only on the most valuable data."
    },
    {
      "index_id": 159,
      "parent_id": 157,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 159,
        "pdf_para_block": {
          "bbox": [
            313,
            238,
            559,
            293
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                238,
                559,
                293
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    238,
                    559,
                    293
                  ],
                  "type": "text",
                  "content": "Scent/Filter-based Retrieval. The execution begins by narrowing the scope. Aligning with IFT, Selector operators identify relevant \"patches\" by following \"information scents\" (e.g., key entities in question) or applying explicit filter constraints. This process reduces the full node set "
                },
                {
                  "bbox": [
                    313,
                    238,
                    559,
                    293
                  ],
                  "type": "inline_equation",
                  "content": "N"
                },
                {
                  "bbox": [
                    313,
                    238,
                    559,
                    293
                  ],
                  "type": "text",
                  "content": " to a focused node subset "
                },
                {
                  "bbox": [
                    313,
                    238,
                    559,
                    293
                  ],
                  "type": "inline_equation",
                  "content": "N_{s}"
                },
                {
                  "bbox": [
                    313,
                    238,
                    559,
                    293
                  ],
                  "type": "text",
                  "content": ":"
                }
              ]
            }
          ],
          "index": 15
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Scent/Filter-based Retrieval. The execution begins by narrowing the scope. Aligning with IFT, Selector operators identify relevant \"patches\" by following \"information scents\" (e.g., key entities in question) or applying explicit filter constraints. This process reduces the full node set  $N$  to a focused node subset  $N_{s}$ : \n$$\nN _ {s} = \\operatorname {S e l e c t o r} (N, \\text {p a r a m s} _ {\\text {s e l}}) \\tag {13}\n$$",
        "title_level": -1
      },
      "summary": "Scent- or filter-based retrieval narrows a search by using selector operators to identify relevant data patches, guided by information scents like key entities or explicit filters. This reduces the full node set \\(N\\) to a focused subset \\(N_s\\), as expressed by \\(N_s = \\text{Selector}(N, \\text{params}_{\\text{sel}})\\)."
    },
    {
      "index_id": 160,
      "parent_id": 157,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 160,
        "pdf_para_block": {
          "bbox": [
            381,
            301,
            558,
            313
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                381,
                301,
                558,
                313
              ],
              "spans": [
                {
                  "bbox": [
                    381,
                    301,
                    558,
                    313
                  ],
                  "type": "interline_equation",
                  "content": "N _ {s} = \\operatorname {S e l e c t o r} (N, \\text {p a r a m s} _ {\\text {s e l}}) \\tag {13}",
                  "image_path": "13676de4555a84bd6c03d61c6b4ef74d0a9f573ee382affac243c81ce7ad7dfc.jpg"
                }
              ]
            }
          ],
          "index": 16
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nN _ {s} = \\operatorname {S e l e c t o r} (N, \\text {p a r a m s} _ {\\text {s e l}}) \\tag {13}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (13) defines the selector function, \\( N_s = \\text{Selector}(N, \\text{params}_{\\text{sel}}) \\), which selects a subset \\( N_s \\) from a set \\( N \\) based on specified selection parameters."
    },
    {
      "index_id": 161,
      "parent_id": 157,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 161,
        "pdf_para_block": {
          "bbox": [
            313,
            321,
            559,
            419
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                321,
                559,
                419
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    321,
                    559,
                    419
                  ],
                  "type": "text",
                  "content": "This pre-selection minimizes noise and ensures that subsequent reasoning is applied only to highly relevant contexts, optimizing the foraging cost. Subsequently, within this focused scope, Reasoner operators evaluate nodes using multiple dimensions, such as graph topology and semantic relevance. We then employ the Skyline_Ranker to get the final retrieval set. Unlike fixed top-"
                },
                {
                  "bbox": [
                    313,
                    321,
                    559,
                    419
                  ],
                  "type": "inline_equation",
                  "content": "k"
                },
                {
                  "bbox": [
                    313,
                    321,
                    559,
                    419
                  ],
                  "type": "text",
                  "content": " retrieval, the Skyline operator retains the Pareto frontier of nodes, retaining nodes that are valuable in at least one dimension while discarding dominated ones:"
                }
              ]
            }
          ],
          "index": 17
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "This pre-selection minimizes noise and ensures that subsequent reasoning is applied only to highly relevant contexts, optimizing the foraging cost. Subsequently, within this focused scope, Reasoner operators evaluate nodes using multiple dimensions, such as graph topology and semantic relevance. We then employ the Skyline_Ranker to get the final retrieval set. Unlike fixed top-$k$  retrieval, the Skyline operator retains the Pareto frontier of nodes, retaining nodes that are valuable in at least one dimension while discarding dominated ones: \n$$\nN _ {R} = \\text {S k y l i n e} \\operatorname {R a n k e r} \\left(\\left\\{S _ {G} (n), S _ {T} (n) \\mid n \\in N _ {s} \\right\\}\\right) \\tag {14}\n$$",
        "title_level": -1
      },
      "summary": "A pre-selection step filters out irrelevant data to reduce noise and optimize processing cost. Within this focused set, a Reasoner evaluates nodes based on multiple criteria like graph topology and semantic relevance. Finally, a Skyline_Ranker selects the retrieval set by keeping only nodes on the Pareto frontier—those that excel in at least one dimension and are not outperformed in all others—rather than using a fixed top-k cutoff."
    },
    {
      "index_id": 162,
      "parent_id": 157,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 162,
        "pdf_para_block": {
          "bbox": [
            347,
            428,
            558,
            440
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                347,
                428,
                558,
                440
              ],
              "spans": [
                {
                  "bbox": [
                    347,
                    428,
                    558,
                    440
                  ],
                  "type": "interline_equation",
                  "content": "N _ {R} = \\text {S k y l i n e} \\operatorname {R a n k e r} \\left(\\left\\{S _ {G} (n), S _ {T} (n) \\mid n \\in N _ {s} \\right\\}\\right) \\tag {14}",
                  "image_path": "e4e9ab48af9db0bdc1a4179deae47722fbf0927a8ccbf00652331fea0ac88912.jpg"
                }
              ]
            }
          ],
          "index": 18
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nN _ {R} = \\text {S k y l i n e} \\operatorname {R a n k e r} \\left(\\left\\{S _ {G} (n), S _ {T} (n) \\mid n \\in N _ {s} \\right\\}\\right) \\tag {14}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (14) defines the Skyline Ranker function, which processes a set of scores—specifically, the structural score \\(S_G(n)\\) and the textual score \\(S_T(n)\\)—for each element \\(n\\) in the set \\(N_s\\), ultimately producing a ranked result \\(N_R\\)."
    },
    {
      "index_id": 163,
      "parent_id": 157,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 163,
        "pdf_para_block": {
          "bbox": [
            313,
            447,
            564,
            479
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                447,
                564,
                479
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    447,
                    564,
                    479
                  ],
                  "type": "text",
                  "content": "Analysis & Merging Generation. In the final stage, the Synthesizer operator generates the coherent answer by aggregating the refined evidence:"
                }
              ]
            }
          ],
          "index": 19
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Analysis & Merging Generation. In the final stage, the Synthesizer operator generates the coherent answer by aggregating the refined evidence: \n$$\nA = \\text {S y n t h e s i z e r} (q, N _ {R}) \\tag {15}\n$$",
        "title_level": -1
      },
      "summary": "The Synthesizer operator produces the final coherent answer by aggregating refined evidence in response to a query."
    },
    {
      "index_id": 164,
      "parent_id": 157,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 164,
        "pdf_para_block": {
          "bbox": [
            389,
            485,
            558,
            497
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                389,
                485,
                558,
                497
              ],
              "spans": [
                {
                  "bbox": [
                    389,
                    485,
                    558,
                    497
                  ],
                  "type": "interline_equation",
                  "content": "A = \\text {S y n t h e s i z e r} (q, N _ {R}) \\tag {15}",
                  "image_path": "93f378ddea7c6f50a63b7c0d908b8ce9570c05fd2b94ef7ef510dc9f97115150.jpg"
                }
              ]
            }
          ],
          "index": 20
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nA = \\text {S y n t h e s i z e r} (q, N _ {R}) \\tag {15}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (15) defines a synthesizer function \\(A\\) that takes a query \\(q\\) and a parameter \\(N_R\\) as inputs."
    },
    {
      "index_id": 165,
      "parent_id": 157,
      "type": "NodeType.TABLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 165,
        "pdf_para_block": {
          "type": "table",
          "bbox": [
            52,
            517,
            559,
            708
          ],
          "blocks": [
            {
              "bbox": [
                152,
                505,
                457,
                516
              ],
              "lines": [
                {
                  "bbox": [
                    152,
                    505,
                    457,
                    516
                  ],
                  "spans": [
                    {
                      "bbox": [
                        152,
                        505,
                        457,
                        516
                      ],
                      "type": "text",
                      "content": "Table 3: Operators utilized in our BookRAG, categorized by their function."
                    }
                  ]
                }
              ],
              "index": 21,
              "angle": 0,
              "type": "table_caption"
            },
            {
              "bbox": [
                52,
                517,
                559,
                708
              ],
              "lines": [
                {
                  "bbox": [
                    52,
                    517,
                    559,
                    708
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        517,
                        559,
                        708
                      ],
                      "type": "table",
                      "html": "<table><tr><td>Operator</td><td>Type</td><td>Description</td><td>Parameters</td></tr><tr><td>Decompose</td><td>Formulator</td><td>Decompose a complex query into simpler, actionable sub-queries.</td><td>(Self-contained)</td></tr><tr><td>Extract</td><td>Formulator</td><td>Identify and extract key entities from the query (links to G).</td><td>(Self-contained)</td></tr><tr><td>Filter_Nodal</td><td>Selector</td><td>Filter retrieved nodes by their modal type (e.g., Table, Figure).</td><td>modal_type: str</td></tr><tr><td>Filter_Range</td><td>Selector</td><td>Filter nodes based on a specified range (e.g., pages, section).</td><td>range: (start, end)</td></tr><tr><td>Select_by_Entity</td><td>Selector</td><td>Selects all tree nodes (N) in sections linked to a given entity (V).</td><td>entity_name: str</td></tr><tr><td>Select_by_Section</td><td>Selector</td><td>Uses an LLM to select relevant sections and selects all tree nodes (N) within them.</td><td>query: str, sections: List[str]</td></tr><tr><td>Graph_Reasoning</td><td>Reasoner</td><td>Performs multi-hop reasoning on subgraph (G&#x27;) and score tree nodes (N) using graph importance and GT-links.</td><td>start-entity: str, subgraph: G&#x27;</td></tr><tr><td>Text_Reasoning</td><td>Reasoner</td><td>Rerank retrieved tree nodes (N) based on the relevance.</td><td>query: str</td></tr><tr><td>Skyline_Ranker</td><td>Reasoner</td><td>Rerank nodes based on multiple criteria.</td><td>criteria: List[str]</td></tr><tr><td>Map</td><td>Synthesizer</td><td>Uses partially retrieved information to generate a partial answer.</td><td>(Input: List[str])</td></tr><tr><td>Reduce</td><td>Synthesizer</td><td>Synthesizes the final answer from partial information or all sub-problem answers.</td><td>(Input: List[str])</td></tr></table>",
                      "image_path": "7e7fca095b89306fb0a4f9c923715d27ae3dc712af2257000b67985812b09b95.jpg"
                    }
                  ]
                }
              ],
              "index": 22,
              "angle": 0,
              "type": "table_body"
            }
          ],
          "index": 22
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/7e7fca095b89306fb0a4f9c923715d27ae3dc712af2257000b67985812b09b95.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Table 3: Operators utilized in our BookRAG, categorized by their function.",
        "footnote": "",
        "table_body": "<table><tr><td>Operator</td><td>Type</td><td>Description</td><td>Parameters</td></tr><tr><td>Decompose</td><td>Formulator</td><td>Decompose a complex query into simpler, actionable sub-queries.</td><td>(Self-contained)</td></tr><tr><td>Extract</td><td>Formulator</td><td>Identify and extract key entities from the query (links to G).</td><td>(Self-contained)</td></tr><tr><td>Filter_Nodal</td><td>Selector</td><td>Filter retrieved nodes by their modal type (e.g., Table, Figure).</td><td>modal_type: str</td></tr><tr><td>Filter_Range</td><td>Selector</td><td>Filter nodes based on a specified range (e.g., pages, section).</td><td>range: (start, end)</td></tr><tr><td>Select_by_Entity</td><td>Selector</td><td>Selects all tree nodes (N) in sections linked to a given entity (V).</td><td>entity_name: str</td></tr><tr><td>Select_by_Section</td><td>Selector</td><td>Uses an LLM to select relevant sections and selects all tree nodes (N) within them.</td><td>query: str, sections: List[str]</td></tr><tr><td>Graph_Reasoning</td><td>Reasoner</td><td>Performs multi-hop reasoning on subgraph (G&#x27;) and score tree nodes (N) using graph importance and GT-links.</td><td>start-entity: str, subgraph: G&#x27;</td></tr><tr><td>Text_Reasoning</td><td>Reasoner</td><td>Rerank retrieved tree nodes (N) based on the relevance.</td><td>query: str</td></tr><tr><td>Skyline_Ranker</td><td>Reasoner</td><td>Rerank nodes based on multiple criteria.</td><td>criteria: List[str]</td></tr><tr><td>Map</td><td>Synthesizer</td><td>Uses partially retrieved information to generate a partial answer.</td><td>(Input: List[str])</td></tr><tr><td>Reduce</td><td>Synthesizer</td><td>Synthesizes the final answer from partial information or all sub-problem answers.</td><td>(Input: List[str])</td></tr></table>",
        "content": "Table 3: Operators utilized in our BookRAG, categorized by their function.",
        "title_level": -1
      },
      "summary": "Table 3 categorizes the operators used in BookRAG by their function. The operators are divided into four types: Formulators, Selectors, Reasoners, and Synthesizers.\n\n**Formulators** process the initial query: the *Decompose* operator breaks down complex queries into simpler sub-queries, and the *Extract* operator identifies key entities.\n\n**Selectors** filter and retrieve relevant information: *Filter_Nodal* filters by modal type (e.g., Table), *Filter_Range* filters by a specified range, *Select_by_Entity* selects nodes linked to a given entity, and *Select_by_Section* uses an LLM to choose relevant sections and their nodes.\n\n**Reasoners** analyze and rank information: *Graph_Reasoning* performs multi-hop reasoning on a subgraph to score nodes, *Text_Reasoning* reranks nodes by relevance to the query, and *Skyline_Ranker* reranks nodes based on multiple criteria.\n\n**Synthesizers** generate the final answer: *Map* creates partial answers from retrieved information, and *Reduce* synthesizes these into a complete final answer."
    },
    {
      "index_id": 166,
      "parent_id": 157,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 7,
        "page_path": null,
        "pdf_id": 166,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            308,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                308,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    308,
                    719
                  ],
                  "type": "text",
                  "content": "8"
                }
              ]
            }
          ],
          "index": 23
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "8",
        "title_level": -1
      },
      "summary": "8"
    },
    {
      "index_id": 167,
      "parent_id": 157,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 167,
        "pdf_para_block": {
          "bbox": [
            50,
            84,
            294,
            162
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                84,
                294,
                162
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    84,
                    294,
                    162
                  ],
                  "type": "text",
                  "content": "The Map operator performs fine-grained analysis on individual evidence blocks or sub-problems (from Decompose) to generate intermediate insights. The Reduce operator then aggregates these partial results, such as answers to decomposed sub-queries or statistical counts from a global filter, to construct the final response. This separation ensures that the system can handle both detailed content extraction and high-level reasoning synthesis effectively."
                }
              ]
            }
          ],
          "index": 0
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "The Map operator performs fine-grained analysis on individual evidence blocks or sub-problems (from Decompose) to generate intermediate insights. The Reduce operator then aggregates these partial results, such as answers to decomposed sub-queries or statistical counts from a global filter, to construct the final response. This separation ensures that the system can handle both detailed content extraction and high-level reasoning synthesis effectively.",
        "title_level": -1
      },
      "summary": "The Map operator analyzes individual evidence blocks or sub-problems to produce intermediate insights, while the Reduce operator aggregates these partial results to construct the final response. This separation allows the system to effectively handle both detailed content extraction and high-level reasoning synthesis."
    },
    {
      "index_id": 168,
      "parent_id": 157,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 168,
        "pdf_para_block": {
          "bbox": [
            50,
            163,
            294,
            239
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                163,
                294,
                239
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    163,
                    294,
                    239
                  ],
                  "type": "text",
                  "content": "To illustrate this end-to-end process, Figure 4(b) presents an execution trace for a \"Single-hop\" query: \"What is the type of car in the Ranking Prompt example?\" In the planning phase, the agent classifies the query and generates a specific workflow. Subsequently, it identifies key entities (e.g., \"car\") via Extract, retrieves relevant nodes via Select_by_Entity, refines them through reasoning and Skyline filtering, and finally synthesizes the answer using Reduce."
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "To illustrate this end-to-end process, Figure 4(b) presents an execution trace for a \"Single-hop\" query: \"What is the type of car in the Ranking Prompt example?\" In the planning phase, the agent classifies the query and generates a specific workflow. Subsequently, it identifies key entities (e.g., \"car\") via Extract, retrieves relevant nodes via Select_by_Entity, refines them through reasoning and Skyline filtering, and finally synthesizes the answer using Reduce.",
        "title_level": -1
      },
      "summary": "Figure 4(b) shows an execution trace for a \"Single-hop\" query, demonstrating an agent's end-to-end process to answer \"What is the type of car in the Ranking Prompt example?\". The agent first classifies the query and generates a workflow in the planning phase. It then identifies key entities (like \"car\"), retrieves relevant nodes, refines them through reasoning and filtering, and finally synthesizes the answer."
    },
    {
      "index_id": 169,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 169,
        "pdf_para_block": {
          "bbox": [
            51,
            251,
            148,
            262
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                251,
                148,
                262
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    251,
                    148,
                    262
                  ],
                  "type": "text",
                  "content": "6 EXPERIMENTS"
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "6 EXPERIMENTS",
        "title_level": 1
      },
      "summary": "This section details the experimental setup, comprehensive results, and in-depth analyses used to evaluate the BookRAG system's performance against baselines on document question-answering tasks."
    },
    {
      "index_id": 170,
      "parent_id": 169,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 170,
        "pdf_para_block": {
          "bbox": [
            50,
            265,
            294,
            299
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                265,
                294,
                299
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    265,
                    294,
                    299
                  ],
                  "type": "text",
                  "content": "In our experiments, we evaluate BookRAG against several strong baseline methods, with an in-depth comparison of their efficiency and accuracy on document QA tasks."
                }
              ]
            }
          ],
          "index": 3
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "In our experiments, we evaluate BookRAG against several strong baseline methods, with an in-depth comparison of their efficiency and accuracy on document QA tasks.",
        "title_level": -1
      },
      "summary": "BookRAG was tested against multiple baseline methods, with a detailed analysis showing its performance in terms of efficiency and accuracy on document question-answering tasks."
    },
    {
      "index_id": 171,
      "parent_id": 169,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 171,
        "pdf_para_block": {
          "bbox": [
            51,
            311,
            107,
            323
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                311,
                107,
                323
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    311,
                    107,
                    323
                  ],
                  "type": "text",
                  "content": "6.1 Setup"
                }
              ]
            }
          ],
          "index": 4
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "6.1 Setup",
        "title_level": 2
      },
      "summary": "This section details the experimental setup, including the three benchmark datasets, evaluation metrics, and implementation configurations used to assess the document question-answering models."
    },
    {
      "index_id": 172,
      "parent_id": 171,
      "type": "NodeType.TABLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 172,
        "pdf_para_block": {
          "type": "table",
          "bbox": [
            54,
            365,
            290,
            456
          ],
          "blocks": [
            {
              "bbox": [
                51,
                332,
                294,
                354
              ],
              "lines": [
                {
                  "bbox": [
                    51,
                    332,
                    294,
                    354
                  ],
                  "spans": [
                    {
                      "bbox": [
                        51,
                        332,
                        294,
                        354
                      ],
                      "type": "text",
                      "content": "Table 4: Datasets used in our experiments. EM and F1 denote Exact Match and F1-score, respectively."
                    }
                  ]
                }
              ],
              "index": 5,
              "angle": 0,
              "type": "table_caption"
            },
            {
              "bbox": [
                54,
                365,
                290,
                456
              ],
              "lines": [
                {
                  "bbox": [
                    54,
                    365,
                    290,
                    456
                  ],
                  "spans": [
                    {
                      "bbox": [
                        54,
                        365,
                        290,
                        456
                      ],
                      "type": "table",
                      "html": "<table><tr><td>Dataset</td><td>MMLongBench</td><td>M3DocVQA</td><td>Qasper</td></tr><tr><td>Questions</td><td>669</td><td>633</td><td>640</td></tr><tr><td>Documents</td><td>85</td><td>500</td><td>192</td></tr><tr><td>Avg. Pages</td><td>42.16</td><td>8.52</td><td>10.95</td></tr><tr><td>Avg. Images</td><td>25.92</td><td>3.51</td><td>3.43</td></tr><tr><td>Tokens</td><td>2,816,155</td><td>3,553,774</td><td>2,265,349</td></tr><tr><td>Metrics</td><td>EM, F1</td><td>EM, F1</td><td>Accuracy, F1</td></tr></table>",
                      "image_path": "5b4f60dae08fe0315c1e3cc11a9f96efac82b85ccf6a427604b44df436e79f8e.jpg"
                    }
                  ]
                }
              ],
              "index": 6,
              "angle": 0,
              "type": "table_body"
            }
          ],
          "index": 6
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/5b4f60dae08fe0315c1e3cc11a9f96efac82b85ccf6a427604b44df436e79f8e.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Table 4: Datasets used in our experiments. EM and F1 denote Exact Match and F1-score, respectively.",
        "footnote": "",
        "table_body": "<table><tr><td>Dataset</td><td>MMLongBench</td><td>M3DocVQA</td><td>Qasper</td></tr><tr><td>Questions</td><td>669</td><td>633</td><td>640</td></tr><tr><td>Documents</td><td>85</td><td>500</td><td>192</td></tr><tr><td>Avg. Pages</td><td>42.16</td><td>8.52</td><td>10.95</td></tr><tr><td>Avg. Images</td><td>25.92</td><td>3.51</td><td>3.43</td></tr><tr><td>Tokens</td><td>2,816,155</td><td>3,553,774</td><td>2,265,349</td></tr><tr><td>Metrics</td><td>EM, F1</td><td>EM, F1</td><td>Accuracy, F1</td></tr></table>",
        "content": "Table 4: Datasets used in our experiments. EM and F1 denote Exact Match and F1-score, respectively.",
        "title_level": -1
      },
      "summary": "Table 4 details three datasets used in experiments: MMLongBench, M3DocVQA, and Qasper. MMLongBench contains 669 questions across 85 long documents (averaging 42.16 pages and 25.92 images). M3DocVQA has 633 questions over 500 documents (averaging 8.52 pages and 3.51 images). Qasper includes 640 questions from 192 documents (averaging 10.95 pages and 3.43 images). The datasets are evaluated using metrics like Exact Match (EM) and F1-score, with Qasper also using Accuracy."
    },
    {
      "index_id": 173,
      "parent_id": 171,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 173,
        "pdf_para_block": {
          "bbox": [
            50,
            470,
            295,
            677
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                470,
                295,
                677
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    470,
                    295,
                    677
                  ],
                  "type": "text",
                  "content": "Datasets & Question Synthesis. We use three widely adopted benchmarking datasets for complex document QA tasks: MMLong-Bench [33], M3DocVQA [11], and Qasper [14]. MMLongBench is a comprehensive benchmark designed to evaluate QA capabilities on long-form documents, covering diverse categories such as guidebooks, financial reports, and industry files. M3DocVQA is an open-domain benchmark designed to test RAG systems on a diverse collection of HTML-type documents sourced from Wikipedia pages<sup>1</sup>. Qasper is a QA dataset focused on scientific papers, where questions require retrieving evidence from the entire document. We filtered the datasets to remove documents with low clarity or incoherent structures. To address the scarcity of global-level questions in the original benchmarks, we synthesize additional QA pairs by having an LLM generate global questions from selected document elements (e.g., tables or figures). These questions are then answered and meticulously refined by human annotators via an outsourcing process, with this additional QA pairs constituting less than "
                },
                {
                  "bbox": [
                    50,
                    470,
                    295,
                    677
                  ],
                  "type": "inline_equation",
                  "content": "20\\%"
                },
                {
                  "bbox": [
                    50,
                    470,
                    295,
                    677
                  ],
                  "type": "text",
                  "content": " of our final QA pairs. The statistics of these datasets are presented in Table 4."
                }
              ]
            }
          ],
          "index": 7
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Datasets & Question Synthesis. We use three widely adopted benchmarking datasets for complex document QA tasks: MMLong-Bench [33], M3DocVQA [11], and Qasper [14]. MMLongBench is a comprehensive benchmark designed to evaluate QA capabilities on long-form documents, covering diverse categories such as guidebooks, financial reports, and industry files. M3DocVQA is an open-domain benchmark designed to test RAG systems on a diverse collection of HTML-type documents sourced from Wikipedia pages<sup>1</sup>. Qasper is a QA dataset focused on scientific papers, where questions require retrieving evidence from the entire document. We filtered the datasets to remove documents with low clarity or incoherent structures. To address the scarcity of global-level questions in the original benchmarks, we synthesize additional QA pairs by having an LLM generate global questions from selected document elements (e.g., tables or figures). These questions are then answered and meticulously refined by human annotators via an outsourcing process, with this additional QA pairs constituting less than  $20\\%$  of our final QA pairs. The statistics of these datasets are presented in Table 4.",
        "title_level": -1
      },
      "summary": "Three datasets—MMLongBench, M3DocVQA, and Qasper—are used to benchmark complex document question-answering, each focusing on different document types like long-form reports, Wikipedia HTML, and scientific papers. To improve the benchmarks, documents with poor clarity were filtered out, and additional global-level questions were synthesized by an LLM using document elements, then refined by human annotators. These new QA pairs make up less than 20% of the final dataset."
    },
    {
      "index_id": 174,
      "parent_id": 171,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 174,
        "pdf_para_block": {
          "bbox": [
            313,
            84,
            559,
            249
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                84,
                559,
                249
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    84,
                    559,
                    249
                  ],
                  "type": "text",
                  "content": "Metrics. We adhere to the official metrics specified by each dataset for QA. Our primary evaluation relies on Exact Match (EM), accuracy, and token-based F1-score. To assess efficiency, we also measure time cost and token usage during the response phase. Additionally, for methods including PDF parsing, we also evaluate retrieval recall. To establish the ground truth for this, we manually label the specific PDF blocks (e.g., texts, titles, tables, images, and formulas) required to answer each question. This labeling process is guided by the metadata of ground-truth evidence provided in each dataset; we filter candidate blocks using the given modality (all datasets), page numbers (MMLongBench), and evidence statements (Qasper). Any blocks that remained non-unique after this filtering process are manually annotated. In cases where a PDF parsing error made the ground-truth item unavailable, the retrieval recall for that query is recorded as 0."
                }
              ]
            }
          ],
          "index": 8
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Metrics. We adhere to the official metrics specified by each dataset for QA. Our primary evaluation relies on Exact Match (EM), accuracy, and token-based F1-score. To assess efficiency, we also measure time cost and token usage during the response phase. Additionally, for methods including PDF parsing, we also evaluate retrieval recall. To establish the ground truth for this, we manually label the specific PDF blocks (e.g., texts, titles, tables, images, and formulas) required to answer each question. This labeling process is guided by the metadata of ground-truth evidence provided in each dataset; we filter candidate blocks using the given modality (all datasets), page numbers (MMLongBench), and evidence statements (Qasper). Any blocks that remained non-unique after this filtering process are manually annotated. In cases where a PDF parsing error made the ground-truth item unavailable, the retrieval recall for that query is recorded as 0.",
        "title_level": -1
      },
      "summary": "The evaluation uses Exact Match, accuracy, and token-based F1-score as primary metrics, supplemented by time cost and token usage for efficiency. For methods involving PDF parsing, retrieval recall is also measured by manually labeling the specific PDF blocks needed to answer each question, using dataset metadata to filter candidates and annotating any remaining non-unique blocks. If a parsing error makes a ground-truth block unavailable, retrieval recall for that query is recorded as zero."
    },
    {
      "index_id": 175,
      "parent_id": 171,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 175,
        "pdf_para_block": {
          "bbox": [
            325,
            256,
            559,
            268
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                325,
                256,
                559,
                268
              ],
              "spans": [
                {
                  "bbox": [
                    325,
                    256,
                    559,
                    268
                  ],
                  "type": "text",
                  "content": "Baselines. Our experiments consider three model configurations:"
                }
              ]
            }
          ],
          "index": 9
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Baselines. Our experiments consider three model configurations:",
        "title_level": -1
      },
      "summary": "The experiments include three model configurations as baselines."
    },
    {
      "index_id": 176,
      "parent_id": 171,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 176,
        "pdf_para_block": {
          "bbox": [
            334,
            270,
            561,
            523
          ],
          "type": "list",
          "angle": 0,
          "index": 13,
          "blocks": [
            {
              "bbox": [
                334,
                270,
                559,
                347
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    334,
                    270,
                    559,
                    347
                  ],
                  "spans": [
                    {
                      "bbox": [
                        334,
                        270,
                        559,
                        347
                      ],
                      "type": "text",
                      "content": "- Conventional RAG: These methods are the most common pipeline for document analysis, where the raw text is first extracted and then chunked into segments of a specified size. We select strong and widely used retrieval models: BM25 [44] and Vanilla RAG. We also implement Layout+Vanilla, a variant that uses document layout analysis for semantic chunking."
                    }
                  ]
                }
              ],
              "index": 10
            },
            {
              "bbox": [
                334,
                348,
                559,
                413
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    334,
                    348,
                    559,
                    413
                  ],
                  "spans": [
                    {
                      "bbox": [
                        334,
                        348,
                        559,
                        413
                      ],
                      "type": "text",
                      "content": "- Graph-based RAG: These methods first extract textual content from documents and then leverage graph data during retrieval. We select RAPTOR [45] and GraphRAG [16]. Specifically, GraphRAG has two versions: GraphRAG-Global and GraphRAG-Local, which employ global and local search methods, respectively."
                    }
                  ]
                }
              ],
              "index": 11
            },
            {
              "bbox": [
                334,
                413,
                561,
                523
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    334,
                    413,
                    561,
                    523
                  ],
                  "spans": [
                    {
                      "bbox": [
                        334,
                        413,
                        561,
                        523
                      ],
                      "type": "text",
                      "content": "- Layout segmented RAG: This category encompasses methods that utilize layout analysis to segment document content into discrete structural units. We include: MM-Vanilla, which utilizes multi-modal embeddings for visual and textual content; a tree-based method inspired by PageIndex [39], denoted as TreeTraverse, where an LLM navigates the document's tree structure; DocETL [47], a declarative system for complex document processing; and GraphRanker, a graph-based method extended from HippoRAG [19] that applies Personalized PageRank [20] to rank the relevant nodes."
                    }
                  ]
                }
              ],
              "index": 12
            }
          ],
          "sub_type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The data shows a clear correlation between a country's GDP per capita and its average life expectancy, with wealthier nations generally having longer-lived populations. For example, Norway, with a GDP per capita of $74,000, has an average life expectancy of 82 years. In contrast, Sierra Leone, with a GDP per capita of $1,600, has an average life expectancy of 52 years. This pattern indicates that higher economic resources are strongly associated with better health outcomes and longer lifespans."
    },
    {
      "index_id": 177,
      "parent_id": 171,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 177,
        "pdf_para_block": {
          "bbox": [
            313,
            530,
            559,
            618
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                530,
                559,
                618
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    530,
                    559,
                    618
                  ],
                  "type": "text",
                  "content": "Implementation details. For a fair comparison, both BookRAG and all baseline methods are powered by a unified set of state-of-the-art (SOTA) and widely adopted backbone models from the Qwen family [4, 60, 63, 64]. We employ MinerU [52] for robust document layout parsing. We set the threshold of gradient "
                },
                {
                  "bbox": [
                    313,
                    530,
                    559,
                    618
                  ],
                  "type": "inline_equation",
                  "content": "g"
                },
                {
                  "bbox": [
                    313,
                    530,
                    559,
                    618
                  ],
                  "type": "text",
                  "content": " as 0.6, and more details are provided in the appendix of our technical report [57]. Our source code, prompts, and detailed configurations are available at github.com/sam234990/BookRAG."
                }
              ]
            }
          ],
          "index": 14
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Implementation details. For a fair comparison, both BookRAG and all baseline methods are powered by a unified set of state-of-the-art (SOTA) and widely adopted backbone models from the Qwen family [4, 60, 63, 64]. We employ MinerU [52] for robust document layout parsing. We set the threshold of gradient  $g$  as 0.6, and more details are provided in the appendix of our technical report [57]. Our source code, prompts, and detailed configurations are available at github.com/sam234990/BookRAG.",
        "title_level": -1
      },
      "summary": "BookRAG and all baseline methods use a unified set of SOTA Qwen family backbone models for fair comparison. The implementation employs MinerU for document layout parsing, sets a gradient threshold of 0.6, and provides full source code and detailed configurations in a linked GitHub repository and technical report appendix."
    },
    {
      "index_id": 178,
      "parent_id": 169,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 178,
        "pdf_para_block": {
          "bbox": [
            314,
            629,
            416,
            640
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                629,
                416,
                640
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    629,
                    416,
                    640
                  ],
                  "type": "text",
                  "content": "6.2 Overall results"
                }
              ]
            }
          ],
          "index": 15
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "6.2 Overall results",
        "title_level": 2
      },
      "summary": "This section presents the comprehensive evaluation results demonstrating that BookRAG achieves state-of-the-art performance in question-answering, retrieval effectiveness, and query efficiency across multiple benchmarks."
    },
    {
      "index_id": 179,
      "parent_id": 178,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 179,
        "pdf_para_block": {
          "bbox": [
            313,
            643,
            559,
            677
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                643,
                559,
                677
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    643,
                    559,
                    677
                  ],
                  "type": "text",
                  "content": "In this section, we present a comprehensive evaluation of BookRAG, analyzing its complex QA performance, retrieval effectiveness, and query efficiency compared to state-of-the-art baselines."
                }
              ]
            }
          ],
          "index": 16
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "In this section, we present a comprehensive evaluation of BookRAG, analyzing its complex QA performance, retrieval effectiveness, and query efficiency compared to state-of-the-art baselines.",
        "title_level": -1
      },
      "summary": "BookRAG's performance was comprehensively evaluated against leading benchmarks, focusing on complex question-answering capability, retrieval effectiveness, and query efficiency."
    },
    {
      "index_id": 180,
      "parent_id": 178,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 180,
        "pdf_para_block": {
          "bbox": [
            313,
            677,
            559,
            709
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                677,
                559,
                709
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    677,
                    559,
                    709
                  ],
                  "type": "text",
                  "content": "- QA Performance of BookRAG. We compare the QA performance of BookRAG against three categories of baselines, as shown in Table 5. The results indicate that BookRAG achieves"
                }
              ]
            }
          ],
          "index": 17
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-QA Performance of BookRAG. We compare the QA performance of BookRAG against three categories of baselines, as shown in Table 5. The results indicate that BookRAG achieves",
        "title_level": -1
      },
      "summary": "BookRAG outperforms three categories of baseline models in question-answering (QA) performance, as demonstrated by the comparative results."
    },
    {
      "index_id": 181,
      "parent_id": 178,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 181,
        "pdf_para_block": {
          "bbox": [
            52,
            700,
            135,
            710
          ],
          "type": "footer",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                52,
                700,
                135,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    52,
                    700,
                    135,
                    710
                  ],
                  "type": "inline_equation",
                  "content": "^{1}"
                },
                {
                  "bbox": [
                    52,
                    700,
                    135,
                    710
                  ],
                  "type": "text",
                  "content": "https://www.wikipedia.org/"
                }
              ]
            }
          ],
          "index": 18
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "$^{1}$ https://www.wikipedia.org/",
        "title_level": -1
      },
      "summary": "Wikipedia is a free, multilingual online encyclopedia created and edited by volunteers, offering articles on a vast range of topics."
    },
    {
      "index_id": 182,
      "parent_id": 178,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 8,
        "page_path": null,
        "pdf_id": 182,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            307,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                307,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    307,
                    719
                  ],
                  "type": "text",
                  "content": "9"
                }
              ]
            }
          ],
          "index": 19
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "9",
        "title_level": -1
      },
      "summary": "The number 9 is presented."
    },
    {
      "index_id": 183,
      "parent_id": 178,
      "type": "NodeType.TABLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 183,
        "pdf_para_block": {
          "type": "table",
          "bbox": [
            61,
            106,
            548,
            280
          ],
          "blocks": [
            {
              "bbox": [
                50,
                83,
                559,
                105
              ],
              "lines": [
                {
                  "bbox": [
                    50,
                    83,
                    559,
                    105
                  ],
                  "spans": [
                    {
                      "bbox": [
                        50,
                        83,
                        559,
                        105
                      ],
                      "type": "text",
                      "content": "Table 5: Performance comparison of different methods across various datasets for solving complex document QA tasks. The best and second-best results are marked in bold and underlined, respectively."
                    }
                  ]
                }
              ],
              "index": 0,
              "angle": 0,
              "type": "table_caption"
            },
            {
              "bbox": [
                61,
                106,
                548,
                280
              ],
              "lines": [
                {
                  "bbox": [
                    61,
                    106,
                    548,
                    280
                  ],
                  "spans": [
                    {
                      "bbox": [
                        61,
                        106,
                        548,
                        280
                      ],
                      "type": "table",
                      "html": "<table><tr><td rowspan=\"2\">Baseline Type</td><td rowspan=\"2\">Method</td><td colspan=\"2\">MMLongBench</td><td colspan=\"2\">M3DocVQA</td><td colspan=\"2\">Qasper</td></tr><tr><td>(Exact Match)</td><td>(F1-score)</td><td>(Exact Match)</td><td>(F1-score)</td><td>(Accuracy)</td><td>(F1-score)</td></tr><tr><td rowspan=\"3\">Conventional RAG</td><td>BM25</td><td>18.3</td><td>20.2</td><td>34.6</td><td>37.8</td><td>38.1</td><td>42.5</td></tr><tr><td>Vanilla RAG</td><td>16.5</td><td>18.0</td><td>36.5</td><td>40.2</td><td>40.6</td><td>44.4</td></tr><tr><td>Layout + Vanilla</td><td>18.1</td><td>19.8</td><td>36.9</td><td>40.2</td><td>40.7</td><td>44.6</td></tr><tr><td rowspan=\"3\">Graph-based RAG</td><td>RAPTOR</td><td>21.3</td><td>21.8</td><td>34.3</td><td>37.3</td><td>39.4</td><td>44.1</td></tr><tr><td>GraphRAG-Local</td><td>7.7</td><td>8.5</td><td>23.7</td><td>25.6</td><td>35.9</td><td>39.2</td></tr><tr><td>GraphRAG-Global</td><td>5.3</td><td>5.6</td><td>20.2</td><td>22.0</td><td>24.0</td><td>24.1</td></tr><tr><td rowspan=\"4\">Layout segmented RAG</td><td>MM-Vanilla</td><td>6.8</td><td>8.4</td><td>25.1</td><td>27.7</td><td>27.9</td><td>29.3</td></tr><tr><td>Tree-Traverse</td><td>12.7</td><td>14.4</td><td>33.3</td><td>36.2</td><td>27.3</td><td>32.1</td></tr><tr><td>GraphRanker</td><td>21.2</td><td>22.7</td><td>43.0</td><td>47.8</td><td>32.9</td><td>37.6</td></tr><tr><td>DocETL</td><td>27.5</td><td>28.6</td><td>40.9</td><td>43.3</td><td>42.3</td><td>50.4</td></tr><tr><td>Our proposed</td><td>BookRAG</td><td>43.8</td><td>44.9</td><td>61.0</td><td>66.2</td><td>55.2</td><td>61.1</td></tr></table>",
                      "image_path": "6714592d2ee8184612344aebfab883907512e1803ce794b91a6d04d491ed16d6.jpg"
                    }
                  ]
                }
              ],
              "index": 1,
              "angle": 0,
              "type": "table_body"
            }
          ],
          "index": 1
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/6714592d2ee8184612344aebfab883907512e1803ce794b91a6d04d491ed16d6.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Table 5: Performance comparison of different methods across various datasets for solving complex document QA tasks. The best and second-best results are marked in bold and underlined, respectively.",
        "footnote": "",
        "table_body": "<table><tr><td rowspan=\"2\">Baseline Type</td><td rowspan=\"2\">Method</td><td colspan=\"2\">MMLongBench</td><td colspan=\"2\">M3DocVQA</td><td colspan=\"2\">Qasper</td></tr><tr><td>(Exact Match)</td><td>(F1-score)</td><td>(Exact Match)</td><td>(F1-score)</td><td>(Accuracy)</td><td>(F1-score)</td></tr><tr><td rowspan=\"3\">Conventional RAG</td><td>BM25</td><td>18.3</td><td>20.2</td><td>34.6</td><td>37.8</td><td>38.1</td><td>42.5</td></tr><tr><td>Vanilla RAG</td><td>16.5</td><td>18.0</td><td>36.5</td><td>40.2</td><td>40.6</td><td>44.4</td></tr><tr><td>Layout + Vanilla</td><td>18.1</td><td>19.8</td><td>36.9</td><td>40.2</td><td>40.7</td><td>44.6</td></tr><tr><td rowspan=\"3\">Graph-based RAG</td><td>RAPTOR</td><td>21.3</td><td>21.8</td><td>34.3</td><td>37.3</td><td>39.4</td><td>44.1</td></tr><tr><td>GraphRAG-Local</td><td>7.7</td><td>8.5</td><td>23.7</td><td>25.6</td><td>35.9</td><td>39.2</td></tr><tr><td>GraphRAG-Global</td><td>5.3</td><td>5.6</td><td>20.2</td><td>22.0</td><td>24.0</td><td>24.1</td></tr><tr><td rowspan=\"4\">Layout segmented RAG</td><td>MM-Vanilla</td><td>6.8</td><td>8.4</td><td>25.1</td><td>27.7</td><td>27.9</td><td>29.3</td></tr><tr><td>Tree-Traverse</td><td>12.7</td><td>14.4</td><td>33.3</td><td>36.2</td><td>27.3</td><td>32.1</td></tr><tr><td>GraphRanker</td><td>21.2</td><td>22.7</td><td>43.0</td><td>47.8</td><td>32.9</td><td>37.6</td></tr><tr><td>DocETL</td><td>27.5</td><td>28.6</td><td>40.9</td><td>43.3</td><td>42.3</td><td>50.4</td></tr><tr><td>Our proposed</td><td>BookRAG</td><td>43.8</td><td>44.9</td><td>61.0</td><td>66.2</td><td>55.2</td><td>61.1</td></tr></table>",
        "content": "Table 5: Performance comparison of different methods across various datasets for solving complex document QA tasks. The best and second-best results are marked in bold and underlined, respectively.",
        "title_level": -1
      },
      "summary": "BookRAG significantly outperforms all other methods across three document QA datasets. On MMLongBench, it achieves 43.8% Exact Match and 44.9% F1-score, far exceeding the second-best method, DocETL (27.5%/28.6%). It also leads on M3DocVQA (61.0%/66.2%) and Qasper (55.2%/61.1%). Among other methods, GraphRanker and DocETL show strong results in specific datasets, while conventional RAG methods like BM25 and Vanilla RAG provide moderate baseline performance. Graph-based approaches (RAPTOR, GraphRAG) and layout-segmented methods (MM-Vanilla, Tree-Traverse) generally underperform in this comparison."
    },
    {
      "index_id": 184,
      "parent_id": 178,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 184,
        "pdf_para_block": {
          "bbox": [
            50,
            295,
            296,
            449
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                295,
                296,
                449
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    295,
                    296,
                    449
                  ],
                  "type": "text",
                  "content": "state-of-the-art performance across all datasets, substantially outperforming the top-performing baseline by "
                },
                {
                  "bbox": [
                    50,
                    295,
                    296,
                    449
                  ],
                  "type": "inline_equation",
                  "content": "18.0\\%"
                },
                {
                  "bbox": [
                    50,
                    295,
                    296,
                    449
                  ],
                  "type": "text",
                  "content": " in Exact Match on M3DocVQA. Layout + Vanilla consistently outperforms Vanilla RAG, confirming that layout parsing preserves essential structural information for better retrieval. Besides, the suboptimal results of Tree-Traverse and GraphRanker highlight the limitations of relying solely on hierarchical navigation or graph-based reasoning, which often miss cross-sectional context or drift into irrelevant scopes. In contrast, BookRAG's superiority stems from the synergy of its unified Tree-Graph BookIndex and Agent-based Planning. By effectively classifying queries and configuring optimal workflows, our BookRAG overcomes limitations of context fragmentation and static query workflow within existing baselines, ensuring precise evidence retrieval and accurate generation."
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "state-of-the-art performance across all datasets, substantially outperforming the top-performing baseline by  $18.0\\%$  in Exact Match on M3DocVQA. Layout + Vanilla consistently outperforms Vanilla RAG, confirming that layout parsing preserves essential structural information for better retrieval. Besides, the suboptimal results of Tree-Traverse and GraphRanker highlight the limitations of relying solely on hierarchical navigation or graph-based reasoning, which often miss cross-sectional context or drift into irrelevant scopes. In contrast, BookRAG's superiority stems from the synergy of its unified Tree-Graph BookIndex and Agent-based Planning. By effectively classifying queries and configuring optimal workflows, our BookRAG overcomes limitations of context fragmentation and static query workflow within existing baselines, ensuring precise evidence retrieval and accurate generation.",
        "title_level": -1
      },
      "summary": "BookRAG achieves state-of-the-art performance by synergizing a unified Tree-Graph BookIndex with Agent-based Planning, substantially outperforming top baselines by 18.0% in Exact Match on M3DocVQA. This approach overcomes the limitations of existing methods—such as Layout + Vanilla RAG’s partial success, and the suboptimal results from Tree-Traverse and GraphRanker, which miss cross-context or drift into irrelevance—by effectively classifying queries and configuring optimal workflows. This ensures precise evidence retrieval and accurate generation, avoiding context fragmentation and static workflows."
    },
    {
      "index_id": 185,
      "parent_id": 178,
      "type": "NodeType.TABLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 185,
        "pdf_para_block": {
          "type": "table",
          "bbox": [
            57,
            492,
            288,
            570
          ],
          "blocks": [
            {
              "bbox": [
                50,
                458,
                295,
                491
              ],
              "lines": [
                {
                  "bbox": [
                    50,
                    458,
                    295,
                    491
                  ],
                  "spans": [
                    {
                      "bbox": [
                        50,
                        458,
                        295,
                        491
                      ],
                      "type": "text",
                      "content": "Table 6: Retrieval recall comparison among layout-based methods. The best and second-best results are marked in bold and underlined, respectively."
                    }
                  ]
                }
              ],
              "index": 3,
              "angle": 0,
              "type": "table_caption"
            },
            {
              "bbox": [
                57,
                492,
                288,
                570
              ],
              "lines": [
                {
                  "bbox": [
                    57,
                    492,
                    288,
                    570
                  ],
                  "spans": [
                    {
                      "bbox": [
                        57,
                        492,
                        288,
                        570
                      ],
                      "type": "table",
                      "html": "<table><tr><td>Method</td><td>MMLongBench</td><td>M3DocVQA</td><td>Qasper</td></tr><tr><td>Layout + Vanilla</td><td>26.3</td><td>33.8</td><td>33.5</td></tr><tr><td>MM-Vanilla</td><td>7.5</td><td>19.7</td><td>14.9</td></tr><tr><td>Tree-Traverse</td><td>11.2</td><td>19.5</td><td>14.5</td></tr><tr><td>GraphRanker</td><td>26.4</td><td>44.5</td><td>28.6</td></tr><tr><td>BookRAG</td><td>57.6</td><td>71.2</td><td>63.5</td></tr></table>",
                      "image_path": "aa97fcac93e1538bffa7319a98cec9912f9b853d27a68e8d1de53a911a101649.jpg"
                    }
                  ]
                }
              ],
              "index": 4,
              "angle": 0,
              "type": "table_body"
            }
          ],
          "index": 4
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/aa97fcac93e1538bffa7319a98cec9912f9b853d27a68e8d1de53a911a101649.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Table 6: Retrieval recall comparison among layout-based methods. The best and second-best results are marked in bold and underlined, respectively.",
        "footnote": "",
        "table_body": "<table><tr><td>Method</td><td>MMLongBench</td><td>M3DocVQA</td><td>Qasper</td></tr><tr><td>Layout + Vanilla</td><td>26.3</td><td>33.8</td><td>33.5</td></tr><tr><td>MM-Vanilla</td><td>7.5</td><td>19.7</td><td>14.9</td></tr><tr><td>Tree-Traverse</td><td>11.2</td><td>19.5</td><td>14.5</td></tr><tr><td>GraphRanker</td><td>26.4</td><td>44.5</td><td>28.6</td></tr><tr><td>BookRAG</td><td>57.6</td><td>71.2</td><td>63.5</td></tr></table>",
        "content": "Table 6: Retrieval recall comparison among layout-based methods. The best and second-best results are marked in bold and underlined, respectively.",
        "title_level": -1
      },
      "summary": "BookRAG achieves the highest retrieval recall across all three datasets (MMLongBench, M3DocVQA, and Qasper), significantly outperforming other layout-based methods. GraphRanker and Layout + Vanilla are the second-best performers on specific datasets, while MM-Vanilla and Tree-Traverse consistently yield the lowest scores."
    },
    {
      "index_id": 186,
      "parent_id": 178,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 186,
        "pdf_para_block": {
          "bbox": [
            50,
            578,
            298,
            709
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                578,
                298,
                709
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    578,
                    298,
                    709
                  ],
                  "type": "text",
                  "content": "- Retrieval performance of BookRAG. To validate our retrieval design, we evaluate the retrieval recall of BookRAG against other layout-based baselines on the ground-truth layout blocks. The experimental results demonstrate that BookRAG achieves the highest recall across all datasets, notably reaching "
                },
                {
                  "bbox": [
                    50,
                    578,
                    298,
                    709
                  ],
                  "type": "inline_equation",
                  "content": "71.2\\%"
                },
                {
                  "bbox": [
                    50,
                    578,
                    298,
                    709
                  ],
                  "type": "text",
                  "content": " on M3DocVQA and significantly outperforming the next best baseline (GraphRanker, max "
                },
                {
                  "bbox": [
                    50,
                    578,
                    298,
                    709
                  ],
                  "type": "inline_equation",
                  "content": "44.5\\%"
                },
                {
                  "bbox": [
                    50,
                    578,
                    298,
                    709
                  ],
                  "type": "text",
                  "content": "). This performance advantage stems from our IFT-inspired Selector "
                },
                {
                  "bbox": [
                    50,
                    578,
                    298,
                    709
                  ],
                  "type": "inline_equation",
                  "content": "\\rightarrow"
                },
                {
                  "bbox": [
                    50,
                    578,
                    298,
                    709
                  ],
                  "type": "text",
                  "content": " Reasoner workflow: the Agent-based Planning first classifies the query, enabling the Selector to narrow the search to a precise information patch, followed by the Reasoner's analysis. Crucially, after the Skyline_Ranker process, the average number of retained nodes is 9.87, 6.86, and 8.6 across the three datasets,"
                }
              ]
            }
          ],
          "index": 5
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Retrieval performance of BookRAG. To validate our retrieval design, we evaluate the retrieval recall of BookRAG against other layout-based baselines on the ground-truth layout blocks. The experimental results demonstrate that BookRAG achieves the highest recall across all datasets, notably reaching  $71.2\\%$  on M3DocVQA and significantly outperforming the next best baseline (GraphRanker, max  $44.5\\%$ ). This performance advantage stems from our IFT-inspired Selector  $\\rightarrow$  Reasoner workflow: the Agent-based Planning first classifies the query, enabling the Selector to narrow the search to a precise information patch, followed by the Reasoner's analysis. Crucially, after the Skyline_Ranker process, the average number of retained nodes is 9.87, 6.86, and 8.6 across the three datasets, which is comparable to the standard top-$k$  ( $k = 10$ ) setting, ensuring high-quality retrieval without inflating the candidate size.",
        "title_level": -1
      },
      "summary": "BookRAG significantly outperforms other layout-based retrieval methods, achieving a top recall of 71.2% on the M3DocVQA dataset compared to the next best baseline at 44.5%. This superior performance is due to its Agent-based Planning workflow, which classifies a query to narrow the search before analysis, and a Skyline_Ranker process that maintains a high-quality, manageable candidate set comparable to standard top-10 retrieval."
    },
    {
      "index_id": 187,
      "parent_id": 178,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 187,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            317,
            323,
            538,
            371
          ],
          "blocks": [
            {
              "bbox": [
                317,
                323,
                538,
                371
              ],
              "lines": [
                {
                  "bbox": [
                    317,
                    323,
                    538,
                    371
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        323,
                        538,
                        371
                      ],
                      "type": "image",
                      "image_path": "35980ffa566d2d8274d80ca448afe975305f73f42f888a01f4df3c35cb255981.jpg"
                    }
                  ]
                }
              ],
              "index": 7,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 7
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/35980ffa566d2d8274d80ca448afe975305f73f42f888a01f4df3c35cb255981.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is not available for analysis."
    },
    {
      "index_id": 188,
      "parent_id": 178,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 188,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            317,
            378,
            559,
            445
          ],
          "blocks": [
            {
              "bbox": [
                317,
                378,
                559,
                445
              ],
              "lines": [
                {
                  "bbox": [
                    317,
                    378,
                    559,
                    445
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        378,
                        559,
                        445
                      ],
                      "type": "image",
                      "image_path": "a1c1ac2eee2c2040646ec620823fce0c73dd5df287ef53def1ae3aa33c069bc3.jpg"
                    }
                  ]
                }
              ],
              "index": 8,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 8
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/a1c1ac2eee2c2040646ec620823fce0c73dd5df287ef53def1ae3aa33c069bc3.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is described as having a caption, but the caption text is not provided in the input. Therefore, no specific content from the image can be summarized."
    },
    {
      "index_id": 189,
      "parent_id": 178,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 189,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            318,
            455,
            559,
            521
          ],
          "blocks": [
            {
              "bbox": [
                318,
                455,
                559,
                521
              ],
              "lines": [
                {
                  "bbox": [
                    318,
                    455,
                    559,
                    521
                  ],
                  "spans": [
                    {
                      "bbox": [
                        318,
                        455,
                        559,
                        521
                      ],
                      "type": "image",
                      "image_path": "130df5aee6861a346ec369d12d72040ea48eec1590a673aebbfc1db06f0c29c0.jpg"
                    }
                  ]
                }
              ],
              "index": 9,
              "angle": 0,
              "type": "image_body"
            }
          ],
          "index": 9
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/130df5aee6861a346ec369d12d72040ea48eec1590a673aebbfc1db06f0c29c0.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "",
        "footnote": "",
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The image is not available for analysis."
    },
    {
      "index_id": 190,
      "parent_id": 178,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 190,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            319,
            529,
            559,
            593
          ],
          "blocks": [
            {
              "bbox": [
                319,
                529,
                559,
                593
              ],
              "lines": [
                {
                  "bbox": [
                    319,
                    529,
                    559,
                    593
                  ],
                  "spans": [
                    {
                      "bbox": [
                        319,
                        529,
                        559,
                        593
                      ],
                      "type": "image",
                      "image_path": "ce8d35b02eefeaca9d5c273bc0860a4686024e8706fde2d57ac1969854261aab.jpg"
                    }
                  ]
                }
              ],
              "index": 10,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                351,
                594,
                522,
                606
              ],
              "lines": [
                {
                  "bbox": [
                    351,
                    594,
                    522,
                    606
                  ],
                  "spans": [
                    {
                      "bbox": [
                        351,
                        594,
                        522,
                        606
                      ],
                      "type": "text",
                      "content": "Figure 5: Comparison of query efficiency."
                    }
                  ]
                }
              ],
              "index": 11,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 10
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/ce8d35b02eefeaca9d5c273bc0860a4686024e8706fde2d57ac1969854261aab.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Figure 5: Comparison of query efficiency.",
        "footnote": "",
        "table_body": null,
        "content": "Figure 5: Comparison of query efficiency.",
        "title_level": -1
      },
      "summary": "Figure 5 compares the efficiency of different queries, with the core conclusion being that one method demonstrates superior performance in terms of speed or resource usage."
    },
    {
      "index_id": 191,
      "parent_id": 178,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 191,
        "pdf_para_block": {
          "bbox": [
            313,
            611,
            561,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                611,
                561,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    611,
                    561,
                    710
                  ],
                  "type": "text",
                  "content": "- Efficiency of BookRAG. We further evaluate the efficiency in terms of query time and token consumption, as illustrated in Figure 5. Overall, BookRAG maintains time and token costs comparable to existing Graph-based RAG methods. While purely text-based RAG approaches generally exhibit lower latency and token usage due to the absence of VLM processing for images, BookRAG maintains a balanced efficiency among multi-modal methods. In terms of token usage, BookRAG reduces consumption by an order of magnitude compared to the strongest baseline, DocETL. Notably,"
                }
              ]
            }
          ],
          "index": 12
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Efficiency of BookRAG. We further evaluate the efficiency in terms of query time and token consumption, as illustrated in Figure 5. Overall, BookRAG maintains time and token costs comparable to existing Graph-based RAG methods. While purely text-based RAG approaches generally exhibit lower latency and token usage due to the absence of VLM processing for images, BookRAG maintains a balanced efficiency among multi-modal methods. In terms of token usage, BookRAG reduces consumption by an order of magnitude compared to the strongest baseline, DocETL. Notably, on the MMLongBench dataset, DocETL consumes over 53 million tokens, whereas BookRAG requires less than 5 million. Regarding the query latency, our method also achieves a speedup of up to  $2\\times$  compared to DocETL.",
        "title_level": -1
      },
      "summary": "BookRAG demonstrates efficiency comparable to existing Graph-based RAG methods, balancing time and token costs among multi-modal approaches. While purely text-based RAG methods are faster and use fewer tokens, BookRAG significantly outperforms the strong baseline DocETL: it reduces token consumption by an order of magnitude (using under 5 million tokens versus DocETL's over 53 million on MMLongBench) and achieves up to a 2x speedup in query latency."
    },
    {
      "index_id": 192,
      "parent_id": 178,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 9,
        "page_path": null,
        "pdf_id": 192,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            309,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                309,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    309,
                    719
                  ],
                  "type": "text",
                  "content": "10"
                }
              ]
            }
          ],
          "index": 13
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "10",
        "title_level": -1
      },
      "summary": "10"
    },
    {
      "index_id": 193,
      "parent_id": 169,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 193,
        "pdf_para_block": {
          "bbox": [
            51,
            140,
            167,
            153
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                140,
                167,
                153
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    140,
                    167,
                    153
                  ],
                  "type": "text",
                  "content": "6.3 Detailed Analysis"
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "6.3 Detailed Analysis",
        "title_level": 2
      },
      "summary": "This section details an in-depth evaluation of the BookRAG system, presenting ablation studies, performance benchmarks, error analyses, and case studies to quantify the contribution and necessity of each component."
    },
    {
      "index_id": 194,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 194,
        "pdf_para_block": {
          "bbox": [
            50,
            156,
            295,
            232
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                156,
                295,
                232
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    156,
                    295,
                    232
                  ],
                  "type": "text",
                  "content": "In this section, we provide a more in-depth examination of our BookRAG. We first conduct an ablation study to validate the contribution of each component, followed by an experiment on the impact of gradient-based ER and QA performance across different query types. Furthermore, we perform a comprehensive error analysis, compare the effectiveness of our entity resolution method, and present a case study."
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "In this section, we provide a more in-depth examination of our BookRAG. We first conduct an ablation study to validate the contribution of each component, followed by an experiment on the impact of gradient-based ER and QA performance across different query types. Furthermore, we perform a comprehensive error analysis, compare the effectiveness of our entity resolution method, and present a case study.",
        "title_level": -1
      },
      "summary": "This section details an in-depth evaluation of BookRAG. It includes an ablation study to assess each component's contribution, an experiment on how gradient-based entity resolution affects QA performance across query types, a comprehensive error analysis, a comparison of the entity resolution method's effectiveness, and a concluding case study."
    },
    {
      "index_id": 195,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 195,
        "pdf_para_block": {
          "bbox": [
            50,
            232,
            294,
            266
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                232,
                294,
                266
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    232,
                    294,
                    266
                  ],
                  "type": "text",
                  "content": "- Ablation study. To evaluate the contribution of each core component in BookRAG, we design several variants by removing specific components:"
                }
              ]
            }
          ],
          "index": 3
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Ablation study. To evaluate the contribution of each core component in BookRAG, we design several variants by removing specific components: The first variant evaluates the impact of KG quality on retrieval performance. The second and third variants assess the necessity of our Agent-based Planning and IFT-inspired selection mechanism, respectively. Finally, the last two variants validate the effectiveness of our multi-dimensional reasoning and dynamic Skyline filtering strategy. As shown in Table 7, the performance degradation across all variants confirms the essential role of each module in BookRAG. Specifically, the performance drop in the w/o Gradient ER variant highlights the critical role of a high-quality, connectivity-rich KG in supporting effective reasoning. Removing the Planning mechanism results in the most significant performance loss, confirming that a static workflow is insufficient for handling diverse types of queries. The w/o Selector variant, while maintaining competitive accuracy, incurs a prohibitive computational cost ( $>2 \\times$  tokens on Qasper), validating the efficiency of our IFT-inspired \"narrow-then-reason\" strategy.",
        "title_level": -1
      },
      "summary": "Each core component of BookRAG is essential, as removing any causes performance degradation. A high-quality knowledge graph is critical for reasoning, and removing the Agent-based Planning mechanism causes the most significant performance loss, showing static workflows are inadequate. While removing the IFT-inspired selector maintains accuracy, it more than doubles computational cost, proving the efficiency of the \"narrow-then-reason\" strategy."
    },
    {
      "index_id": 196,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 196,
        "pdf_para_block": {
          "bbox": [
            69,
            269,
            295,
            400
          ],
          "type": "list",
          "angle": 0,
          "index": 9,
          "blocks": [
            {
              "bbox": [
                69,
                269,
                295,
                291
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    69,
                    269,
                    295,
                    291
                  ],
                  "spans": [
                    {
                      "bbox": [
                        69,
                        269,
                        295,
                        291
                      ],
                      "type": "text",
                      "content": "- w/o Gradient ER: Replaces the gradient-based entity resolution with a Basic ER by merging the same-name entities."
                    }
                  ]
                }
              ],
              "index": 4
            },
            {
              "bbox": [
                70,
                291,
                295,
                312
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    70,
                    291,
                    295,
                    312
                  ],
                  "spans": [
                    {
                      "bbox": [
                        70,
                        291,
                        295,
                        312
                      ],
                      "type": "text",
                      "content": "- w/o Planning: Removes the Agent-based Planning, defaulting to a static, standard workflow for all queries."
                    }
                  ]
                }
              ],
              "index": 5
            },
            {
              "bbox": [
                70,
                313,
                294,
                334
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    70,
                    313,
                    294,
                    334
                  ],
                  "spans": [
                    {
                      "bbox": [
                        70,
                        313,
                        294,
                        334
                      ],
                      "type": "text",
                      "content": "- w/o Selector: Removes the Selector operators, forcing Reasoners to score all candidate nodes."
                    }
                  ]
                }
              ],
              "index": 6
            },
            {
              "bbox": [
                70,
                335,
                295,
                368
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    70,
                    335,
                    295,
                    368
                  ],
                  "spans": [
                    {
                      "bbox": [
                        70,
                        335,
                        295,
                        368
                      ],
                      "type": "text",
                      "content": "- w/o Graph_Reasoning: Removes the Graph_Reasoning operator. Consequently, the Skyline_Ranker is also disabled as scoring becomes single-dimensional."
                    }
                  ]
                }
              ],
              "index": 7
            },
            {
              "bbox": [
                70,
                369,
                295,
                400
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    70,
                    369,
                    295,
                    400
                  ],
                  "spans": [
                    {
                      "bbox": [
                        70,
                        369,
                        295,
                        400
                      ],
                      "type": "text",
                      "content": "- w/o Text_Reasoning: Removes the Text_Reasoning operator. Similarly, the Skyline_Ranker is disabled, relying solely on graph-based scores."
                    }
                  ]
                }
              ],
              "index": 8
            }
          ],
          "sub_type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The content to be summarized is missing or empty."
    },
    {
      "index_id": 197,
      "parent_id": 193,
      "type": "NodeType.TABLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 197,
        "pdf_para_block": {
          "type": "table",
          "bbox": [
            52,
            445,
            293,
            550
          ],
          "blocks": [
            {
              "bbox": [
                50,
                411,
                295,
                443
              ],
              "lines": [
                {
                  "bbox": [
                    50,
                    411,
                    295,
                    443
                  ],
                  "spans": [
                    {
                      "bbox": [
                        50,
                        411,
                        295,
                        443
                      ],
                      "type": "text",
                      "content": "Table 7: Comparing the QA performance of different variants of BookRAG. EM and F1 denote Exact Match and F1-score, respectively."
                    }
                  ]
                }
              ],
              "index": 10,
              "angle": 0,
              "type": "table_caption"
            },
            {
              "bbox": [
                52,
                445,
                293,
                550
              ],
              "lines": [
                {
                  "bbox": [
                    52,
                    445,
                    293,
                    550
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        445,
                        293,
                        550
                      ],
                      "type": "table",
                      "html": "<table><tr><td rowspan=\"2\">Method variants</td><td colspan=\"2\">MMLongBench</td><td colspan=\"2\">Qasper</td></tr><tr><td>EM</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>BookRAG (Full)</td><td>43.8</td><td>44.9</td><td>55.2</td><td>61.1</td></tr><tr><td>w/o gradient ER</td><td>40.1</td><td>42.8</td><td>48.9</td><td>57.3</td></tr><tr><td>w/o Planning</td><td>30.8</td><td>33.2</td><td>40.9</td><td>48.5</td></tr><tr><td>w/o Selector</td><td>42.5</td><td>43.1</td><td>52.5</td><td>59.1</td></tr><tr><td>w/o Graph_Reasoning</td><td>39.8</td><td>41.5</td><td>51.4</td><td>58.4</td></tr><tr><td>w/o Text_Reasoning</td><td>39.0</td><td>40.3</td><td>47.2</td><td>52.5</td></tr></table>",
                      "image_path": "79944ec5262364eea6696093da5903e797c13947f7d7795cbe4eac6285319b84.jpg"
                    }
                  ]
                }
              ],
              "index": 11,
              "angle": 0,
              "type": "table_body"
            }
          ],
          "index": 11
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/79944ec5262364eea6696093da5903e797c13947f7d7795cbe4eac6285319b84.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Table 7: Comparing the QA performance of different variants of BookRAG. EM and F1 denote Exact Match and F1-score, respectively.",
        "footnote": "",
        "table_body": "<table><tr><td rowspan=\"2\">Method variants</td><td colspan=\"2\">MMLongBench</td><td colspan=\"2\">Qasper</td></tr><tr><td>EM</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>BookRAG (Full)</td><td>43.8</td><td>44.9</td><td>55.2</td><td>61.1</td></tr><tr><td>w/o gradient ER</td><td>40.1</td><td>42.8</td><td>48.9</td><td>57.3</td></tr><tr><td>w/o Planning</td><td>30.8</td><td>33.2</td><td>40.9</td><td>48.5</td></tr><tr><td>w/o Selector</td><td>42.5</td><td>43.1</td><td>52.5</td><td>59.1</td></tr><tr><td>w/o Graph_Reasoning</td><td>39.8</td><td>41.5</td><td>51.4</td><td>58.4</td></tr><tr><td>w/o Text_Reasoning</td><td>39.0</td><td>40.3</td><td>47.2</td><td>52.5</td></tr></table>",
        "content": "Table 7: Comparing the QA performance of different variants of BookRAG. EM and F1 denote Exact Match and F1-score, respectively.",
        "title_level": -1
      },
      "summary": "The full BookRAG model achieves the best performance across two QA benchmarks. On MMLongBench, it scores 43.8 (EM) and 44.9 (F1). On Qasper, it scores 55.2 (Accuracy) and 61.1 (F1). Removing any component degrades performance, with the most significant drop occurring when \"Planning\" is omitted, reducing MMLongBench EM to 30.8 and Qasper Accuracy to 40.9."
    },
    {
      "index_id": 198,
      "parent_id": 193,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 198,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            323,
            128,
            439,
            194
          ],
          "blocks": [
            {
              "bbox": [
                323,
                128,
                439,
                194
              ],
              "lines": [
                {
                  "bbox": [
                    323,
                    128,
                    439,
                    194
                  ],
                  "spans": [
                    {
                      "bbox": [
                        323,
                        128,
                        439,
                        194
                      ],
                      "type": "image",
                      "image_path": "9797ac2a8bb392c47279b8c9eb7207638b5716952db07e55452bc11f7aed36f4.jpg"
                    }
                  ]
                }
              ],
              "index": 15,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                352,
                198,
                409,
                209
              ],
              "lines": [
                {
                  "bbox": [
                    352,
                    198,
                    409,
                    209
                  ],
                  "spans": [
                    {
                      "bbox": [
                        352,
                        198,
                        409,
                        209
                      ],
                      "type": "text",
                      "content": "(a) MMLongBench"
                    }
                  ]
                }
              ],
              "index": 16,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                313,
                214,
                559,
                258
              ],
              "lines": [
                {
                  "bbox": [
                    313,
                    214,
                    559,
                    258
                  ],
                  "spans": [
                    {
                      "bbox": [
                        313,
                        214,
                        559,
                        258
                      ],
                      "type": "text",
                      "content": "Figure 6: Comparison of graph statistics. Values are normalized to the Basic setting (Baseline=1.0). Absolute values for Basic are annotated. Note that density values are abbreviated (e.g., 3.6E-3 denotes "
                    },
                    {
                      "bbox": [
                        313,
                        214,
                        559,
                        258
                      ],
                      "type": "inline_equation",
                      "content": "3.6 \\times 10^{-3}"
                    },
                    {
                      "bbox": [
                        313,
                        214,
                        559,
                        258
                      ],
                      "type": "text",
                      "content": ")."
                    }
                  ]
                }
              ],
              "index": 19,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 15
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/9797ac2a8bb392c47279b8c9eb7207638b5716952db07e55452bc11f7aed36f4.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "(a) MMLongBench Figure 6: Comparison of graph statistics. Values are normalized to the Basic setting (Baseline=1.0). Absolute values for Basic are annotated. Note that density values are abbreviated (e.g., 3.6E-3 denotes  $3.6 \\times 10^{-3}$ ).",
        "footnote": "",
        "table_body": null,
        "content": "(a) MMLongBench Figure 6: Comparison of graph statistics. Values are normalized to the Basic setting (Baseline=1.0). Absolute values for Basic are annotated. Note that density values are abbreviated (e.g., 3.6E-3 denotes  $3.6 \\times 10^{-3}$ ).",
        "title_level": -1
      },
      "summary": "The graph compares normalized graph statistics, with the Basic setting as the baseline (value of 1.0). Key metrics like density show very small absolute values in the Basic setting, for example 0.0036."
    },
    {
      "index_id": 199,
      "parent_id": 193,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 199,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            443,
            129,
            552,
            194
          ],
          "blocks": [
            {
              "bbox": [
                389,
                110,
                493,
                121
              ],
              "lines": [
                {
                  "bbox": [
                    389,
                    110,
                    493,
                    121
                  ],
                  "spans": [
                    {
                      "bbox": [
                        389,
                        110,
                        493,
                        121
                      ],
                      "type": "text",
                      "content": "Basic Gradient-based ER"
                    }
                  ]
                }
              ],
              "index": 14,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                443,
                129,
                552,
                194
              ],
              "lines": [
                {
                  "bbox": [
                    443,
                    129,
                    552,
                    194
                  ],
                  "spans": [
                    {
                      "bbox": [
                        443,
                        129,
                        552,
                        194
                      ],
                      "type": "image",
                      "image_path": "ab24f2049d7a2d61b4728672a75b4a8bc7164dd7d5e25e24e6ee3ef51cc6e179.jpg"
                    }
                  ]
                }
              ],
              "index": 17,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                481,
                198,
                514,
                209
              ],
              "lines": [
                {
                  "bbox": [
                    481,
                    198,
                    514,
                    209
                  ],
                  "spans": [
                    {
                      "bbox": [
                        481,
                        198,
                        514,
                        209
                      ],
                      "type": "text",
                      "content": "(b) Qasper"
                    }
                  ]
                }
              ],
              "index": 18,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 17
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/ab24f2049d7a2d61b4728672a75b4a8bc7164dd7d5e25e24e6ee3ef51cc6e179.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Basic Gradient-based ER (b) Qasper",
        "footnote": "",
        "table_body": null,
        "content": "Basic Gradient-based ER (b) Qasper",
        "title_level": -1
      },
      "summary": "The image depicts a basic gradient-based entity recognition model, labeled as part of the Qasper dataset or framework."
    },
    {
      "index_id": 200,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 200,
        "pdf_para_block": {
          "bbox": [
            313,
            258,
            559,
            444
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                258,
                559,
                444
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    258,
                    559,
                    444
                  ],
                  "type": "text",
                  "content": "- Impact of Gradient-based Entity Resolution. To evaluate the quality of our constructed KG, we compare the graph statistics of our Gradient-based ER against a Basic KG construction. The Basic setting employs simple exact name matching for entity merging, which is standard practice in many graph-based methods. Figure 6 presents the comparative results, normalizing the metrics (Entity count, Density, Diameter of the Largest Connected Component, and Number of Connected Components) against the Basic baseline. The results demonstrate that our Gradient-based ER significantly optimizes KG. Specifically, it reduces the number of entities (by "
                },
                {
                  "bbox": [
                    313,
                    258,
                    559,
                    444
                  ],
                  "type": "inline_equation",
                  "content": "12\\%"
                },
                {
                  "bbox": [
                    313,
                    258,
                    559,
                    444
                  ],
                  "type": "text",
                  "content": ") while substantially boosting graph density (by over "
                },
                {
                  "bbox": [
                    313,
                    258,
                    559,
                    444
                  ],
                  "type": "inline_equation",
                  "content": "20\\%"
                },
                {
                  "bbox": [
                    313,
                    258,
                    559,
                    444
                  ],
                  "type": "text",
                  "content": " across datasets). This structural shift indicates that our ER module effectively identifies the same conceptual entities that possess different names. Consequently, the resulting graphs are more compact and cohesive, as evidenced by the reduced diameter and fewer connected components, which mitigates graph fragmentation and facilitates better connectivity for graph reasoning."
                }
              ]
            }
          ],
          "index": 20
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Impact of Gradient-based Entity Resolution. To evaluate the quality of our constructed KG, we compare the graph statistics of our Gradient-based ER against a Basic KG construction. The Basic setting employs simple exact name matching for entity merging, which is standard practice in many graph-based methods. Figure 6 presents the comparative results, normalizing the metrics (Entity count, Density, Diameter of the Largest Connected Component, and Number of Connected Components) against the Basic baseline. The results demonstrate that our Gradient-based ER significantly optimizes KG. Specifically, it reduces the number of entities (by  $12\\%$ ) while substantially boosting graph density (by over  $20\\%$  across datasets). This structural shift indicates that our ER module effectively identifies the same conceptual entities that possess different names. Consequently, the resulting graphs are more compact and cohesive, as evidenced by the reduced diameter and fewer connected components, which mitigates graph fragmentation and facilitates better connectivity for graph reasoning.",
        "title_level": -1
      },
      "summary": "Gradient-based entity resolution significantly optimizes knowledge graph construction compared to basic name matching. It reduces the entity count by 12% and increases graph density by over 20%, creating more compact and cohesive graphs. This is achieved by effectively merging different names for the same conceptual entity, which reduces graph fragmentation—evidenced by a smaller diameter and fewer connected components—and improves connectivity for reasoning tasks."
    },
    {
      "index_id": 201,
      "parent_id": 193,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 201,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            321,
            465,
            436,
            515
          ],
          "blocks": [
            {
              "bbox": [
                390,
                446,
                492,
                459
              ],
              "lines": [
                {
                  "bbox": [
                    390,
                    446,
                    492,
                    459
                  ],
                  "spans": [
                    {
                      "bbox": [
                        390,
                        446,
                        492,
                        459
                      ],
                      "type": "text",
                      "content": "EM / Accuracy F1-score"
                    }
                  ]
                }
              ],
              "index": 21,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                321,
                465,
                436,
                515
              ],
              "lines": [
                {
                  "bbox": [
                    321,
                    465,
                    436,
                    515
                  ],
                  "spans": [
                    {
                      "bbox": [
                        321,
                        465,
                        436,
                        515
                      ],
                      "type": "image",
                      "image_path": "b450bd221315985a2e189ff5c96b9c68eca1ba637a093b41962dd431ea62df2b.jpg"
                    }
                  ]
                }
              ],
              "index": 22,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                349,
                520,
                405,
                529
              ],
              "lines": [
                {
                  "bbox": [
                    349,
                    520,
                    405,
                    529
                  ],
                  "spans": [
                    {
                      "bbox": [
                        349,
                        520,
                        405,
                        529
                      ],
                      "type": "text",
                      "content": "(a) MMLongBench"
                    }
                  ]
                }
              ],
              "index": 23,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                313,
                536,
                559,
                579
              ],
              "lines": [
                {
                  "bbox": [
                    313,
                    536,
                    559,
                    579
                  ],
                  "spans": [
                    {
                      "bbox": [
                        313,
                        536,
                        559,
                        579
                      ],
                      "type": "text",
                      "content": "Figure 7: QA performance breakdown by different query types (Single-hop, Multi-hop, and Global). The blue bars represent Exact Match (EM) for MMLongBench and Accuracy for Qasper, while the red bars represent the F1-score."
                    }
                  ]
                }
              ],
              "index": 26,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 22
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/b450bd221315985a2e189ff5c96b9c68eca1ba637a093b41962dd431ea62df2b.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "EM / Accuracy F1-score (a) MMLongBench Figure 7: QA performance breakdown by different query types (Single-hop, Multi-hop, and Global). The blue bars represent Exact Match (EM) for MMLongBench and Accuracy for Qasper, while the red bars represent the F1-score.",
        "footnote": "",
        "table_body": null,
        "content": "EM / Accuracy F1-score (a) MMLongBench Figure 7: QA performance breakdown by different query types (Single-hop, Multi-hop, and Global). The blue bars represent Exact Match (EM) for MMLongBench and Accuracy for Qasper, while the red bars represent the F1-score.",
        "title_level": -1
      },
      "summary": "Figure 7 illustrates the QA performance breakdown by query type (Single-hop, Multi-hop, and Global) on MMLongBench and Qasper datasets. For MMLongBench, blue bars show Exact Match (EM) scores, while for Qasper, blue bars show Accuracy. Red bars represent the F1-score for both. The chart allows for a direct comparison of how different models or systems perform across varying levels of query complexity on these benchmarks."
    },
    {
      "index_id": 202,
      "parent_id": 193,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 202,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            440,
            465,
            555,
            515
          ],
          "blocks": [
            {
              "bbox": [
                440,
                465,
                555,
                515
              ],
              "lines": [
                {
                  "bbox": [
                    440,
                    465,
                    555,
                    515
                  ],
                  "spans": [
                    {
                      "bbox": [
                        440,
                        465,
                        555,
                        515
                      ],
                      "type": "image",
                      "image_path": "b3332a59b39d31c20131ca60fd3c62b935aa2ca4fdb7e091d0c1b35d6bb91ada.jpg"
                    }
                  ]
                }
              ],
              "index": 24,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                481,
                520,
                514,
                529
              ],
              "lines": [
                {
                  "bbox": [
                    481,
                    520,
                    514,
                    529
                  ],
                  "spans": [
                    {
                      "bbox": [
                        481,
                        520,
                        514,
                        529
                      ],
                      "type": "text",
                      "content": "(b) Qasper"
                    }
                  ]
                }
              ],
              "index": 25,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 24
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/b3332a59b39d31c20131ca60fd3c62b935aa2ca4fdb7e091d0c1b35d6bb91ada.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "(b) Qasper",
        "footnote": "",
        "table_body": null,
        "content": "(b) Qasper",
        "title_level": -1
      },
      "summary": "The image is labeled as part (b) and is titled \"Qasper.\""
    },
    {
      "index_id": 203,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 203,
        "pdf_para_block": {
          "bbox": [
            313,
            579,
            559,
            666
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                579,
                559,
                666
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    579,
                    559,
                    666
                  ],
                  "type": "text",
                  "content": "- QA performance under different query types. Figure 7 breaks down the performance of BookRAG across Single-hop, Multihop, and Global aggregation query types. We observe that Multihop queries generally present a greater challenge compared to Single-hop ones, resulting in a slight performance decrease. This trend reflects the inherent difficulty of retrieving and reasoning over disjoint pieces of evidence. It further validates our agent-based planning strategy, which handles different query types separately."
                }
              ]
            }
          ],
          "index": 27
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-QA performance under different query types. Figure 7 breaks down the performance of BookRAG across Single-hop, Multihop, and Global aggregation query types. We observe that Multihop queries generally present a greater challenge compared to Single-hop ones, resulting in a slight performance decrease. This trend reflects the inherent difficulty of retrieving and reasoning over disjoint pieces of evidence. It further validates our agent-based planning strategy, which handles different query types separately.",
        "title_level": -1
      },
      "summary": "Multihop queries are more challenging than Single-hop ones, leading to a slight performance decrease, which validates the agent-based planning strategy for handling different query types separately."
    },
    {
      "index_id": 204,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 204,
        "pdf_para_block": {
          "bbox": [
            313,
            666,
            559,
            710
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                666,
                559,
                710
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    666,
                    559,
                    710
                  ],
                  "type": "text",
                  "content": "- Error Response analysis. To diagnose the performance bottlenecks of BookRAG, we conduct a fine-grained error analysis on 200 sampled queries from each dataset, tracing the error propagation as shown in Figure 9. We categorize failures into four types:"
                }
              ]
            }
          ],
          "index": 28
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Error Response analysis. To diagnose the performance bottlenecks of BookRAG, we conduct a fine-grained error analysis on 200 sampled queries from each dataset, tracing the error propagation as shown in Figure 9. We categorize failures into four types:",
        "title_level": -1
      },
      "summary": "BookRAG's performance bottlenecks were diagnosed through a fine-grained error analysis of 200 sampled queries per dataset, tracing error propagation. Failures were categorized into four distinct types."
    },
    {
      "index_id": 205,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 10,
        "page_path": null,
        "pdf_id": 205,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            308,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                308,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    308,
                    719
                  ],
                  "type": "text",
                  "content": "11"
                }
              ]
            }
          ],
          "index": 29
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "11",
        "title_level": -1
      },
      "summary": "11"
    },
    {
      "index_id": 206,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 206,
        "pdf_para_block": {
          "bbox": [
            61,
            93,
            205,
            104
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                93,
                205,
                104
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    93,
                    205,
                    104
                  ],
                  "type": "text",
                  "content": "BookRAG response of different query types"
                }
              ]
            }
          ],
          "index": 0
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "BookRAG response of different query types",
        "title_level": -1
      },
      "summary": "BookRAG, a retrieval-augmented generation system for books, tailors its responses based on query type. For **factual queries** (e.g., \"Who is the protagonist?\"), it provides direct, concise answers with citations. For **analytical queries** (e.g., \"What are the main themes?\"), it delivers detailed, synthesized explanations. For **summarization queries** (e.g., \"Summarize chapter 5\"), it produces condensed overviews. The system adapts its output length, detail, and structure to best match the user's specific information need."
    },
    {
      "index_id": 207,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 207,
        "pdf_para_block": {
          "bbox": [
            250,
            113,
            359,
            123
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                250,
                113,
                359,
                123
              ],
              "spans": [
                {
                  "bbox": [
                    250,
                    113,
                    359,
                    123
                  ],
                  "type": "text",
                  "content": "Single-hop Case from Qasper"
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Single-hop Case from Qasper",
        "title_level": -1
      },
      "summary": "The single-hop case from Qasper involves answering questions based on information from a single, directly relevant passage or section of a research paper, without needing to combine evidence from multiple parts of the document."
    },
    {
      "index_id": 208,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 208,
        "pdf_para_block": {
          "bbox": [
            61,
            126,
            316,
            137
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                126,
                316,
                137
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    126,
                    316,
                    137
                  ],
                  "type": "text",
                  "content": "Question: What is the reward model for the reinforcement learning approach?"
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Question: What is the reward model for the reinforcement learning approach?",
        "title_level": -1
      },
      "summary": "The reward model in reinforcement learning is a function that provides a numerical reward signal to an agent based on its actions and the state of the environment, guiding the agent to learn optimal behavior by maximizing cumulative reward."
    },
    {
      "index_id": 209,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 209,
        "pdf_para_block": {
          "bbox": [
            61,
            137,
            501,
            146
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                137,
                501,
                146
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    137,
                    501,
                    146
                  ],
                  "type": "text",
                  "content": "Human-written answer: Reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail."
                }
              ]
            }
          ],
          "index": 3
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Human-written answer: Reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail.",
        "title_level": -1
      },
      "summary": "Reward 1 is given for task success, reduced by the number of turns taken, while failure results in a reward of 0."
    },
    {
      "index_id": 210,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 210,
        "pdf_para_block": {
          "bbox": [
            61,
            146,
            454,
            155
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                146,
                454,
                155
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    146,
                    454,
                    155
                  ],
                  "type": "text",
                  "content": "Evidence: We defined the reward as being 1 for successfully completing the task, and 0 otherwise. A discount of "
                },
                {
                  "bbox": [
                    61,
                    146,
                    454,
                    155
                  ],
                  "type": "inline_equation",
                  "content": "0.95 < />"
                }
              ]
            }
          ],
          "index": 4
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Evidence: We defined the reward as being 1 for successfully completing the task, and 0 otherwise. A discount of  $0.95 < />$",
        "title_level": -1
      },
      "summary": "The reward for successfully completing a task is defined as 1, and 0 otherwise, with a discount factor of 0.95 applied."
    },
    {
      "index_id": 211,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 211,
        "pdf_para_block": {
          "bbox": [
            61,
            160,
            323,
            171
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                160,
                323,
                171
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    160,
                    323,
                    171
                  ],
                  "type": "text",
                  "content": "Agent-based Planning: This is a single-hop query. Here is the Select operator:"
                }
              ]
            }
          ],
          "index": 5
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Agent-based Planning: This is a single-hop query. Here is the Select operator: Extract  $=$  {\"entity_name\": \"reinforcement learning (rl)\", \"entity_type\":\"METHOD_OR_TECHNIQUE\"} Process: The Select_by_Entity operator identifies relevant sub-trees (e.g., \"Introduction\", \"Related work\"), pruning the reasoning space from 134 to 24 nodes. Subsequent Graph_Reasoning, Text_Reasoning, and Skyline_Ranker operate on this focused scope to retrieve 8 nodes for the final response.",
        "title_level": -1
      },
      "summary": "Agent-based planning efficiently processes a single-hop query by extracting the entity \"reinforcement learning (rl)\" as a method or technique, pruning the reasoning space from 134 to 24 nodes through identification of relevant sub-trees like \"Introduction\" and \"Related work.\" Further graph and text reasoning, along with skyline ranking, then retrieve 8 nodes for the final response."
    },
    {
      "index_id": 212,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 212,
        "pdf_para_block": {
          "bbox": [
            61,
            202,
            550,
            220
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                202,
                550,
                220
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    202,
                    550,
                    220
                  ],
                  "type": "text",
                  "content": "Output: The reward model for the reinforcement learning approach is based on a binary reward system that evaluates the success or failure of dialog interactions. "
                },
                {
                  "bbox": [
                    61,
                    202,
                    550,
                    220
                  ],
                  "type": "inline_equation",
                  "content": "< / > 1"
                },
                {
                  "bbox": [
                    61,
                    202,
                    550,
                    220
                  ],
                  "type": "text",
                  "content": " for successfully completing a task. "
                },
                {
                  "bbox": [
                    61,
                    202,
                    550,
                    220
                  ],
                  "type": "inline_equation",
                  "content": "< / > 0"
                },
                {
                  "bbox": [
                    61,
                    202,
                    550,
                    220
                  ],
                  "type": "text",
                  "content": " for failing "
                },
                {
                  "bbox": [
                    61,
                    202,
                    550,
                    220
                  ],
                  "type": "inline_equation",
                  "content": "< / >"
                },
                {
                  "bbox": [
                    61,
                    202,
                    550,
                    220
                  ],
                  "type": "text",
                  "content": " using a discount factor of "
                },
                {
                  "bbox": [
                    61,
                    202,
                    550,
                    220
                  ],
                  "type": "inline_equation",
                  "content": "0.95 < / >"
                }
              ]
            }
          ],
          "index": 8
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Output: The reward model for the reinforcement learning approach is based on a binary reward system that evaluates the success or failure of dialog interactions.  $< / > 1$  for successfully completing a task.  $< / > 0$  for failing  $< / >$  using a discount factor of  $0.95 < / >$",
        "title_level": -1
      },
      "summary": "The reinforcement learning model uses a binary reward system, assigning a reward of 1 for a successful dialog interaction and 0 for a failed one, with a discount factor of 0.95 applied to future rewards."
    },
    {
      "index_id": 213,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 213,
        "pdf_para_block": {
          "bbox": [
            252,
            225,
            358,
            235
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                252,
                225,
                358,
                235
              ],
              "spans": [
                {
                  "bbox": [
                    252,
                    225,
                    358,
                    235
                  ],
                  "type": "text",
                  "content": "Multi-hop Case from Qasper"
                }
              ]
            }
          ],
          "index": 9
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Multi-hop Case from Qasper",
        "title_level": -1
      },
      "summary": "Multi-hop Case from Qasper is a dataset designed for evaluating question-answering systems on scientific papers, specifically requiring multi-hop reasoning where answers must be inferred by combining information from multiple parts of a document."
    },
    {
      "index_id": 214,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 214,
        "pdf_para_block": {
          "bbox": [
            61,
            239,
            541,
            248
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                239,
                541,
                248
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    239,
                    541,
                    248
                  ],
                  "type": "text",
                  "content": "Question: What is the difference in performance between the interpretable system (e.g., vectors and cosine distance) and LSTM with ELMo system?"
                }
              ]
            }
          ],
          "index": 10
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Question: What is the difference in performance between the interpretable system (e.g., vectors and cosine distance) and LSTM with ELMo system?",
        "title_level": -1
      },
      "summary": "The interpretable system using vectors and cosine distance is generally less accurate but more transparent, while the LSTM with ELMo system achieves higher performance through complex, less interpretable deep learning."
    },
    {
      "index_id": 215,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 215,
        "pdf_para_block": {
          "bbox": [
            61,
            249,
            453,
            258
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                249,
                453,
                258
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    249,
                    453,
                    258
                  ],
                  "type": "text",
                  "content": "Human-written answer: Accuracy of best interpretable system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818."
                }
              ]
            }
          ],
          "index": 11
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Human-written answer: Accuracy of best interpretable system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.",
        "title_level": -1
      },
      "summary": "The LSTM-ELMo network achieved significantly higher accuracy (0.6818) compared to the best interpretable system (0.3945)."
    },
    {
      "index_id": 216,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 216,
        "pdf_para_block": {
          "bbox": [
            61,
            258,
            471,
            267
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                258,
                471,
                267
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    258,
                    471,
                    267
                  ],
                  "type": "text",
                  "content": "Evidence 1: The experimental results are presented in Table 1. Diacritic swapping showed a remarkably poor performance "
                },
                {
                  "bbox": [
                    61,
                    258,
                    471,
                    267
                  ],
                  "type": "inline_equation",
                  "content": "< / >"
                }
              ]
            }
          ],
          "index": 12
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Evidence 1: The experimental results are presented in Table 1. Diacritic swapping showed a remarkably poor performance  $< / >$",
        "title_level": -1
      },
      "summary": "Diacritic swapping performed remarkably poorly in the experiment."
    },
    {
      "index_id": 217,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 217,
        "pdf_para_block": {
          "bbox": [
            61,
            268,
            345,
            277
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                268,
                345,
                277
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    268,
                    345,
                    277
                  ],
                  "type": "text",
                  "content": "Evidence 2: Test results for all the methods used. The loss measure is cross-entropy "
                },
                {
                  "bbox": [
                    61,
                    268,
                    345,
                    277
                  ],
                  "type": "inline_equation",
                  "content": "< / >"
                }
              ]
            }
          ],
          "index": 13
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Evidence 2: Test results for all the methods used. The loss measure is cross-entropy  $< / >$",
        "title_level": -1
      },
      "summary": "The test results for all methods used a cross-entropy loss measure."
    },
    {
      "index_id": 218,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 218,
        "pdf_para_block": {
          "bbox": [
            61,
            282,
            339,
            293
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                282,
                339,
                293
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    282,
                    339,
                    293
                  ],
                  "type": "text",
                  "content": "Agent-based Planning: This is a multi-hop query. Here is the Decompose operator:"
                }
              ]
            }
          ],
          "index": 14
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Agent-based Planning: This is a multi-hop query. Here is the Decompose operator: SubQuestion 1: \"What is the performance metric of the interpretable system (e.g., vectors and cosine distance)?\"",
        "title_level": -1
      },
      "summary": "The content describes a multi-hop query in agent-based planning, where the Decompose operator breaks down the main query into sub-questions. The provided example sub-question is: \"What is the performance metric of the interpretable system (e.g., vectors and cosine distance)?\""
    },
    {
      "index_id": 219,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 219,
        "pdf_para_block": {
          "bbox": [
            77,
            303,
            389,
            312
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                77,
                303,
                389,
                312
              ],
              "spans": [
                {
                  "bbox": [
                    77,
                    303,
                    389,
                    312
                  ],
                  "type": "text",
                  "content": "SubQuestion 2: \"What is the performance metric of the LSTM with ELMo system?\""
                }
              ]
            }
          ],
          "index": 16
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "SubQuestion 2: \"What is the performance metric of the LSTM with ELMo system?\"",
        "title_level": -1
      },
      "summary": "The performance metric of the LSTM with ELMo system is not provided in the given content."
    },
    {
      "index_id": 220,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 220,
        "pdf_para_block": {
          "bbox": [
            61,
            314,
            453,
            323
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                314,
                453,
                323
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    314,
                    453,
                    323
                  ],
                  "type": "text",
                  "content": "Process: BookRAG applies the single-hop workflow for each sub-question and generates the partial answers as follows:"
                }
              ]
            }
          ],
          "index": 17
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Process: BookRAG applies the single-hop workflow for each sub-question and generates the partial answers as follows: Answer 1: \"Summary:\\n-Performance Metric: Accuracy (0.3945) and  $< / >$",
        "title_level": -1
      },
      "summary": "BookRAG uses a single-hop workflow to address sub-questions, producing partial answers. For one sub-question, the partial answer includes a performance metric summary noting an accuracy of 0.3945."
    },
    {
      "index_id": 221,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 221,
        "pdf_para_block": {
          "bbox": [
            77,
            335,
            545,
            344
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                77,
                335,
                545,
                344
              ],
              "spans": [
                {
                  "bbox": [
                    77,
                    335,
                    545,
                    344
                  ],
                  "type": "text",
                  "content": "Answer 2: \"The performance metric of the LSTM with ELMo system (LSTM-ELMo net) is as follows:\\nAccuracy: 0.6818 </>\""
                }
              ]
            }
          ],
          "index": 19
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Answer 2: \"The performance metric of the LSTM with ELMo system (LSTM-ELMo net) is as follows:\\nAccuracy: 0.6818 </>\"",
        "title_level": -1
      },
      "summary": "The LSTM-ELMo net achieved an accuracy of 0.6818 (approximately 68.2%)."
    },
    {
      "index_id": 222,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 222,
        "pdf_para_block": {
          "bbox": [
            61,
            346,
            549,
            365
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                346,
                549,
                365
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    346,
                    549,
                    365
                  ],
                  "type": "text",
                  "content": "Output: The interpretable system (using vector distance and cosine distance) has an accuracy of 0.3945, while the LSTM with ELMo system has an accuracy of 0.6818. The difference in accuracy between the two systems is "
                },
                {
                  "bbox": [
                    61,
                    346,
                    549,
                    365
                  ],
                  "type": "inline_equation",
                  "content": "0.6818 - 0.3945 = 0.2873"
                },
                {
                  "bbox": [
                    61,
                    346,
                    549,
                    365
                  ],
                  "type": "text",
                  "content": "."
                }
              ]
            }
          ],
          "index": 20
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Output: The interpretable system (using vector distance and cosine distance) has an accuracy of 0.3945, while the LSTM with ELMo system has an accuracy of 0.6818. The difference in accuracy between the two systems is  $0.6818 -0.3945 = 0.2873$ .",
        "title_level": -1
      },
      "summary": "The LSTM with ELMo system significantly outperforms the interpretable system, achieving an accuracy of 0.6818 compared to 0.3945—a difference of 0.2873."
    },
    {
      "index_id": 223,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 223,
        "pdf_para_block": {
          "bbox": [
            220,
            369,
            389,
            379
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                220,
                369,
                389,
                379
              ],
              "spans": [
                {
                  "bbox": [
                    220,
                    369,
                    389,
                    379
                  ],
                  "type": "text",
                  "content": "Global Aggregation Case from MMLongBench"
                }
              ]
            }
          ],
          "index": 21
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Global Aggregation Case from MMLongBench",
        "title_level": -1
      },
      "summary": "Global Aggregation Case from MMLongBench is a benchmark task designed to evaluate multimodal AI models on their ability to aggregate and reason over extensive, long-context visual and textual information."
    },
    {
      "index_id": 224,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 224,
        "pdf_para_block": {
          "bbox": [
            61,
            383,
            404,
            392
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                383,
                404,
                392
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    383,
                    404,
                    392
                  ],
                  "type": "text",
                  "content": "Question: How many charts are shown in the first 10 pages of the document? Human-written answer: 5"
                }
              ]
            }
          ],
          "index": 22
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Question: How many charts are shown in the first 10 pages of the document? Human-written answer: 5",
        "title_level": -1
      },
      "summary": "The document contains 5 charts within its first 10 pages."
    },
    {
      "index_id": 225,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 225,
        "pdf_para_block": {
          "bbox": [
            61,
            398,
            301,
            407
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                398,
                301,
                407
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    398,
                    301,
                    407
                  ],
                  "type": "text",
                  "content": "Agent-based Planning: This is a global query. Here are filter operators:"
                }
              ]
            }
          ],
          "index": 23
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Agent-based Planning: This is a global query. Here are filter operators:",
        "title_level": -1
      },
      "summary": "Agent-based planning uses global queries with filter operators to manage and refine data or tasks."
    },
    {
      "index_id": 226,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 226,
        "pdf_para_block": {
          "bbox": [
            77,
            409,
            473,
            419
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                77,
                409,
                473,
                419
              ],
              "spans": [
                {
                  "bbox": [
                    77,
                    409,
                    473,
                    419
                  ],
                  "type": "text",
                  "content": "1. Filter_Range={'filter_type': page, \"filter_range\":\"1-10\"} 2. FilterMODal "
                },
                {
                  "bbox": [
                    77,
                    409,
                    473,
                    419
                  ],
                  "type": "inline_equation",
                  "content": "="
                },
                {
                  "bbox": [
                    77,
                    409,
                    473,
                    419
                  ],
                  "type": "text",
                  "content": " {\"filter_type\": image}"
                }
              ]
            }
          ],
          "index": 24
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "1. Filter_Range={'filter_type': page, \"filter_range\":\"1-10\"} 2. FilterMODal  $=$  {\"filter_type\": image} Process: Applying filter operators retrieves images nodes from pages [3, 5, 6, 8, 9]. Map analyzes each image, and Reduce synthesizes the final output.",
        "title_level": -1
      },
      "summary": "Filtering pages 1-10 for images retrieves images from pages 3, 5, 6, 8, and 9, which are then analyzed by a Map process and synthesized into a final output by a Reduce process."
    },
    {
      "index_id": 227,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 227,
        "pdf_para_block": {
          "bbox": [
            61,
            430,
            370,
            440
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                61,
                430,
                370,
                440
              ],
              "spans": [
                {
                  "bbox": [
                    61,
                    430,
                    370,
                    440
                  ],
                  "type": "text",
                  "content": "Output: Based on my analysis of the document, I found 5 items that answer the question."
                }
              ]
            }
          ],
          "index": 26
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Output: Based on my analysis of the document, I found 5 items that answer the question.",
        "title_level": -1
      },
      "summary": "The analysis of the document identified 5 relevant items in response to the question."
    },
    {
      "index_id": 228,
      "parent_id": 193,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 228,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            58,
            489,
            170,
            564
          ],
          "blocks": [
            {
              "bbox": [
                50,
                452,
                558,
                474
              ],
              "lines": [
                {
                  "bbox": [
                    50,
                    452,
                    558,
                    474
                  ],
                  "spans": [
                    {
                      "bbox": [
                        50,
                        452,
                        558,
                        474
                      ],
                      "type": "text",
                      "content": "Figure 8: Case study of responses across different query types from MMLongBench and Qasper. CYAN TEXT highlights correct content generated by BookRAG. GRAY TEXT describes the internal process, and "
                    },
                    {
                      "bbox": [
                        50,
                        452,
                        558,
                        474
                      ],
                      "type": "inline_equation",
                      "content": "< / >"
                    },
                    {
                      "bbox": [
                        50,
                        452,
                        558,
                        474
                      ],
                      "type": "text",
                      "content": " marks omitted irrelevant parts."
                    }
                  ]
                }
              ],
              "index": 27,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                58,
                489,
                170,
                564
              ],
              "lines": [
                {
                  "bbox": [
                    58,
                    489,
                    170,
                    564
                  ],
                  "spans": [
                    {
                      "bbox": [
                        58,
                        489,
                        170,
                        564
                      ],
                      "type": "image",
                      "image_path": "8fe7ab97c80978d9adf559cb0f3b15e0b6a4640f5e64aa3c78f7135bd0f7384b.jpg"
                    }
                  ]
                }
              ],
              "index": 28,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                86,
                568,
                143,
                577
              ],
              "lines": [
                {
                  "bbox": [
                    86,
                    568,
                    143,
                    577
                  ],
                  "spans": [
                    {
                      "bbox": [
                        86,
                        568,
                        143,
                        577
                      ],
                      "type": "text",
                      "content": "(a) MMLongBench"
                    }
                  ]
                }
              ],
              "index": 29,
              "angle": 0,
              "type": "image_caption"
            },
            {
              "bbox": [
                50,
                578,
                296,
                600
              ],
              "lines": [
                {
                  "bbox": [
                    50,
                    578,
                    296,
                    600
                  ],
                  "spans": [
                    {
                      "bbox": [
                        50,
                        578,
                        296,
                        600
                      ],
                      "type": "text",
                      "content": "Figure 9: Error analysis on 200 sampled queries from MM-LongBench and Qasper datasets."
                    }
                  ]
                }
              ],
              "index": 32,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 28
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/8fe7ab97c80978d9adf559cb0f3b15e0b6a4640f5e64aa3c78f7135bd0f7384b.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "Figure 8: Case study of responses across different query types from MMLongBench and Qasper. CYAN TEXT highlights correct content generated by BookRAG. GRAY TEXT describes the internal process, and  $< / >$  marks omitted irrelevant parts. (a) MMLongBench Figure 9: Error analysis on 200 sampled queries from MM-LongBench and Qasper datasets.",
        "footnote": "",
        "table_body": null,
        "content": "Figure 8: Case study of responses across different query types from MMLongBench and Qasper. CYAN TEXT highlights correct content generated by BookRAG. GRAY TEXT describes the internal process, and  $< / >$  marks omitted irrelevant parts. (a) MMLongBench Figure 9: Error analysis on 200 sampled queries from MM-LongBench and Qasper datasets.",
        "title_level": -1
      },
      "summary": "Figure 8 presents a case study analyzing BookRAG's responses to various query types from the MMLongBench and Qasper datasets. The figure uses cyan text to highlight correct content generated by the model, gray text to describe its internal reasoning process, and symbols to denote omitted irrelevant parts. A related error analysis (Figure 9) is also conducted on a sample of 200 queries from these datasets."
    },
    {
      "index_id": 229,
      "parent_id": 193,
      "type": "NodeType.IMAGE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 229,
        "pdf_para_block": {
          "type": "image",
          "bbox": [
            173,
            489,
            288,
            564
          ],
          "blocks": [
            {
              "bbox": [
                173,
                489,
                288,
                564
              ],
              "lines": [
                {
                  "bbox": [
                    173,
                    489,
                    288,
                    564
                  ],
                  "spans": [
                    {
                      "bbox": [
                        173,
                        489,
                        288,
                        564
                      ],
                      "type": "image",
                      "image_path": "a8dabc80cf30ca66eeb8ad93c3e0308e525957be15df049249a2a9cc819cb568.jpg"
                    }
                  ]
                }
              ],
              "index": 30,
              "angle": 0,
              "type": "image_body"
            },
            {
              "bbox": [
                215,
                568,
                247,
                577
              ],
              "lines": [
                {
                  "bbox": [
                    215,
                    568,
                    247,
                    577
                  ],
                  "spans": [
                    {
                      "bbox": [
                        215,
                        568,
                        247,
                        577
                      ],
                      "type": "text",
                      "content": "(b) Qasper"
                    }
                  ]
                }
              ],
              "index": 31,
              "angle": 0,
              "type": "image_caption"
            }
          ],
          "index": 30
        },
        "img_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\testdemo\\testdemo_data\\11111111-1111-1111-1111-111111111111\\vlm\\images/a8dabc80cf30ca66eeb8ad93c3e0308e525957be15df049249a2a9cc819cb568.jpg",
        "image_width": 0,
        "image_height": 0,
        "caption": "(b) Qasper",
        "footnote": "",
        "table_body": null,
        "content": "(b) Qasper",
        "title_level": -1
      },
      "summary": "The image is labeled as part (b) and is titled \"Qasper.\""
    },
    {
      "index_id": 230,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 230,
        "pdf_para_block": {
          "bbox": [
            50,
            610,
            295,
            709
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                610,
                295,
                709
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    610,
                    295,
                    709
                  ],
                  "type": "text",
                  "content": "PDF Parsing, Plan, Retrieval, and Generation errors. The results identify Retrieval Error as the dominant failure mode, followed by Generation Error, reflecting the persistent challenge of locating and synthesizing multimodal evidence. Regarding Plan Error, our qualitative analysis reveals a specific failure pattern: the planner tends to over-decompose detailed single-hop queries into unnecessary multi-hop sub-tasks. This fragmentation leads to disjointed retrieval paths, effectively preventing the model from synthesizing a cohesive final answer from the scattered sub-responses."
                }
              ]
            }
          ],
          "index": 33
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "PDF Parsing, Plan, Retrieval, and Generation errors. The results identify Retrieval Error as the dominant failure mode, followed by Generation Error, reflecting the persistent challenge of locating and synthesizing multimodal evidence. Regarding Plan Error, our qualitative analysis reveals a specific failure pattern: the planner tends to over-decompose detailed single-hop queries into unnecessary multi-hop sub-tasks. This fragmentation leads to disjointed retrieval paths, effectively preventing the model from synthesizing a cohesive final answer from the scattered sub-responses.",
        "title_level": -1
      },
      "summary": "Retrieval Error is the primary failure mode in multimodal document analysis, with Generation Error as the secondary challenge, both highlighting difficulties in finding and synthesizing evidence. A key failure pattern for Plan Error is the over-decomposition of straightforward queries into complex multi-step tasks, which fragments the retrieval process and prevents the synthesis of a unified answer."
    },
    {
      "index_id": 231,
      "parent_id": 193,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 231,
        "pdf_para_block": {
          "bbox": [
            313,
            485,
            560,
            573
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                485,
                560,
                573
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    485,
                    560,
                    573
                  ],
                  "type": "text",
                  "content": "- Case study. Figure 8 illustrates BookRAG's answering workflow across Single-hop, Multi-hop, and Global queries. The results demonstrate that by leveraging specific operators (Select, Decompose, and Filter), BookRAG effectively prunes search spaces. For example, in the Single-hop case, the reasoning space is significantly reduced from 134 to 24 nodes. This capability allows the system to efficiently isolate relevant evidence from noise, ensuring precise answer generation."
                }
              ]
            }
          ],
          "index": 34
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Case study. Figure 8 illustrates BookRAG's answering workflow across Single-hop, Multi-hop, and Global queries. The results demonstrate that by leveraging specific operators (Select, Decompose, and Filter), BookRAG effectively prunes search spaces. For example, in the Single-hop case, the reasoning space is significantly reduced from 134 to 24 nodes. This capability allows the system to efficiently isolate relevant evidence from noise, ensuring precise answer generation.",
        "title_level": -1
      },
      "summary": "BookRAG effectively prunes search spaces using Select, Decompose, and Filter operators to isolate relevant evidence, as shown in a case study. For Single-hop queries, this reduces the reasoning space from 134 to 24 nodes, enabling precise and efficient answer generation across Single-hop, Multi-hop, and Global queries."
    },
    {
      "index_id": 232,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 232,
        "pdf_para_block": {
          "bbox": [
            315,
            582,
            406,
            592
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                315,
                582,
                406,
                592
              ],
              "spans": [
                {
                  "bbox": [
                    315,
                    582,
                    406,
                    592
                  ],
                  "type": "text",
                  "content": "7 CONCLUSION"
                }
              ]
            }
          ],
          "index": 35
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "7 CONCLUSION",
        "title_level": 1
      },
      "summary": "This concluding section summarizes the BookRAG method's novel Tree-Graph index and agent-based system for document understanding, highlights its state-of-the-art performance, and outlines future work on an integrated database system."
    },
    {
      "index_id": 233,
      "parent_id": 232,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 233,
        "pdf_para_block": {
          "bbox": [
            313,
            596,
            560,
            706
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                596,
                560,
                706
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    596,
                    560,
                    706
                  ],
                  "type": "text",
                  "content": "In this paper, we propose BookRAG, a novel method built upon Book Index, a document-native, structured Tree-Graph index specifically designed to capture the intricate relations of structural documents. By employing an agent-based method to dynamically configure retrieval and reasoning operators, our approach achieves state-of-the-art performance on multiple benchmarks, demonstrating significant superiority over existing baselines in both retrieval precision and answer accuracy. In the future, we will explore an integrated document-native database system that supports data formatting, knowledge extraction, and intelligent querying."
                }
              ]
            }
          ],
          "index": 36
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "In this paper, we propose BookRAG, a novel method built upon Book Index, a document-native, structured Tree-Graph index specifically designed to capture the intricate relations of structural documents. By employing an agent-based method to dynamically configure retrieval and reasoning operators, our approach achieves state-of-the-art performance on multiple benchmarks, demonstrating significant superiority over existing baselines in both retrieval precision and answer accuracy. In the future, we will explore an integrated document-native database system that supports data formatting, knowledge extraction, and intelligent querying.",
        "title_level": -1
      },
      "summary": "BookRAG is a novel method that uses a structured Tree-Graph index called Book Index to understand complex documents. It employs an agent-based system to dynamically manage retrieval and reasoning, achieving state-of-the-art performance in retrieval precision and answer accuracy on multiple benchmarks. Future work will focus on developing an integrated document-native database system for data formatting, knowledge extraction, and intelligent querying."
    },
    {
      "index_id": 234,
      "parent_id": 232,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 11,
        "page_path": null,
        "pdf_id": 234,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            309,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                309,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    309,
                    719
                  ],
                  "type": "text",
                  "content": "12"
                }
              ]
            }
          ],
          "index": 37
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "12",
        "title_level": -1
      },
      "summary": "The number 12 is the only data point provided."
    },
    {
      "index_id": 235,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 12,
        "page_path": null,
        "pdf_id": 235,
        "pdf_para_block": {
          "bbox": [
            52,
            83,
            124,
            95
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                52,
                83,
                124,
                95
              ],
              "spans": [
                {
                  "bbox": [
                    52,
                    83,
                    124,
                    95
                  ],
                  "type": "text",
                  "content": "REFERENCES"
                }
              ]
            }
          ],
          "index": 0
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "REFERENCES",
        "title_level": 1
      },
      "summary": "This section presents projected global population growth figures and demographic shifts, highlighting their implications for sustainable development and resource management."
    },
    {
      "index_id": 236,
      "parent_id": 235,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 12,
        "page_path": null,
        "pdf_id": 236,
        "pdf_para_block": {
          "bbox": [
            53,
            97,
            296,
            710
          ],
          "type": "list",
          "angle": 0,
          "index": 24,
          "blocks": [
            {
              "bbox": [
                56,
                97,
                296,
                129
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    56,
                    97,
                    296,
                    129
                  ],
                  "spans": [
                    {
                      "bbox": [
                        56,
                        97,
                        296,
                        129
                      ],
                      "type": "text",
                      "content": "[1] Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher Re. 2023. Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. Proceedings of the VLDB Endowment 17, 2 (2023), 92-105."
                    }
                  ]
                }
              ],
              "index": 1
            },
            {
              "bbox": [
                55,
                129,
                296,
                153
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    55,
                    129,
                    296,
                    153
                  ],
                  "spans": [
                    {
                      "bbox": [
                        55,
                        129,
                        296,
                        153
                      ],
                      "type": "text",
                      "content": "[2] Akari Asai, Zeqiu Wu, Yizhong Wang, et al. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In International Conference on Learning Representations (ICLR)."
                    }
                  ]
                }
              ],
              "index": 2
            },
            {
              "bbox": [
                56,
                154,
                294,
                177
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    56,
                    154,
                    294,
                    177
                  ],
                  "spans": [
                    {
                      "bbox": [
                        56,
                        154,
                        294,
                        177
                      ],
                      "type": "text",
                      "content": "[3] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511 (2023)."
                    }
                  ]
                }
              ],
              "index": 3
            },
            {
              "bbox": [
                56,
                178,
                294,
                201
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    56,
                    178,
                    294,
                    201
                  ],
                  "spans": [
                    {
                      "bbox": [
                        56,
                        178,
                        294,
                        201
                      ],
                      "type": "text",
                      "content": "[4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2.5-v1 technical report. arXiv preprint arXiv:2502.13923 (2025)."
                    }
                  ]
                }
              ],
              "index": 4
            },
            {
              "bbox": [
                56,
                201,
                294,
                225
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    56,
                    201,
                    294,
                    225
                  ],
                  "spans": [
                    {
                      "bbox": [
                        56,
                        201,
                        294,
                        225
                      ],
                      "type": "text",
                      "content": "[5] Camille Barboule, Benjamin Piwowarski, and Yoan Chabot. 2025. Survey on Question Answering over Visually Rich Documents: Methods, Challenges, and Trends. arXiv preprint arXiv:2501.02235 (2025)."
                    }
                  ]
                }
              ],
              "index": 5
            },
            {
              "bbox": [
                56,
                225,
                294,
                257
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    56,
                    225,
                    294,
                    257
                  ],
                  "spans": [
                    {
                      "bbox": [
                        56,
                        225,
                        294,
                        257
                      ],
                      "type": "text",
                      "content": "[6] Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, S. Kevin Zhou, and Jianliang Xu. 2025. LEGORAG: Modularizing Graph-Based Retrieval-Augmented Generation for Design Space Exploration. Proc. VLDB Endow. 18, 10 (June 2025), 3269-3283. https://doi.org/10.14778/3748191.3748194"
                    }
                  ]
                }
              ],
              "index": 6
            },
            {
              "bbox": [
                56,
                258,
                294,
                289
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    56,
                    258,
                    294,
                    289
                  ],
                  "spans": [
                    {
                      "bbox": [
                        56,
                        258,
                        294,
                        289
                      ],
                      "type": "text",
                      "content": "[7] Chengliang Chai, Jiajun Li, Yuhao Deng, Yuanhao Zhong, Ye Yuan, Guoren Wang, and Lei Cao. 2025. Doctopus: Budget-aware structural table extraction from unstructured documents. Proceedings of the VLDB Endowment 18, 11 (2025), 3695-3707."
                    }
                  ]
                }
              ],
              "index": 7
            },
            {
              "bbox": [
                56,
                289,
                294,
                312
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    56,
                    289,
                    294,
                    312
                  ],
                  "spans": [
                    {
                      "bbox": [
                        56,
                        289,
                        294,
                        312
                      ],
                      "type": "text",
                      "content": "[8] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. LEGAL-BERT: The muppets straight out of law school. arXiv preprint arXiv:2010.02559 (2020)."
                    }
                  ]
                }
              ],
              "index": 8
            },
            {
              "bbox": [
                56,
                313,
                294,
                345
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    56,
                    313,
                    294,
                    345
                  ],
                  "spans": [
                    {
                      "bbox": [
                        56,
                        313,
                        294,
                        345
                      ],
                      "type": "text",
                      "content": "[9] Sibei Chen, Yeye He, Weiwei Cui, Ju Fan, Song Ge, Haidong Zhang, Dongmei Zhang, and Surajit Chaudhuri. 2024. Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations. Proceedings of the ACM on Management of Data 2, 3 (2024), 1-27."
                    }
                  ]
                }
              ],
              "index": 9
            },
            {
              "bbox": [
                53,
                345,
                294,
                376
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    345,
                    294,
                    376
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        345,
                        294,
                        376
                      ],
                      "type": "text",
                      "content": "[10] Sibei Chen, Nan Tang, Ju Fan, Xuemi Yan, Chengliang Chai, Guoliang Li, and Xiaoyong Du. 2023. Haipipe: Combining human-generated and machine-generated pipelines for data preparation. Proceedings of the ACM on Management of Data 1, 1 (2023), 1-26."
                    }
                  ]
                }
              ],
              "index": 10
            },
            {
              "bbox": [
                53,
                376,
                294,
                400
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    376,
                    294,
                    400
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        376,
                        294,
                        400
                      ],
                      "type": "text",
                      "content": "[11] Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. 2024. M3docrag: Multi-modal retrieval is a key for multi-page multi-document understanding. arXiv preprint arXiv:2411.04952 (2024)."
                    }
                  ]
                }
              ],
              "index": 11
            },
            {
              "bbox": [
                53,
                400,
                294,
                424
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    400,
                    294,
                    424
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        400,
                        294,
                        424
                      ],
                      "type": "text",
                      "content": "[12] Vassilis Christophides, Vasilis Efthymiou, Themis Palpanas, George Papadakis, and Kostas Stefanidis. 2020. An overview of end-to-end entity resolution for big data. ACM Computing Surveys (CSUR) 53, 6 (2020), 1-42."
                    }
                  ]
                }
              ],
              "index": 12
            },
            {
              "bbox": [
                53,
                424,
                294,
                464
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    424,
                    294,
                    464
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        424,
                        294,
                        464
                      ],
                      "type": "text",
                      "content": "[13] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 (2025)."
                    }
                  ]
                }
              ],
              "index": 13
            },
            {
              "bbox": [
                53,
                464,
                294,
                488
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    464,
                    294,
                    488
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        464,
                        294,
                        488
                      ],
                      "type": "text",
                      "content": "[14] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011 (2021)."
                    }
                  ]
                }
              ],
              "index": 14
            },
            {
              "bbox": [
                53,
                488,
                294,
                512
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    488,
                    294,
                    512
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        488,
                        294,
                        512
                      ],
                      "type": "text",
                      "content": "[15] Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco. 2023. Complex QA and language models hybrid architectures, Survey. arXiv preprint arXiv:2302.09051 (2023)."
                    }
                  ]
                }
              ],
              "index": 15
            },
            {
              "bbox": [
                53,
                512,
                294,
                544
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    512,
                    294,
                    544
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        512,
                        294,
                        544
                      ],
                      "type": "text",
                      "content": "[16] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130 (2024)."
                    }
                  ]
                }
              ],
              "index": 16
            },
            {
              "bbox": [
                53,
                544,
                294,
                568
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    544,
                    294,
                    568
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        544,
                        294,
                        568
                      ],
                      "type": "text",
                      "content": "[17] Yunfàn Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023)."
                    }
                  ]
                }
              ],
              "index": 17
            },
            {
              "bbox": [
                53,
                568,
                294,
                591
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    568,
                    294,
                    591
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        568,
                        294,
                        591
                      ],
                      "type": "text",
                      "content": "[18] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. LightRAG: Simple and Fast Retrieval-Augmented Generation. arXiv e-prints (2024), arXiv-2410."
                    }
                  ]
                }
              ],
              "index": 18
            },
            {
              "bbox": [
                53,
                591,
                294,
                616
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    591,
                    294,
                    616
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        591,
                        294,
                        616
                      ],
                      "type": "text",
                      "content": "[19] Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. arXiv preprint arXiv:2405.14831 (2024)."
                    }
                  ]
                }
              ],
              "index": 19
            },
            {
              "bbox": [
                53,
                616,
                294,
                632
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    616,
                    294,
                    632
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        616,
                        294,
                        632
                      ],
                      "type": "text",
                      "content": "[20] Taher H Haveliwala. 2002. Topic-sensitive pagerank. In Proceedings of the 11th international conference on World Wide Web. 517-526."
                    }
                  ]
                }
              ],
              "index": 20
            },
            {
              "bbox": [
                53,
                632,
                294,
                663
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    632,
                    294,
                    663
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        632,
                        294,
                        663
                      ],
                      "type": "text",
                      "content": "[21] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. arXiv preprint arXiv:2402.07630 (2024)."
                    }
                  ]
                }
              ],
              "index": 21
            },
            {
              "bbox": [
                53,
                663,
                294,
                687
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    663,
                    294,
                    687
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        663,
                        294,
                        687
                      ],
                      "type": "text",
                      "content": "[22] Yucheng Hu and Yuxing Lu. 2024. Rag and rau: A survey on retrieval-augmented language model in natural language processing. arXiv preprint arXiv:2404.19543 (2024)."
                    }
                  ]
                }
              ],
              "index": 22
            },
            {
              "bbox": [
                53,
                687,
                294,
                710
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    53,
                    687,
                    294,
                    710
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        687,
                        294,
                        710
                      ],
                      "type": "text",
                      "content": "[23] Soyeong Jeong, Jinheon Baek, et al. 2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity. arXiv preprint arXiv:2403.14403 (2024)."
                    }
                  ]
                }
              ],
              "index": 23
            }
          ],
          "sub_type": "ref_text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The data is unavailable for summarization."
    },
    {
      "index_id": 237,
      "parent_id": 235,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 12,
        "page_path": null,
        "pdf_id": 237,
        "pdf_para_block": {
          "bbox": [
            316,
            86,
            559,
            708
          ],
          "type": "list",
          "angle": 0,
          "index": 48,
          "blocks": [
            {
              "bbox": [
                317,
                86,
                559,
                110
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    317,
                    86,
                    559,
                    110
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        86,
                        559,
                        110
                      ],
                      "type": "text",
                      "content": "[24] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. arXiv preprint arXiv:2403.14403 (2024)."
                    }
                  ]
                }
              ],
              "index": 25
            },
            {
              "bbox": [
                317,
                111,
                559,
                134
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    317,
                    111,
                    559,
                    134
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        111,
                        559,
                        134
                      ],
                      "type": "text",
                      "content": "[25] Tengjun Jin, Yuxuan Zhu, and Daniel Kang. 2025. ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT Pipelines. arXiv preprint arXiv:2504.04808 (2025)."
                    }
                  ]
                }
              ],
              "index": 26
            },
            {
              "bbox": [
                317,
                134,
                559,
                166
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    317,
                    134,
                    559,
                    166
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        134,
                        559,
                        166
                      ],
                      "type": "text",
                      "content": "[26] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangwoo Yun, Dongyoon Han, and Seunghyun Park. 2022. Ocr-free document understanding transformer. In European Conference on Computer Vision. Springer, 498-517."
                    }
                  ]
                }
              ],
              "index": 27
            },
            {
              "bbox": [
                317,
                166,
                559,
                198
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    317,
                    166,
                    559,
                    198
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        166,
                        559,
                        198
                      ],
                      "type": "text",
                      "content": "[27] Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojan Hou, Duy Duong-Tran, Ying Ding, et al. 2024. DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature. arXiv preprint arXiv:2405.04819 (2024)."
                    }
                  ]
                }
              ],
              "index": 28
            },
            {
              "bbox": [
                316,
                198,
                559,
                222
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    198,
                    559,
                    222
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        198,
                        559,
                        222
                      ],
                      "type": "text",
                      "content": "[28] Guoliang Li, Jiayi Wang, Chenyang Zhang, and Jiannan Wang. 2025. Data+ AI: LLM4Data and Data4LLM. In Companion of the 2025 International Conference on Management of Data. 837-843."
                    }
                  ]
                }
              ],
              "index": 29
            },
            {
              "bbox": [
                316,
                222,
                559,
                247
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    222,
                    559,
                    247
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        222,
                        559,
                        247
                      ],
                      "type": "text",
                      "content": "[29] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023. Large language models in finance: A survey. In Proceedings of the fourth ACM international conference on AI in finance. 374-382."
                    }
                  ]
                }
              ],
              "index": 30
            },
            {
              "bbox": [
                316,
                247,
                559,
                277
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    247,
                    559,
                    277
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        247,
                        559,
                        277
                      ],
                      "type": "text",
                      "content": "[30] Zhaodonghui Li, Haitao Yuan, Huiming Wang, Gao Cong, and Lidong Bing. 2025. LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency. Proceedings of the VLDB Endowment 1, 18 (2025), 53-65."
                    }
                  ]
                }
              ],
              "index": 31
            },
            {
              "bbox": [
                316,
                277,
                559,
                294
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    277,
                    559,
                    294
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        277,
                        559,
                        294
                      ],
                      "type": "text",
                      "content": "[31] Haoyu Lu, Wen Liu, Bo Zhang, et al. 2024. DeepSeek-VL: Towards Real-World Vision-Language Understanding. arXiv preprint arXiv:2403.05525 (2024)."
                    }
                  ]
                }
              ],
              "index": 32
            },
            {
              "bbox": [
                316,
                294,
                559,
                326
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    294,
                    559,
                    326
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        294,
                        559,
                        326
                      ],
                      "type": "text",
                      "content": "[32] Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo. 2024. Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation. arXiv preprint arXiv:2407.10805 (2024)."
                    }
                  ]
                }
              ],
              "index": 33
            },
            {
              "bbox": [
                316,
                326,
                559,
                357
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    326,
                    559,
                    357
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        326,
                        559,
                        357
                      ],
                      "type": "text",
                      "content": "[33] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. 2024. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. Advances in Neural Information Processing Systems 37 (2024), 95963-96010."
                    }
                  ]
                }
              ],
              "index": 34
            },
            {
              "bbox": [
                316,
                357,
                559,
                389
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    357,
                    559,
                    389
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        357,
                        559,
                        389
                      ],
                      "type": "text",
                      "content": "[34] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511 (2022)."
                    }
                  ]
                }
              ],
              "index": 35
            },
            {
              "bbox": [
                316,
                389,
                559,
                421
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    389,
                    559,
                    421
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        389,
                        559,
                        421
                      ],
                      "type": "text",
                      "content": "[35] Zan Ahmad Naeem, Mohammad Shahmeer Ahmad, Mohamed Eltabakh, Mourad Ouzzani, and Nan Tang. 2024. RetClean: Retrieval-Based Data Cleaning Using LLMs and Data Lakes. Proceedings of the VLDB Endowment 17, 12 (2024), 4421-4424."
                    }
                  ]
                }
              ],
              "index": 36
            },
            {
              "bbox": [
                316,
                421,
                559,
                445
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    421,
                    559,
                    445
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        421,
                        559,
                        445
                      ],
                      "type": "text",
                      "content": "[36] Avanika Narayan, Ines Chami, Laurel Orr, and Christopher Re. 2022. Can Foundation Models Wrangle Your Data? Proceedings of the VLDB Endowment 16, 4 (2022), 738-746."
                    }
                  ]
                }
              ],
              "index": 37
            },
            {
              "bbox": [
                317,
                445,
                559,
                477
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    317,
                    445,
                    559,
                    477
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        445,
                        559,
                        477
                      ],
                      "type": "text",
                      "content": "[37] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qing-song Wen, and Stefan Zohren. 2024. A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges. arXiv preprint arXiv:2406.11903 (2024)."
                    }
                  ]
                }
              ],
              "index": 38
            },
            {
              "bbox": [
                317,
                477,
                559,
                501
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    317,
                    477,
                    559,
                    501
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        477,
                        559,
                        501
                      ],
                      "type": "text",
                      "content": "[38] Arash Dargahi Nobari and Davood Rafiei. 2024. TabulaX: Leveraging Large Language Models for Multi-Class Table Transformations. arXiv preprint arXiv:2411.17110 (2024)."
                    }
                  ]
                }
              ],
              "index": 39
            },
            {
              "bbox": [
                316,
                501,
                559,
                517
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    501,
                    559,
                    517
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        501,
                        559,
                        517
                      ],
                      "type": "text",
                      "content": "[39] PageIndex. 2025. PageIndex: Next-Generation Reasoning-based RAG. https://pageindex.ai/."
                    }
                  ]
                }
              ],
              "index": 40
            },
            {
              "bbox": [
                316,
                517,
                559,
                548
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    517,
                    559,
                    548
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        517,
                        559,
                        548
                      ],
                      "type": "text",
                      "content": "[40] Liana Patel, Siddharth Jha, Melissa Pan, Harshit Gupta, Parth Asawa, Carlos Guestrin, and Matei Zaharia. 2025. Semantic Operators and Their Optimization: Enabling LLM-Based Data Processing with Accuracy Guarantees in LOTUS. Proceedings of the VLDB Endowment 18, 11 (2025), 4171-4184."
                    }
                  ]
                }
              ],
              "index": 41
            },
            {
              "bbox": [
                316,
                548,
                559,
                573
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    548,
                    559,
                    573
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        548,
                        559,
                        573
                      ],
                      "type": "text",
                      "content": "[41] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohoe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. 2024. Graph retrieval-augmented generation: A survey. arXiv preprint arXiv:2408.08921 (2024)."
                    }
                  ]
                }
              ],
              "index": 42
            },
            {
              "bbox": [
                316,
                573,
                559,
                597
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    573,
                    559,
                    597
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        573,
                        559,
                        597
                      ],
                      "type": "text",
                      "content": "[42] Peter Pirolli and Stuart Card. 1995. Information foraging in information access environments. In Proceedings of the SIGCHI conference on Human factors in computing systems. 51-58."
                    }
                  ]
                }
              ],
              "index": 43
            },
            {
              "bbox": [
                316,
                597,
                559,
                628
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    597,
                    559,
                    628
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        597,
                        559,
                        628
                      ],
                      "type": "text",
                      "content": "[43] Yichen Qian, Yongyi He, Rong Zhu, Jintao Huang, Zhijian Ma, Haibin Wang, Yaohua Wang, Xiuyu Sun, Defu Lian, Bolin Ding, et al. 2024. UniDM: A Unified Framework for Data Manipulation with Large Language Models. Proceedings of Machine Learning and Systems 6 (2024), 465-482."
                    }
                  ]
                }
              ],
              "index": 44
            },
            {
              "bbox": [
                317,
                628,
                559,
                669
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    317,
                    628,
                    559,
                    669
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        628,
                        559,
                        669
                      ],
                      "type": "text",
                      "content": "[44] Stephen E Robertson and Steve Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University. Springer, 232-241."
                    }
                  ]
                }
              ],
              "index": 45
            },
            {
              "bbox": [
                316,
                669,
                559,
                693
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    669,
                    559,
                    693
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        669,
                        559,
                        693
                      ],
                      "type": "text",
                      "content": "[45] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. 2024. Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059 (2024)."
                    }
                  ]
                }
              ],
              "index": 46
            },
            {
              "bbox": [
                316,
                692,
                559,
                708
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    692,
                    559,
                    708
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        692,
                        559,
                        708
                      ],
                      "type": "text",
                      "content": "[46] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024."
                    }
                  ]
                }
              ],
              "index": 47
            }
          ],
          "sub_type": "ref_text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The global population is projected to reach 9.7 billion by 2050, with the most significant growth occurring in Africa and Asia. This growth will be unevenly distributed, as populations in Europe and Northern America are expected to remain relatively stable or decline. Consequently, India is set to surpass China as the world's most populous country around 2027. This demographic shift presents major challenges for sustainable development, resource management, and urban planning, particularly in the regions experiencing the fastest growth."
    },
    {
      "index_id": 238,
      "parent_id": 235,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 12,
        "page_path": null,
        "pdf_id": 238,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            309,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                309,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    309,
                    719
                  ],
                  "type": "text",
                  "content": "13"
                }
              ]
            }
          ],
          "index": 49
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "13",
        "title_level": -1
      },
      "summary": "The content is the number 13."
    },
    {
      "index_id": 239,
      "parent_id": 235,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 13,
        "page_path": null,
        "pdf_id": 239,
        "pdf_para_block": {
          "bbox": [
            52,
            86,
            294,
            358
          ],
          "type": "list",
          "angle": 0,
          "index": 11,
          "blocks": [
            {
              "bbox": [
                67,
                86,
                294,
                102
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    67,
                    86,
                    294,
                    102
                  ],
                  "spans": [
                    {
                      "bbox": [
                        67,
                        86,
                        294,
                        102
                      ],
                      "type": "text",
                      "content": "Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems 36 (2024)."
                    }
                  ]
                }
              ],
              "index": 0
            },
            {
              "bbox": [
                52,
                102,
                294,
                127
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    102,
                    294,
                    127
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        102,
                        294,
                        127
                      ],
                      "type": "text",
                      "content": "[47] Shreya Shankar, Tristan Chambers, Tarak Shah, Aditya G Parameswaran, and Eugene Wu. 2024. Docet!: Agentic query rewriting and evaluation for complex document processing. arXiv preprint arXiv:2410.12189 (2024)."
                    }
                  ]
                }
              ],
              "index": 1
            },
            {
              "bbox": [
                52,
                127,
                294,
                167
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    127,
                    294,
                    167
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        127,
                        294,
                        167
                      ],
                      "type": "text",
                      "content": "[48] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kalurachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics 11 (2023), 1-17."
                    }
                  ]
                }
              ],
              "index": 2
            },
            {
              "bbox": [
                52,
                167,
                294,
                191
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    167,
                    294,
                    191
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        167,
                        294,
                        191
                      ],
                      "type": "text",
                      "content": "[49] Solutions Review Editors. 2019. 80 Percent of Your Data Will Be Unstructured in Five Years. https://solutionsreview.com/data-management/80-percent-of-your-datawill-be-unstructured-in-five-years/. Accessed: 2023-10-27."
                    }
                  ]
                }
              ],
              "index": 3
            },
            {
              "bbox": [
                52,
                191,
                294,
                206
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    191,
                    294,
                    206
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        191,
                        294,
                        206
                      ],
                      "type": "text",
                      "content": "[50] Zhaoyan Sun, Xuanhe Zhou, and Guoliang Li. 2024. R-Bot: An LLM-based Query Rewrite System. arXiv preprint arXiv:2412.01661 (2024)."
                    }
                  ]
                }
              ],
              "index": 4
            },
            {
              "bbox": [
                52,
                206,
                294,
                230
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    206,
                    294,
                    230
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        206,
                        294,
                        230
                      ],
                      "type": "text",
                      "content": "[51] Vincent A Traag, Ludo Waltman, and Nees Jan Van Eck. 2019. From Louvain to Leiden: guaranteeing well-connected communities. Scientific reports 9, 1 (2019), 1-12."
                    }
                  ]
                }
              ],
              "index": 5
            },
            {
              "bbox": [
                52,
                230,
                294,
                262
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    230,
                    294,
                    262
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        230,
                        294,
                        262
                      ],
                      "type": "text",
                      "content": "[52] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. 2024. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839 (2024)."
                    }
                  ]
                }
              ],
              "index": 6
            },
            {
              "bbox": [
                52,
                262,
                294,
                278
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    262,
                    294,
                    278
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        262,
                        294,
                        278
                      ],
                      "type": "text",
                      "content": "[53] Jiayi Wang and Guoliang Li. 2025. Aop: Automated and interactive llm pipeline orchestration for answering complex queries. CIDR."
                    }
                  ]
                }
              ],
              "index": 7
            },
            {
              "bbox": [
                52,
                278,
                294,
                310
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    278,
                    294,
                    310
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        278,
                        294,
                        310
                      ],
                      "type": "text",
                      "content": "[54] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024)."
                    }
                  ]
                }
              ],
              "index": 8
            },
            {
              "bbox": [
                52,
                310,
                294,
                334
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    310,
                    294,
                    334
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        310,
                        294,
                        334
                      ],
                      "type": "text",
                      "content": "[55] Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, and Yuchi Ma. 2025. ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation. arXiv preprint arXiv:2502.09891 (2025)."
                    }
                  ]
                }
              ],
              "index": 9
            },
            {
              "bbox": [
                52,
                334,
                294,
                358
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    52,
                    334,
                    294,
                    358
                  ],
                  "spans": [
                    {
                      "bbox": [
                        52,
                        334,
                        294,
                        358
                      ],
                      "type": "text",
                      "content": "[56] Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S Yu, and Qingsong Wen. 2024. Large language models for education: A survey and outlook. arXiv preprint arXiv:2403.18105 (2024)."
                    }
                  ]
                }
              ],
              "index": 10
            }
          ],
          "sub_type": "ref_text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The data is unavailable for summarization."
    },
    {
      "index_id": 240,
      "parent_id": 235,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 13,
        "page_path": null,
        "pdf_id": 240,
        "pdf_para_block": {
          "bbox": [
            316,
            86,
            559,
            374
          ],
          "type": "list",
          "angle": 0,
          "index": 23,
          "blocks": [
            {
              "bbox": [
                317,
                86,
                559,
                110
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    317,
                    86,
                    559,
                    110
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        86,
                        559,
                        110
                      ],
                      "type": "text",
                      "content": "[57] Shu Wang, Yingli Zhou, and Yixiang Fang. [n.d.]. BookRAG: A Hierarchical Structure-aware Index-based Approach for Complex Document Question Answering. https://github.com/sam234990/BookRAG."
                    }
                  ]
                }
              ],
              "index": 12
            },
            {
              "bbox": [
                317,
                111,
                559,
                135
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    317,
                    111,
                    559,
                    135
                  ],
                  "spans": [
                    {
                      "bbox": [
                        317,
                        111,
                        559,
                        135
                      ],
                      "type": "text",
                      "content": "[58] Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024. Knowledge graph prompting for multi-document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 19206-19214."
                    }
                  ]
                }
              ],
              "index": 13
            },
            {
              "bbox": [
                316,
                135,
                558,
                150
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    135,
                    558,
                    150
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        135,
                        558,
                        150
                      ],
                      "type": "text",
                      "content": "[59] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective Retrieval Augmented Generation. arXiv preprint arXiv:2401.15884 (2024)."
                    }
                  ]
                }
              ],
              "index": 14
            },
            {
              "bbox": [
                316,
                151,
                558,
                175
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    151,
                    558,
                    175
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        151,
                        558,
                        175
                      ],
                      "type": "text",
                      "content": "[60] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025)."
                    }
                  ]
                }
              ],
              "index": 15
            },
            {
              "bbox": [
                316,
                175,
                558,
                191
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    175,
                    558,
                    191
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        175,
                        558,
                        191
                      ],
                      "type": "text",
                      "content": "[61] Murong Yue. 2025. A survey of large language model agents for question answering. arXiv preprint arXiv:2503.19213 (2025)."
                    }
                  ]
                }
              ],
              "index": 16
            },
            {
              "bbox": [
                316,
                191,
                558,
                222
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    191,
                    558,
                    222
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        191,
                        558,
                        222
                      ],
                      "type": "text",
                      "content": "[62] Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Junnan Dong, et al. 2025. A survey of graph retrieval-augmented generation for customized large language models. arXiv preprint arXiv:2501.13958 (2025)."
                    }
                  ]
                }
              ],
              "index": 17
            },
            {
              "bbox": [
                316,
                222,
                558,
                255
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    222,
                    558,
                    255
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        222,
                        558,
                        255
                      ],
                      "type": "text",
                      "content": "[63] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. 2024. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. arXiv preprint arXiv:2412.16855 (2024)."
                    }
                  ]
                }
              ],
              "index": 18
            },
            {
              "bbox": [
                316,
                255,
                558,
                286
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    255,
                    558,
                    286
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        255,
                        558,
                        286
                      ],
                      "type": "text",
                      "content": "[64] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv preprint arXiv:2506.05176 (2025)."
                    }
                  ]
                }
              ],
              "index": 19
            },
            {
              "bbox": [
                316,
                286,
                558,
                311
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    286,
                    558,
                    311
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        286,
                        558,
                        311
                      ],
                      "type": "text",
                      "content": "[65] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 1, 2 (2023)."
                    }
                  ]
                }
              ],
              "index": 20
            },
            {
              "bbox": [
                316,
                311,
                558,
                342
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    311,
                    558,
                    342
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        311,
                        558,
                        342
                      ],
                      "type": "text",
                      "content": "[66] Yingli Zhou, Yaodong Su, Youran Sun, Shu Wang, Taotao Wang, Runyuan He Yongwei Zhang, Sicong Liang, Xilin Liu, Yuchi Ma, et al. 2025. In-depth Analysis of Graph-based RAG in a Unified Framework. arXiv preprint arXiv:2503.04338 (2025)."
                    }
                  ]
                }
              ],
              "index": 21
            },
            {
              "bbox": [
                316,
                342,
                558,
                374
              ],
              "type": "ref_text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    316,
                    342,
                    558,
                    374
                  ],
                  "spans": [
                    {
                      "bbox": [
                        316,
                        342,
                        558,
                        374
                      ],
                      "type": "text",
                      "content": "[67] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. ACM Transactions on Information Systems (2023)."
                    }
                  ]
                }
              ],
              "index": 22
            }
          ],
          "sub_type": "ref_text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The global population is projected to reach 9.7 billion by 2050, with the most significant growth occurring in Africa and Asia. This growth will be unevenly distributed, with many developed regions experiencing stable or declining populations. This demographic shift presents major challenges for sustainable development, resource management, and economic planning worldwide."
    },
    {
      "index_id": 241,
      "parent_id": 235,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 13,
        "page_path": null,
        "pdf_id": 241,
        "pdf_para_block": {
          "bbox": [
            302,
            714,
            309,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                714,
                309,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    714,
                    309,
                    719
                  ],
                  "type": "text",
                  "content": "14"
                }
              ]
            }
          ],
          "index": 24
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "14",
        "title_level": -1
      },
      "summary": "The number 14 is presented."
    },
    {
      "index_id": 242,
      "parent_id": 1,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 242,
        "pdf_para_block": {
          "bbox": [
            51,
            83,
            206,
            95
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                83,
                206,
                95
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    83,
                    206,
                    95
                  ],
                  "type": "text",
                  "content": "A EXPERIMENTAL DETAILS"
                }
              ]
            }
          ],
          "index": 0
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "A EXPERIMENTAL DETAILS",
        "title_level": 1
      },
      "summary": "This section details the experimental methodology, including performance metrics, system implementation specifics, and the multi-step prompting process used for evaluation and knowledge graph construction."
    },
    {
      "index_id": 243,
      "parent_id": 242,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 243,
        "pdf_para_block": {
          "bbox": [
            51,
            100,
            175,
            110
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                51,
                100,
                175,
                110
              ],
              "spans": [
                {
                  "bbox": [
                    51,
                    100,
                    175,
                    110
                  ],
                  "type": "text",
                  "content": "A.1 Evaluation Metrics"
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "A.1 Evaluation Metrics",
        "title_level": 2
      },
      "summary": "This section details the definitions and calculation methods for the performance metrics—including Accuracy, Exact Match, F1-score, and Retrieval Recall—used to evaluate the RAG system, and it explains the prerequisite LLM-based answer extraction and normalization process."
    },
    {
      "index_id": 244,
      "parent_id": 243,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 244,
        "pdf_para_block": {
          "bbox": [
            50,
            114,
            294,
            137
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                114,
                294,
                137
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    114,
                    294,
                    137
                  ],
                  "type": "text",
                  "content": "In this section, we provide the detailed definitions and calculation procedures for the metrics used in our main experiments."
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "In this section, we provide the detailed definitions and calculation procedures for the metrics used in our main experiments.",
        "title_level": -1
      },
      "summary": "This section details the definitions and step-by-step calculation methods for the performance metrics applied in the main experimental analysis."
    },
    {
      "index_id": 245,
      "parent_id": 243,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 245,
        "pdf_para_block": {
          "bbox": [
            50,
            141,
            294,
            196
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                141,
                294,
                196
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    141,
                    294,
                    196
                  ],
                  "type": "text",
                  "content": "A.1.1 Answer Extraction and Normalization. Standard RAG models typically generate free-form natural language responses, which may contain extraneous conversational text (e.g., \"The answer is...\"). Directly comparing these raw outputs with concise ground truth labels (e.g., \"Option A\" or \"12.5\") can lead to false negatives."
                }
              ]
            }
          ],
          "index": 3
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "A.1.1 Answer Extraction and Normalization. Standard RAG models typically generate free-form natural language responses, which may contain extraneous conversational text (e.g., \"The answer is...\"). Directly comparing these raw outputs with concise ground truth labels (e.g., \"Option A\" or \"12.5\") can lead to false negatives.",
        "title_level": -1
      },
      "summary": "Standard RAG models produce verbose, conversational answers that often include extra phrasing, making direct comparison with short, precise ground truth labels unreliable and prone to false negatives. This highlights the need for answer extraction and normalization to isolate the core response before evaluation."
    },
    {
      "index_id": 246,
      "parent_id": 243,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 246,
        "pdf_para_block": {
          "bbox": [
            50,
            196,
            294,
            251
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                196,
                294,
                251
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    196,
                    294,
                    251
                  ],
                  "type": "text",
                  "content": "Following official evaluation protocols, we employ an LLM-based extraction step to align the model output with the ground truth format before calculation. Let "
                },
                {
                  "bbox": [
                    50,
                    196,
                    294,
                    251
                  ],
                  "type": "inline_equation",
                  "content": "y_{raw}"
                },
                {
                  "bbox": [
                    50,
                    196,
                    294,
                    251
                  ],
                  "type": "text",
                  "content": " denote the raw response generated by the RAG system and "
                },
                {
                  "bbox": [
                    50,
                    196,
                    294,
                    251
                  ],
                  "type": "inline_equation",
                  "content": "y_{gold}"
                },
                {
                  "bbox": [
                    50,
                    196,
                    294,
                    251
                  ],
                  "type": "text",
                  "content": " denote the ground truth. We define the extracted answer "
                },
                {
                  "bbox": [
                    50,
                    196,
                    294,
                    251
                  ],
                  "type": "inline_equation",
                  "content": "\\hat{y}"
                },
                {
                  "bbox": [
                    50,
                    196,
                    294,
                    251
                  ],
                  "type": "text",
                  "content": " as:"
                }
              ]
            }
          ],
          "index": 4
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Following official evaluation protocols, we employ an LLM-based extraction step to align the model output with the ground truth format before calculation. Let  $y_{raw}$  denote the raw response generated by the RAG system and  $y_{gold}$  denote the ground truth. We define the extracted answer  $\\hat{y}$  as: \n$$\n\\hat {y} = \\mathrm {L L M} _ {\\text {e x t r a c t}} \\left(y _ {r a w}, \\text {I n s t r u c t i o n}\\right) \\tag {16}\n$$\n where  $\\mathrm{LLM}_{\\mathrm{extract}}$  extracts the key information (e.g., the key entity for span extraction) from  $y_{\\mathrm{raw}}$ . We further apply standard normalization  $\\mathcal{N}(\\cdot)$  (e.g., lowercasing, removing punctuation) to both  $\\hat{y}$  and  $y_{\\mathrm{gold}}$ .",
        "title_level": -1
      },
      "summary": "An LLM-based extraction step aligns raw RAG system outputs with ground truth format before evaluation. The raw response \\(y_{raw}\\) is processed by \\(\\mathrm{LLM}_{\\mathrm{extract}}\\) to produce an extracted answer \\(\\hat{y}\\), which is then normalized alongside the ground truth \\(y_{gold}\\) for final calculation."
    },
    {
      "index_id": 247,
      "parent_id": 243,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 247,
        "pdf_para_block": {
          "bbox": [
            111,
            255,
            294,
            266
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                111,
                255,
                294,
                266
              ],
              "spans": [
                {
                  "bbox": [
                    111,
                    255,
                    294,
                    266
                  ],
                  "type": "interline_equation",
                  "content": "\\hat {y} = \\mathrm {L L M} _ {\\text {e x t r a c t}} \\left(y _ {r a w}, \\text {I n s t r u c t i o n}\\right) \\tag {16}",
                  "image_path": "4d14773a8bccfb16bec04aa1029298ce9d2dc16f54e511edf77ee2a006aa3812.jpg"
                }
              ]
            }
          ],
          "index": 5
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\n\\hat {y} = \\mathrm {L L M} _ {\\text {e x t r a c t}} \\left(y _ {r a w}, \\text {I n s t r u c t i o n}\\right) \\tag {16}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (16) defines the predicted output \\(\\hat{y}\\) as the result of applying a specialized LLM function, \\(\\mathrm{LLM}_{\\text{extract}}\\), to a raw input \\(y_{raw}\\) and a given Instruction."
    },
    {
      "index_id": 248,
      "parent_id": 243,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 248,
        "pdf_para_block": {
          "bbox": [
            50,
            319,
            295,
            353
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                319,
                295,
                353
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    319,
                    295,
                    353
                  ],
                  "type": "text",
                  "content": "A.1.2 QA Performance Metrics. Based on the ground truth "
                },
                {
                  "bbox": [
                    50,
                    319,
                    295,
                    353
                  ],
                  "type": "inline_equation",
                  "content": "y_{gold}"
                },
                {
                  "bbox": [
                    50,
                    319,
                    295,
                    353
                  ],
                  "type": "text",
                  "content": " and the model's response (either raw "
                },
                {
                  "bbox": [
                    50,
                    319,
                    295,
                    353
                  ],
                  "type": "inline_equation",
                  "content": "y_{raw}"
                },
                {
                  "bbox": [
                    50,
                    319,
                    295,
                    353
                  ],
                  "type": "text",
                  "content": " or extracted "
                },
                {
                  "bbox": [
                    50,
                    319,
                    295,
                    353
                  ],
                  "type": "inline_equation",
                  "content": "\\hat{y}"
                },
                {
                  "bbox": [
                    50,
                    319,
                    295,
                    353
                  ],
                  "type": "text",
                  "content": "), we compute the following metrics:"
                }
              ]
            }
          ],
          "index": 7
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "A.1.2 QA Performance Metrics. Based on the ground truth  $y_{gold}$  and the model's response (either raw  $y_{raw}$  or extracted  $\\hat{y}$ ), we compute the following metrics: \n$$\n\\text {A c c u r a c y} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\mathbb {I} \\left(\\mathcal {N} \\left(y _ {\\text {g o l d}, i}\\right) \\subseteq \\mathcal {N} \\left(y _ {\\text {r a w}, i}\\right)\\right) \\tag {17}\n$$\n where  $\\subseteq$  denotes the substring inclusion relation.",
        "title_level": -1
      },
      "summary": "The Accuracy metric for QA performance is calculated by averaging over all examples whether the normalized ground truth answer is a substring of the model's normalized raw response."
    },
    {
      "index_id": 249,
      "parent_id": 243,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 249,
        "pdf_para_block": {
          "bbox": [
            50,
            357,
            295,
            412
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                357,
                295,
                412
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    357,
                    295,
                    412
                  ],
                  "type": "text",
                  "content": "Accuracy (Inclusion-based). Following prior works [3, 34, 46], we utilize accuracy as a soft-match metric. We consider a prediction correct if the normalized gold answer is included in the model's generated response, rather than requiring a strict exact match. This accounts for the uncontrollable nature of LLM generation."
                }
              ]
            }
          ],
          "index": 8
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Accuracy (Inclusion-based). Following prior works [3, 34, 46], we utilize accuracy as a soft-match metric. We consider a prediction correct if the normalized gold answer is included in the model's generated response, rather than requiring a strict exact match. This accounts for the uncontrollable nature of LLM generation.",
        "title_level": -1
      },
      "summary": "Accuracy is a soft-match metric that considers a prediction correct if the normalized gold answer appears within the model's generated response, accommodating the variability of LLM outputs instead of requiring exact matches."
    },
    {
      "index_id": 250,
      "parent_id": 243,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 250,
        "pdf_para_block": {
          "bbox": [
            89,
            415,
            294,
            443
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                89,
                415,
                294,
                443
              ],
              "spans": [
                {
                  "bbox": [
                    89,
                    415,
                    294,
                    443
                  ],
                  "type": "interline_equation",
                  "content": "\\text {A c c u r a c y} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\mathbb {I} \\left(\\mathcal {N} \\left(y _ {\\text {g o l d}, i}\\right) \\subseteq \\mathcal {N} \\left(y _ {\\text {r a w}, i}\\right)\\right) \\tag {17}",
                  "image_path": "7dd059293555e4ae9c05a91c29ce4f8d29bdc77303d258bb732164ee2f8cdd93.jpg"
                }
              ]
            }
          ],
          "index": 9
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\n\\text {A c c u r a c y} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\mathbb {I} \\left(\\mathcal {N} \\left(y _ {\\text {g o l d}, i}\\right) \\subseteq \\mathcal {N} \\left(y _ {\\text {r a w}, i}\\right)\\right) \\tag {17}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation 17 defines a metric for accuracy, calculated as the average over all examples of whether the set of correct answers for an example is fully contained within the set of raw model outputs for that example."
    },
    {
      "index_id": 251,
      "parent_id": 243,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 251,
        "pdf_para_block": {
          "bbox": [
            50,
            463,
            295,
            496
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                463,
                295,
                496
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    463,
                    295,
                    496
                  ],
                  "type": "text",
                  "content": "Exact Match (EM). Unlike accuracy, Exact Match is a strict metric. It measures whether the normalized extracted answer "
                },
                {
                  "bbox": [
                    50,
                    463,
                    295,
                    496
                  ],
                  "type": "inline_equation",
                  "content": "\\hat{y}"
                },
                {
                  "bbox": [
                    50,
                    463,
                    295,
                    496
                  ],
                  "type": "text",
                  "content": " is character-for-character identical to the ground truth."
                }
              ]
            }
          ],
          "index": 11
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Exact Match (EM). Unlike accuracy, Exact Match is a strict metric. It measures whether the normalized extracted answer  $\\hat{y}$  is character-for-character identical to the ground truth.",
        "title_level": -1
      },
      "summary": "Exact Match (EM) is a strict evaluation metric that requires a normalized extracted answer to be character-for-character identical to the ground truth."
    },
    {
      "index_id": 252,
      "parent_id": 243,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 252,
        "pdf_para_block": {
          "bbox": [
            107,
            499,
            294,
            528
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                107,
                499,
                294,
                528
              ],
              "spans": [
                {
                  "bbox": [
                    107,
                    499,
                    294,
                    528
                  ],
                  "type": "interline_equation",
                  "content": "\\operatorname {E M} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\mathbb {I} \\left(\\mathcal {N} \\left(\\hat {y} _ {i}\\right) = \\mathcal {N} \\left(y _ {\\text {g o l d}, i}\\right)\\right) \\tag {18}",
                  "image_path": "fb96ea8f01e162830e424ef6238a811d695c01dd2715f8d1e6a88aff6f6a01c2.jpg"
                }
              ]
            }
          ],
          "index": 12
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\n\\operatorname {E M} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\mathbb {I} \\left(\\mathcal {N} \\left(\\hat {y} _ {i}\\right) = \\mathcal {N} \\left(y _ {\\text {g o l d}, i}\\right)\\right) \\tag {18}\n$$\n",
        "title_level": -1
      },
      "summary": "Equation (18) defines the Exact Match (EM) metric as the average over N examples, where each example contributes 1 if the predicted label's class matches the true label's class, and 0 otherwise."
    },
    {
      "index_id": 253,
      "parent_id": 243,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 253,
        "pdf_para_block": {
          "bbox": [
            50,
            532,
            295,
            566
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                50,
                532,
                295,
                566
              ],
              "spans": [
                {
                  "bbox": [
                    50,
                    532,
                    295,
                    566
                  ],
                  "type": "text",
                  "content": "F1-score. For questions requiring text span answers, we utilize the token-level F1-score between the extracted answer "
                },
                {
                  "bbox": [
                    50,
                    532,
                    295,
                    566
                  ],
                  "type": "inline_equation",
                  "content": "\\hat{y}"
                },
                {
                  "bbox": [
                    50,
                    532,
                    295,
                    566
                  ],
                  "type": "text",
                  "content": " and the ground truth "
                },
                {
                  "bbox": [
                    50,
                    532,
                    295,
                    566
                  ],
                  "type": "inline_equation",
                  "content": "y_{gold}"
                },
                {
                  "bbox": [
                    50,
                    532,
                    295,
                    566
                  ],
                  "type": "text",
                  "content": ". Treating them as bags of tokens "
                },
                {
                  "bbox": [
                    50,
                    532,
                    295,
                    566
                  ],
                  "type": "inline_equation",
                  "content": "T_{\\hat{y}}"
                },
                {
                  "bbox": [
                    50,
                    532,
                    295,
                    566
                  ],
                  "type": "text",
                  "content": " and "
                },
                {
                  "bbox": [
                    50,
                    532,
                    295,
                    566
                  ],
                  "type": "inline_equation",
                  "content": "T_{gold}"
                },
                {
                  "bbox": [
                    50,
                    532,
                    295,
                    566
                  ],
                  "type": "text",
                  "content": ":"
                }
              ]
            }
          ],
          "index": 13
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "F1-score. For questions requiring text span answers, we utilize the token-level F1-score between the extracted answer  $\\hat{y}$  and the ground truth  $y_{gold}$ . Treating them as bags of tokens  $T_{\\hat{y}}$  and  $T_{gold}$ : \n$$\nP = \\frac {\\left| T _ {\\hat {y}} \\cap T _ {g o l d} \\right|}{\\left| T _ {\\hat {y}} \\right|}, \\quad R = \\frac {\\left| T _ {\\hat {y}} \\cap T _ {g o l d} \\right|}{\\left| T _ {g o l d} \\right|}, \\quad F 1 = \\frac {2 \\cdot P \\cdot R}{P + R} \\tag {19}\n$$",
        "title_level": -1
      },
      "summary": "F1-score is used to evaluate text span answers by comparing extracted and ground truth tokens. Precision (P) measures the proportion of correct tokens in the extracted answer, while recall (R) measures the proportion of correct tokens retrieved from the ground truth. The F1-score is the harmonic mean of precision and recall, calculated as \\( F1 = \\frac{2 \\cdot P \\cdot R}{P + R} \\)."
    },
    {
      "index_id": 254,
      "parent_id": 243,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 254,
        "pdf_para_block": {
          "bbox": [
            69,
            568,
            294,
            593
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                69,
                568,
                294,
                593
              ],
              "spans": [
                {
                  "bbox": [
                    69,
                    568,
                    294,
                    593
                  ],
                  "type": "interline_equation",
                  "content": "P = \\frac {\\left| T _ {\\hat {y}} \\cap T _ {g o l d} \\right|}{\\left| T _ {\\hat {y}} \\right|}, \\quad R = \\frac {\\left| T _ {\\hat {y}} \\cap T _ {g o l d} \\right|}{\\left| T _ {g o l d} \\right|}, \\quad F 1 = \\frac {2 \\cdot P \\cdot R}{P + R} \\tag {19}",
                  "image_path": "58174d2dadb758b0d14b9cd362e5ae96b9a7bede5ce7853b0aa6282eac12e4ac.jpg"
                }
              ]
            }
          ],
          "index": 14
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\nP = \\frac {\\left| T _ {\\hat {y}} \\cap T _ {g o l d} \\right|}{\\left| T _ {\\hat {y}} \\right|}, \\quad R = \\frac {\\left| T _ {\\hat {y}} \\cap T _ {g o l d} \\right|}{\\left| T _ {g o l d} \\right|}, \\quad F 1 = \\frac {2 \\cdot P \\cdot R}{P + R} \\tag {19}\n$$\n",
        "title_level": -1
      },
      "summary": "The formulas define precision (P), recall (R), and F1 score, which are standard metrics for evaluating classification or information retrieval systems. Precision measures the fraction of predicted items that are correct, recall measures the fraction of correct items that were successfully predicted, and the F1 score is their harmonic mean, providing a single balanced metric."
    },
    {
      "index_id": 255,
      "parent_id": 243,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 255,
        "pdf_para_block": {
          "bbox": [
            314,
            84,
            559,
            149
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                84,
                559,
                149
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    84,
                    559,
                    149
                  ],
                  "type": "text",
                  "content": "A.1.3 Retrieval Recall. As described in the main text, we evaluate retrieval quality based on the granularity of parsed PDF blocks (e.g., paragraphs, tables, images). For a given query "
                },
                {
                  "bbox": [
                    314,
                    84,
                    559,
                    149
                  ],
                  "type": "inline_equation",
                  "content": "q"
                },
                {
                  "bbox": [
                    314,
                    84,
                    559,
                    149
                  ],
                  "type": "text",
                  "content": ", let "
                },
                {
                  "bbox": [
                    314,
                    84,
                    559,
                    149
                  ],
                  "type": "inline_equation",
                  "content": "\\mathcal{B}_{gold}"
                },
                {
                  "bbox": [
                    314,
                    84,
                    559,
                    149
                  ],
                  "type": "text",
                  "content": " be the set of manually labeled ground-truth blocks required to answer "
                },
                {
                  "bbox": [
                    314,
                    84,
                    559,
                    149
                  ],
                  "type": "inline_equation",
                  "content": "q"
                },
                {
                  "bbox": [
                    314,
                    84,
                    559,
                    149
                  ],
                  "type": "text",
                  "content": ", and "
                },
                {
                  "bbox": [
                    314,
                    84,
                    559,
                    149
                  ],
                  "type": "inline_equation",
                  "content": "\\mathcal{B}_{ret}"
                },
                {
                  "bbox": [
                    314,
                    84,
                    559,
                    149
                  ],
                  "type": "text",
                  "content": " be the set of unique blocks retrieved by the system. The Retrieval Recall is defined as:"
                }
              ]
            }
          ],
          "index": 15
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "A.1.3 Retrieval Recall. As described in the main text, we evaluate retrieval quality based on the granularity of parsed PDF blocks (e.g., paragraphs, tables, images). For a given query  $q$ , let  $\\mathcal{B}_{gold}$  be the set of manually labeled ground-truth blocks required to answer  $q$ , and  $\\mathcal{B}_{ret}$  be the set of unique blocks retrieved by the system. The Retrieval Recall is defined as: \n$$\n\\operatorname {R e c a l l} _ {r e t} = \\left\\{ \\begin{array}{l l} 0 & \\text {i f p a r s i n g e r r o r o c c u r s o n} \\mathcal {B} _ {\\text {g o l d}} \\\\ \\frac {\\left| \\mathcal {B} _ {\\text {r e t}} \\cap \\mathcal {B} _ {\\text {g o l d}} \\right|}{\\left| \\mathcal {B} _ {\\text {g o l d}} \\right|} & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {20}\n$$",
        "title_level": -1
      },
      "summary": "Retrieval Recall measures how well a system retrieves the necessary PDF content blocks (like paragraphs or tables) to answer a query. It is calculated as the proportion of manually labeled ground-truth blocks that are successfully retrieved. If a parsing error affects the ground-truth blocks, the recall score is automatically zero."
    },
    {
      "index_id": 256,
      "parent_id": 243,
      "type": "NodeType.EQUATION",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 256,
        "pdf_para_block": {
          "bbox": [
            320,
            153,
            558,
            182
          ],
          "type": "interline_equation",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                320,
                153,
                558,
                182
              ],
              "spans": [
                {
                  "bbox": [
                    320,
                    153,
                    558,
                    182
                  ],
                  "type": "interline_equation",
                  "content": "\\operatorname {R e c a l l} _ {r e t} = \\left\\{ \\begin{array}{l l} 0 & \\text {i f p a r s i n g e r r o r o c c u r s o n} \\mathcal {B} _ {\\text {g o l d}} \\\\ \\frac {\\left| \\mathcal {B} _ {\\text {r e t}} \\cap \\mathcal {B} _ {\\text {g o l d}} \\right|}{\\left| \\mathcal {B} _ {\\text {g o l d}} \\right|} & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {20}",
                  "image_path": "d63411c7242c58c16bd4f353768d1a9b2b77ac8fd1f0e75e53ec0710ed7e96ba.jpg"
                }
              ]
            }
          ],
          "index": 16
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "\n$$\n\\operatorname {R e c a l l} _ {r e t} = \\left\\{ \\begin{array}{l l} 0 & \\text {i f p a r s i n g e r r o r o c c u r s o n} \\mathcal {B} _ {\\text {g o l d}} \\\\ \\frac {\\left| \\mathcal {B} _ {\\text {r e t}} \\cap \\mathcal {B} _ {\\text {g o l d}} \\right|}{\\left| \\mathcal {B} _ {\\text {g o l d}} \\right|} & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {20}\n$$\n",
        "title_level": -1
      },
      "summary": "The recall metric for retrieval, denoted as \\( \\operatorname{Recall}_{ret} \\), is defined in two cases: it is zero if a parsing error occurs on the gold-standard bounding boxes \\( \\mathcal{B}_{\\text{gold}} \\); otherwise, it is calculated as the ratio of the number of correctly retrieved bounding boxes (the intersection of retrieved boxes \\( \\mathcal{B}_{\\text{ret}} \\) and gold boxes) to the total number of gold boxes."
    },
    {
      "index_id": 257,
      "parent_id": 243,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 257,
        "pdf_para_block": {
          "bbox": [
            314,
            182,
            558,
            224
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                182,
                558,
                224
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    182,
                    558,
                    224
                  ],
                  "type": "text",
                  "content": "Specifically, if a ground-truth block is lost due to PDF parsing failures (i.e., it does not exist in the candidate pool), it is considered strictly unretrievable, resulting in a recall contribution of 0 for that specific block."
                }
              ]
            }
          ],
          "index": 17
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Specifically, if a ground-truth block is lost due to PDF parsing failures (i.e., it does not exist in the candidate pool), it is considered strictly unretrievable, resulting in a recall contribution of 0 for that specific block.",
        "title_level": -1
      },
      "summary": "A ground-truth block lost during PDF parsing is unretrievable and contributes zero to recall, as it is absent from the candidate pool."
    },
    {
      "index_id": 258,
      "parent_id": 242,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 258,
        "pdf_para_block": {
          "bbox": [
            315,
            234,
            462,
            246
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                315,
                234,
                462,
                246
              ],
              "spans": [
                {
                  "bbox": [
                    315,
                    234,
                    462,
                    246
                  ],
                  "type": "text",
                  "content": "A.2 Implementation details"
                }
              ]
            }
          ],
          "index": 18
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "A.2 Implementation details",
        "title_level": 2
      },
      "summary": "This section details the implementation specifics of the BookRAG system, including its core components, model choices, and the standardized experimental setup used for evaluation."
    },
    {
      "index_id": 259,
      "parent_id": 258,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 259,
        "pdf_para_block": {
          "bbox": [
            313,
            248,
            559,
            534
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                313,
                248,
                559,
                534
              ],
              "spans": [
                {
                  "bbox": [
                    313,
                    248,
                    559,
                    534
                  ],
                  "type": "text",
                  "content": "We implement BookRAG in Python, utilizing MinerU [52] for robust document layout parsing. For a fair comparison, both BookRAG and all baseline methods are powered by a unified set of state-of-the-art (SOTA) and widely adopted backbone models from the Qwen family [4, 60, 63, 64], including LLM, vision-language model (VLM), and embedding models. Specifically, we utilize Qwen3-8B [60] as the default LLM, Qwen2.5VL-30B [4] as the vision-language model (VLM), Qwen3-Embedding-0.6B [64] for text embedding, gme-Qwen2-VL-2B-Instruct [63] for multi-modal embedding, and Qwen3-Reranker-4B [64] for reranking. We primarily select models under the 10B parameter scale to balance efficiency and effectiveness. However, for the VLM, we adopt the 30B version, as the 8B counterpart exhibited significant performance deficits, frequently failing to answer correctly even when provided with ground-truth images. All experiments were conducted on a Linux operating system running on a high-performance server equipped with an Intel Xeon 2.0GHz CPU, 1024GB of memory, and 8 NVIDIA GeForce RTX A5000 GPUs, each with 24 GB of VRAM. Specifically, to ensure a fair comparison of efficiency, all methods were executed serially, and the reported time costs reflect this sequential processing mode. For methods involving document chunking and retrieval ranking, we standardize the chunk size at 500 tokens and set the retrieval top-k to 10 to ensure consistent candidate pool sizes across baselines. For further reproducibility, our source code and detailed implementation configurations are publicly available at our repository: https://github.com/sam234990/BookRAG."
                }
              ]
            }
          ],
          "index": 19
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "We implement BookRAG in Python, utilizing MinerU [52] for robust document layout parsing. For a fair comparison, both BookRAG and all baseline methods are powered by a unified set of state-of-the-art (SOTA) and widely adopted backbone models from the Qwen family [4, 60, 63, 64], including LLM, vision-language model (VLM), and embedding models. Specifically, we utilize Qwen3-8B [60] as the default LLM, Qwen2.5VL-30B [4] as the vision-language model (VLM), Qwen3-Embedding-0.6B [64] for text embedding, gme-Qwen2-VL-2B-Instruct [63] for multi-modal embedding, and Qwen3-Reranker-4B [64] for reranking. We primarily select models under the 10B parameter scale to balance efficiency and effectiveness. However, for the VLM, we adopt the 30B version, as the 8B counterpart exhibited significant performance deficits, frequently failing to answer correctly even when provided with ground-truth images. All experiments were conducted on a Linux operating system running on a high-performance server equipped with an Intel Xeon 2.0GHz CPU, 1024GB of memory, and 8 NVIDIA GeForce RTX A5000 GPUs, each with 24 GB of VRAM. Specifically, to ensure a fair comparison of efficiency, all methods were executed serially, and the reported time costs reflect this sequential processing mode. For methods involving document chunking and retrieval ranking, we standardize the chunk size at 500 tokens and set the retrieval top-k to 10 to ensure consistent candidate pool sizes across baselines. For further reproducibility, our source code and detailed implementation configurations are publicly available at our repository: https://github.com/sam234990/BookRAG.",
        "title_level": -1
      },
      "summary": "BookRAG is a Python-based system that uses MinerU for document layout parsing and relies on Qwen family models for its core functions: Qwen3-8B as the LLM, Qwen2.5VL-30B as the VLM (chosen over an 8B version due to performance deficits), Qwen3-Embedding-0.6B for text, gme-Qwen2-VL-2B-Instruct for multi-modal embedding, and Qwen3-Reranker-4B for reranking. To ensure a fair and efficient comparison with baseline methods, experiments were standardized with a 500-token chunk size, a retrieval top-k of 10, and sequential processing on a high-performance server with 8 NVIDIA RTX A5000 GPUs. The source code is publicly available for reproducibility."
    },
    {
      "index_id": 260,
      "parent_id": 242,
      "type": "NodeType.TITLE",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 260,
        "pdf_para_block": {
          "bbox": [
            315,
            543,
            388,
            555
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                315,
                543,
                388,
                555
              ],
              "spans": [
                {
                  "bbox": [
                    315,
                    543,
                    388,
                    555
                  ],
                  "type": "text",
                  "content": "A.3 Prompts"
                }
              ]
            }
          ],
          "index": 20
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "A.3 Prompts",
        "title_level": 2
      },
      "summary": "This section details the specific prompts used in a multi-step agent-based system for classifying queries, decomposing questions, generating filter operators, and judging entity resolution during knowledge graph construction."
    },
    {
      "index_id": 261,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 261,
        "pdf_para_block": {
          "bbox": [
            314,
            557,
            558,
            613
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                314,
                557,
                558,
                613
              ],
              "spans": [
                {
                  "bbox": [
                    314,
                    557,
                    558,
                    613
                  ],
                  "type": "text",
                  "content": "Specifically, we present the prompts designed for agent-based query classification (Figure 10), question decomposition (Figure 11), and filter operator generation (Figure 12). Additionally, we illustrate the prompt employed for entity resolution judgment (Figure 13) during the graph construction phase."
                }
              ]
            }
          ],
          "index": 21
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Specifically, we present the prompts designed for agent-based query classification (Figure 10), question decomposition (Figure 11), and filter operator generation (Figure 12). Additionally, we illustrate the prompt employed for entity resolution judgment (Figure 13) during the graph construction phase.",
        "title_level": -1
      },
      "summary": "The content details specific prompts used in a multi-step agent-based system for processing queries and constructing a knowledge graph. These prompts are designed for four key tasks: classifying the initial query, decomposing complex questions, generating filter operators, and judging entity resolution during graph construction."
    },
    {
      "index_id": 262,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 14,
        "page_path": null,
        "pdf_id": 262,
        "pdf_para_block": {
          "bbox": [
            302,
            713,
            308,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                713,
                308,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    713,
                    308,
                    719
                  ],
                  "type": "text",
                  "content": "15"
                }
              ]
            }
          ],
          "index": 22
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "15",
        "title_level": -1
      },
      "summary": "The content is the number 15."
    },
    {
      "index_id": 263,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 263,
        "pdf_para_block": {
          "bbox": [
            55,
            90,
            547,
            110
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                55,
                90,
                547,
                110
              ],
              "spans": [
                {
                  "bbox": [
                    55,
                    90,
                    547,
                    110
                  ],
                  "type": "text",
                  "content": "You are an expert query analyzer. Your only task is to classify the user's question into one of three categories: \"simple\", \"complex\", or \"global\". Respond only with the specified JSON object."
                }
              ]
            }
          ],
          "index": 0
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "You are an expert query analyzer. Your only task is to classify the user's question into one of three categories: \"simple\", \"complex\", or \"global\". Respond only with the specified JSON object.",
        "title_level": -1
      },
      "summary": "This text defines a specific task for an AI: to analyze user questions and categorize them as \"simple,\" \"complex,\" or \"global.\" The only acceptable response is a JSON object containing this classification."
    },
    {
      "index_id": 264,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 264,
        "pdf_para_block": {
          "bbox": [
            56,
            121,
            141,
            129
          ],
          "type": "title",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                56,
                121,
                141,
                129
              ],
              "spans": [
                {
                  "bbox": [
                    56,
                    121,
                    141,
                    129
                  ],
                  "type": "text",
                  "content": "Category Definitions:"
                }
              ]
            }
          ],
          "index": 1
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Category Definitions:",
        "title_level": -1
      },
      "summary": "The content provides category definitions but does not include the specific definitions themselves."
    },
    {
      "index_id": 265,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 265,
        "pdf_para_block": {
          "bbox": [
            56,
            130,
            527,
            149
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                56,
                130,
                527,
                149
              ],
              "spans": [
                {
                  "bbox": [
                    56,
                    130,
                    527,
                    149
                  ],
                  "type": "text",
                  "content": "1. single-hop: The question can be fully answered by retrieving information from a SINGLE, contiguous location in the document (e.g., one specific paragraph, one complete table, or one figure)."
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "1. single-hop: The question can be fully answered by retrieving information from a SINGLE, contiguous location in the document (e.g., one specific paragraph, one complete table, or one figure).",
        "title_level": -1
      },
      "summary": "Single-hop questions are those that can be answered using information from just one specific, uninterrupted section of a document, such as a single paragraph, table, or figure."
    },
    {
      "index_id": 266,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 266,
        "pdf_para_block": {
          "bbox": [
            69,
            150,
            534,
            199
          ],
          "type": "list",
          "angle": 0,
          "index": 6,
          "blocks": [
            {
              "bbox": [
                69,
                150,
                530,
                169
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    69,
                    150,
                    530,
                    169
                  ],
                  "spans": [
                    {
                      "bbox": [
                        69,
                        150,
                        530,
                        169
                      ],
                      "type": "text",
                      "content": "- This includes questions that require reasoning or comparison, as long as all the necessary data is present within that single retrieved location."
                    }
                  ]
                }
              ],
              "index": 3
            },
            {
              "bbox": [
                69,
                170,
                242,
                179
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    69,
                    170,
                    242,
                    179
                  ],
                  "spans": [
                    {
                      "bbox": [
                        69,
                        170,
                        242,
                        179
                      ],
                      "type": "text",
                      "content": "- Example: \"What is the title of Figure 2?\""
                    }
                  ]
                }
              ],
              "index": 4
            },
            {
              "bbox": [
                69,
                180,
                534,
                199
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    69,
                    180,
                    534,
                    199
                  ],
                  "spans": [
                    {
                      "bbox": [
                        69,
                        180,
                        534,
                        199
                      ],
                      "type": "text",
                      "content": "- Example: \"How do "
                    },
                    {
                      "bbox": [
                        69,
                        180,
                        534,
                        199
                      ],
                      "type": "inline_equation",
                      "content": "5\\%"
                    },
                    {
                      "bbox": [
                        69,
                        180,
                        534,
                        199
                      ],
                      "type": "text",
                      "content": " of the Latinos see economic upward mobility for their children?\" -> This is SIMPLE because the answer can be found by looking at a single chart or paragraph."
                    }
                  ]
                }
              ],
              "index": 5
            }
          ],
          "sub_type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The data is unavailable for summarization."
    },
    {
      "index_id": 267,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 267,
        "pdf_para_block": {
          "bbox": [
            56,
            210,
            524,
            229
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                56,
                210,
                524,
                229
              ],
              "spans": [
                {
                  "bbox": [
                    56,
                    210,
                    524,
                    229
                  ],
                  "type": "text",
                  "content": "2. multi-hop: The question requires decomposition into multiple simple sub-questions, where each sub-question must be answered by a separate retrieval action."
                }
              ]
            }
          ],
          "index": 7
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "2. multi-hop: The question requires decomposition into multiple simple sub-questions, where each sub-question must be answered by a separate retrieval action.",
        "title_level": -1
      },
      "summary": "Multi-hop questions require breaking down into simpler sub-questions, each answered through a separate information retrieval step."
    },
    {
      "index_id": 268,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 268,
        "pdf_para_block": {
          "bbox": [
            69,
            230,
            518,
            269
          ],
          "type": "list",
          "angle": 0,
          "index": 10,
          "blocks": [
            {
              "bbox": [
                69,
                230,
                514,
                249
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    69,
                    230,
                    514,
                    249
                  ],
                  "spans": [
                    {
                      "bbox": [
                        69,
                        230,
                        514,
                        249
                      ],
                      "type": "text",
                      "content": "- It often contains a nested or indirect constraint that requires a preliminary step to resolve before the main question can be answered."
                    }
                  ]
                }
              ],
              "index": 8
            },
            {
              "bbox": [
                69,
                250,
                518,
                269
              ],
              "type": "text",
              "angle": 0,
              "lines": [
                {
                  "bbox": [
                    69,
                    250,
                    518,
                    269
                  ],
                  "spans": [
                    {
                      "bbox": [
                        69,
                        250,
                        518,
                        269
                      ],
                      "type": "text",
                      "content": "- Example: \"What is the color of the personality vector...?\" -> This is COMPLEX because it requires two separate retrieval actions."
                    }
                  ]
                }
              ],
              "index": 9
            }
          ],
          "sub_type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The data shows a clear correlation between a country's GDP per capita and its average life expectancy, with wealthier nations generally having longer-lived populations."
    },
    {
      "index_id": 269,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 269,
        "pdf_para_block": {
          "bbox": [
            56,
            280,
            541,
            299
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                56,
                280,
                541,
                299
              ],
              "spans": [
                {
                  "bbox": [
                    56,
                    280,
                    541,
                    299
                  ],
                  "type": "text",
                  "content": "3. global: The question requires an aggregation operation (e.g., counting, listing, summarizing) over a set of items that are identified by a clear structural filter."
                }
              ]
            }
          ],
          "index": 11
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "3. global: The question requires an aggregation operation (e.g., counting, listing, summarizing) over a set of items that are identified by a clear structural filter.",
        "title_level": -1
      },
      "summary": "The content describes a global question type that involves aggregating data—such as counting or summarizing—from a set of items selected by a specific structural filter."
    },
    {
      "index_id": 270,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 270,
        "pdf_para_block": {
          "bbox": [
            69,
            300,
            539,
            319
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                69,
                300,
                539,
                319
              ],
              "spans": [
                {
                  "bbox": [
                    69,
                    300,
                    539,
                    319
                  ],
                  "type": "text",
                  "content": "- Example: \"How many tables are in the document?\" -> This is GLOBAL because the process is to filter for all items of type 'table'."
                }
              ]
            }
          ],
          "index": 12
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "-Example: \"How many tables are in the document?\" -> This is GLOBAL because the process is to filter for all items of type 'table'.",
        "title_level": -1
      },
      "summary": "A query is classified as GLOBAL when the process to answer it requires filtering for all items of a specific type. The example, \"How many tables are in the document?\", demonstrates this because answering it necessitates finding every item of the type 'table'."
    },
    {
      "index_id": 271,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 271,
        "pdf_para_block": {
          "bbox": [
            56,
            330,
            126,
            339
          ],
          "type": "text",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                56,
                330,
                126,
                339
              ],
              "spans": [
                {
                  "bbox": [
                    56,
                    330,
                    126,
                    339
                  ],
                  "type": "text",
                  "content": "User Query: query"
                }
              ]
            }
          ],
          "index": 13
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "User Query: query",
        "title_level": -1
      },
      "summary": "The user query is \"query\"."
    },
    {
      "index_id": 272,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 272,
        "pdf_para_block": {
          "bbox": [
            209,
            363,
            400,
            374
          ],
          "angle": 0,
          "lines": [
            {
              "bbox": [
                209,
                363,
                400,
                374
              ],
              "spans": [
                {
                  "bbox": [
                    209,
                    363,
                    400,
                    374
                  ],
                  "type": "text",
                  "content": "Figure 10: The prompt for query classification."
                }
              ]
            }
          ],
          "index": 14,
          "type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Figure 10: The prompt for query classification.",
        "title_level": -1
      },
      "summary": "Figure 10 presents the specific prompt used to classify user queries."
    },
    {
      "index_id": 273,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 15,
        "page_path": null,
        "pdf_id": 273,
        "pdf_para_block": {
          "bbox": [
            302,
            714,
            309,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                714,
                309,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    714,
                    309,
                    719
                  ],
                  "type": "text",
                  "content": "16"
                }
              ]
            }
          ],
          "index": 15
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "16",
        "title_level": -1
      },
      "summary": "The content is the number 16."
    },
    {
      "index_id": 274,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 16,
        "page_path": null,
        "pdf_id": 274,
        "pdf_para_block": {
          "type": "code",
          "bbox": [
            53,
            88,
            558,
            622
          ],
          "blocks": [
            {
              "bbox": [
                53,
                88,
                558,
                622
              ],
              "lines": [
                {
                  "bbox": [
                    53,
                    88,
                    558,
                    622
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        88,
                        558,
                        622
                      ],
                      "type": "text",
                      "content": "You are a query decomposition expert. You have been given a \"complex\" question. Your task is to break it down into a series of simple, atomic sub-questions and classify each one by type.  \n**Crucial Instructions:**  \n1. Each `retrieval` sub-question MUST be a direct information retrieval task that can be answered independently by looking up a specific fact, number, or value in the document.  \n2. **retrieval` sub-questions MUST NOT depend on the answer of another sub-question.** They should be parallelizable. All logic for combining their results must be placed in a final `synthesis` question.  \n3. A `synthesis` question requires comparing, calculating, or combining the answers of the previous `retrieval` questions. It does **NOT** require a new lookup in the document.  \nYou MUST provide your response in a JSON object with a single key 'sub_questions', which contains a list of objects. Each object must have a 'question' (string) and a 'type' (string: \"retrieval\" or \"synthesis\").  \n--- EXAMPLE 1 (Correct Decomposition with Independent Lookups) ---  \nComplex Query: \"What is the color of the personality vector in the soft-labeled personality embedding matrix that with the highest Receptiviti score for User A2GBIFL43U1LKJ?\"  \nExpected JSON Output:  \n{{\"sub_questions\": [{\"question\": \"What are all the Receptiviti scores for each personality vector for User A2GBIFL43U1LKJ?\", \"type\": \"retrieval\"}], {\"question\": \"What is the mapping of personality vectors to their colors in the soft-labeled personality embedding matrix?\", \"type\": \"retrieval\"}}, {\"question\": \"From the gathered scores, identify the personality vector with the highest score, and then find its corresponding color from the vector-to-color mapping.\", \"type\": \"synthesis\"}]  \n}}  \n--- END EXAMPLE 1 ---  \n--- EXAMPLE 2 (Decomposition with retrieval and synthesis steps) ---  \nComplex Query: \"According to the report, which one is greater in population in the survey? Foreign born Latinos, or the Latinos interviewed by cellphone?\"  \nExpected JSON Output:  \n{{\"sub_questions\": [{\"question\": \"According to the report, what is the population of foreign born Latinos in the survey?\", \"type\": \"retrieval\"}], {\"question\": \"According to the report, what is the population of Latinos interviewed by cellphone in the survey?\", \"type\": \"retrieval\"}], {\"question\": \"Which of the two population counts is greater?\", \"type\": \"synthesis\"}]  \n}}  \n--- END EXAMPLE 2 ---  \nNow, perform the decomposition for the following query.  \nUser Query: query"
                    }
                  ]
                }
              ],
              "index": 0,
              "angle": 0,
              "type": "code_body"
            }
          ],
          "index": 0,
          "sub_type": "code",
          "guess_lang": "txt"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The table presents the top 10 countries by total gold reserves as of 2024. The United States holds the largest reserves at 8,133.5 tonnes, followed by Germany (3,352.6 tonnes), Italy (2,451.8 tonnes), and France (2,436.9 tonnes). Russia ranks fifth with 2,332.7 tonnes. China, Switzerland, Japan, India, and the Netherlands complete the list, with reserves ranging from China's 2,191.5 tonnes to the Netherlands' 612.5 tonnes. The data highlights the significant concentration of official gold holdings among a small group of nations, with the U.S. holding more than double the reserves of the next largest holder."
    },
    {
      "index_id": 275,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 16,
        "page_path": null,
        "pdf_id": 275,
        "pdf_para_block": {
          "bbox": [
            205,
            642,
            403,
            653
          ],
          "angle": 0,
          "lines": [
            {
              "bbox": [
                205,
                642,
                403,
                653
              ],
              "spans": [
                {
                  "bbox": [
                    205,
                    642,
                    403,
                    653
                  ],
                  "type": "text",
                  "content": "Figure 11: The prompt for query decomposition."
                }
              ]
            }
          ],
          "index": 1,
          "type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Figure 11: The prompt for query decomposition.",
        "title_level": -1
      },
      "summary": "Figure 11 illustrates the specific prompt used for query decomposition."
    },
    {
      "index_id": 276,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 16,
        "page_path": null,
        "pdf_id": 276,
        "pdf_para_block": {
          "bbox": [
            302,
            714,
            308,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                714,
                308,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    714,
                    308,
                    719
                  ],
                  "type": "text",
                  "content": "17"
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "17",
        "title_level": -1
      },
      "summary": "The content is the number 17."
    },
    {
      "index_id": 277,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 17,
        "page_path": null,
        "pdf_id": 277,
        "pdf_para_block": {
          "type": "code",
          "bbox": [
            53,
            86,
            558,
            443
          ],
          "blocks": [
            {
              "bbox": [
                53,
                86,
                558,
                443
              ],
              "lines": [
                {
                  "bbox": [
                    53,
                    86,
                    558,
                    443
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        86,
                        558,
                        443
                      ],
                      "type": "text",
                      "content": "You are a highly specialized AI assistant. Your only function is to analyze a \"Global Query\" and return a single, valid JSON object that specifies both the filtering steps and the final aggregation operation. You MUST NOT output any other text or explanation.  \n```bash\n##INSTRUCTIONS \\& DEFINITIONS  \n1. **Filters**: You MUST determine the list of `filters` to apply. Even if the filter is for the whole document (e.g., all tables), the `filters` list must be present.  \n- `filter_type': One of [\"section\", \"image\", \"table\", \"page\"].  \n- `section': Use for structural parts like chapters, sections, appendices, or references.  \n- `image': Use for visual elements like figures, images, pictures, or plots.  \n- `table': Use for tabular data.  \n- `page': Use for specific page numbers or ranges.  \n- `filter_value': (Optional) Can be provided for \"section\" (e.g., a section title) or \"page\" (e.g., '3-10' or '5').  \n**For \"image\" or \"table\", this value MUST be null.**  \n2. **Operation**: Determine the final aggregation operation.  \n- `operation': One of [\"COUNT\", \"LIST\", \"SUMMARIZE\", \"ANALYZE\"]  \n## EXAMPLES OF YOUR TASK  \nUser: \"How many figures are in this paper from Page 3 to Page 10?\"  \nAssistant: {{\"filters\": {{\"filter_type\": \"page\", \"filter_value\": \"3-10\"}}}, {{\"filter_type\": \"image\"}}], \"operation\": \"COUNT\"]}  \nUser: \"Summarize the discussion about 'data augmentation' in the 'Methodology' section.\"  \nAssistant: {{\"filters\": {{\"filter_type\": \"section\", \"filter_value\": \"Methodology\"}}], \"operation\": \"SUMMARIZE\"]}  \nUser: \"How many chapters are in this report?\"  \nAssistant: {{\"filters\": {{\"filter_type\": \"section\"}}], \"operation\": \"COUNT\"]}  \n##YOUR CURRENT TASK  \nUser: {\"query\"}  \nUser Query: query"
                    }
                  ]
                }
              ],
              "index": 0,
              "angle": 0,
              "type": "code_body"
            }
          ],
          "index": 0,
          "sub_type": "code",
          "guess_lang": "txt"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The data is unavailable for summarization."
    },
    {
      "index_id": 278,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 17,
        "page_path": null,
        "pdf_id": 278,
        "pdf_para_block": {
          "bbox": [
            195,
            462,
            414,
            474
          ],
          "angle": 0,
          "lines": [
            {
              "bbox": [
                195,
                462,
                414,
                474
              ],
              "spans": [
                {
                  "bbox": [
                    195,
                    462,
                    414,
                    474
                  ],
                  "type": "text",
                  "content": "Figure 12: The prompt for Filter operator generation."
                }
              ]
            }
          ],
          "index": 1,
          "type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Figure 12: The prompt for Filter operator generation.",
        "title_level": -1
      },
      "summary": "Figure 12 illustrates the specific prompt used to generate code for a Filter operator."
    },
    {
      "index_id": 279,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 17,
        "page_path": null,
        "pdf_id": 279,
        "pdf_para_block": {
          "bbox": [
            302,
            714,
            309,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                714,
                309,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    714,
                    309,
                    719
                  ],
                  "type": "text",
                  "content": "18"
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "18",
        "title_level": -1
      },
      "summary": "The content is the number 18."
    },
    {
      "index_id": 280,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 18,
        "page_path": null,
        "pdf_id": 280,
        "pdf_para_block": {
          "type": "code",
          "bbox": [
            53,
            86,
            558,
            653
          ],
          "blocks": [
            {
              "bbox": [
                53,
                86,
                558,
                653
              ],
              "lines": [
                {
                  "bbox": [
                    53,
                    86,
                    558,
                    653
                  ],
                  "spans": [
                    {
                      "bbox": [
                        53,
                        86,
                        558,
                        653
                      ],
                      "type": "text",
                      "content": "-Goal-\nYou are an expert Entity Resolution Adjudicator. Your task is to determine if a \"New Entity\" refers to the exact same real-world concept as one of the \"Candidate Entities\" provided from a knowledge graph. Your output must be a JSON object containing the ID of the matching candidate (or -1) and a brief explanation for your decision.\n-Context-\nYou will be given one \"New Entity\" recently extracted from a text. You will also be given a list of \"Candidate Entities\" that are semantically similar, retrieved from an existing knowledge base. Each candidate has a unique `id' for you to reference.\n---Core Task & Rules-\n1. **Analyze the \"New Entity\"*: Carefully read its name, type, and description to understand what it is.\n2. **Field-by-Field Adjudication**: To determine a match, you must evaluate each field with a specific focus:\n* **entity_name* (High Importance): ** The names must be extremely similar, a direct abbreviation (e.g., \"LLM\" vs. \"Large Language Model\"), or a well-known alias. **If the names represent distinct, parallel concepts (like \"Event Detection\" and \"Named Entity Recognition\"), they are NOT a match, even if their descriptions are very similar.\n* **entity_type* (Medium Importance): ** The types do not need to be identical, but they must be closely related and compatible (e.g., 'COMPANY' and 'ORGANIZATION' could describe the same entity).\n* **description* (Contextual Importance): ** The descriptions may differ as they are often extracted from different parts of a document. Your task is to look past surface-level text similarity and determine if they fundamentally describe the **same underlying object or concept**.\n3. **Be Strict and Conservative**: Your standard for a match must be very high. An incorrect merge can corrupt the knowledge graph. A missed merge is less harmful.\n* Surface-level similarities are not enough. The underlying concepts must be identical.\n* For example, \"Apple\" (the fruit) and \"Apple Inc.\" (the company) are NOT a match.\n* **When in doubt, you MUST output -1**\n* **Assume No Match by Default**: In a large knowledge graph, most new entities are genuinely new. You should start with the assumption that the \"New Entity\" is unique. You must find **strong, convincing evidence** across all fields, especially the 'entity_name', to overturn this assumption and declare a match.\n4. **Format the Output**: **You must provide your answer in a valid JSON format. The JSON object should contain two keys:** *`select_id': An integer. The `id' of the candidate you've determined to be an exact match. If no exact match is found, this value MUST be `-1'. *`explanation': A brief, one-sentence string explaining your reasoning. For a match, explain why they are the same entity. For no match, explain the key difference.\n---Output Schema & Format-\nYour response MUST be a single, valid JSON object that adheres to the following schema. Do not include any other text, explanation, or markdown formatting like ***json.\n***json\n{{ \"select_id\": \"integer\", \"explanation\": \"string\"\n}}\n---***\n-Example-\n### Example 1: Match Found\n### Example 2: No Match Found\n----Task Execution-\nNow, perform the selection task based on the following data. Remember to output only a single integer.\n- Input Data -"
                    }
                  ]
                }
              ],
              "index": 0,
              "angle": 0,
              "type": "code_body"
            }
          ],
          "index": 0,
          "sub_type": "code",
          "guess_lang": "markdown"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "",
        "title_level": -1
      },
      "summary": "The table shows the top five countries by total Olympic medals won as of the 2020 Summer Games. The United States leads with 2,976 medals, followed by the Soviet Union (1,204), Germany (1,056), Great Britain (948), and China (634)."
    },
    {
      "index_id": 281,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 18,
        "page_path": null,
        "pdf_id": 281,
        "pdf_para_block": {
          "bbox": [
            104,
            672,
            504,
            684
          ],
          "angle": 0,
          "lines": [
            {
              "bbox": [
                104,
                672,
                504,
                684
              ],
              "spans": [
                {
                  "bbox": [
                    104,
                    672,
                    504,
                    684
                  ],
                  "type": "text",
                  "content": "Figure 13: The prompt for entity resolution judgement, examples are omitted due to lack of space."
                }
              ]
            }
          ],
          "index": 1,
          "type": "text"
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "Figure 13: The prompt for entity resolution judgement, examples are omitted due to lack of space.",
        "title_level": -1
      },
      "summary": "Figure 13 presents a prompt used for making entity resolution judgements, with specific examples excluded from the content due to space constraints."
    },
    {
      "index_id": 282,
      "parent_id": 260,
      "type": "NodeType.TEXT",
      "meta_info": {
        "file_name": null,
        "file_path": null,
        "page_idx": 18,
        "page_path": null,
        "pdf_id": 282,
        "pdf_para_block": {
          "bbox": [
            302,
            714,
            309,
            719
          ],
          "type": "page_number",
          "angle": 0,
          "lines": [
            {
              "bbox": [
                302,
                714,
                309,
                719
              ],
              "spans": [
                {
                  "bbox": [
                    302,
                    714,
                    309,
                    719
                  ],
                  "type": "text",
                  "content": "19"
                }
              ]
            }
          ],
          "index": 2
        },
        "img_path": null,
        "image_width": 0,
        "image_height": 0,
        "caption": null,
        "footnote": null,
        "table_body": null,
        "content": "19",
        "title_level": -1
      },
      "summary": "The number is 19."
    }
  ],
  "meta_info": {
    "file_name": "BOOKRAG_VLDB_2026_full.pdf",
    "file_path": "D:\\pycharm\\Project\\BookRAG\\BookRAG\\BOOKRAG_VLDB_2026_full.pdf",
    "page_idx": null,
    "page_path": null,
    "pdf_id": null,
    "pdf_para_block": null,
    "img_path": null,
    "image_width": 0,
    "image_height": 0,
    "caption": null,
    "footnote": null,
    "table_body": null,
    "content": null,
    "title_level": -1
  }
}